[
  
  {
    "title": "TDX Specification Introduction",
    "url": "/posts/tdx-intro/",
    "categories": "Confidential Computing, Intel TDX",
    "tags": "",
    "date": "2023-06-15 00:00:00 -0400",
    





    
    "snippet": "This slide provides an overview of Intel TDX, which I presented to my colleagues. It delves into the general architecture of Intel TDX and highlights its capability to ensure confidentiality and in...",
    "content": "This slide provides an overview of Intel TDX, which I presented to my colleagues. It delves into the general architecture of Intel TDX and highlights its capability to ensure confidentiality and integrity for confidential virtual machines.  This is an embedded Microsoft Office presentation, powered by Office."
  },
  
  {
    "title": "TD VM Life Cycle Part 1",
    "url": "/posts/TD-VM-LIFECYCLE-3/",
    "categories": "Confidential Computing, Intel TDX",
    "tags": "",
    "date": "2023-04-05 00:00:00 -0400",
    





    
    "snippet": "TD Boot Memory Setup (TDH.MEM.SEPT.ADD-TDH.MR.EXTEND)In the previous postings, we built the meta data required for launching TD VM such as TDR, TDCS and VMCS of VCPU. However, to actually run code ...",
    "content": "TD Boot Memory Setup (TDH.MEM.SEPT.ADD-TDH.MR.EXTEND)In the previous postings, we built the meta data required for launching TD VM such as TDR, TDCS and VMCS of VCPU. However, to actually run code inside the TD,we need memory pages and its mappings. We will see how TDX Module builds up the Secure EPT for private memories and add initial set of TD private pages using TDH.MEM.SEPT.ADD and TDH.MEM.PAGE.ADD, respectively.2439 static int tdx_vm_ioctl(struct kvm *kvm, void __user *argp)2440 {......2453         case KVM_TDX_INIT_MEM_REGION:2454                 r = tdx_init_mem_region(kvm, &amp;tdx_cmd);2455                 break;2456         case KVM_TDX_FINALIZE_VM:2457                 r = tdx_td_finalizemr(kvm);2458                 break;Loading TDVF to TD VMTypically, initial pages of the TD VM contain Virtual BIOS code and data along with some clear pages for stacks and heap. Most of the guest TD code and data isdynamically loaded at a later stage. Because the TDVF image is prepared by the user process (QEMU) and passed to the KVM, the KVM should interacts with QEMUand TDX Module to successfully load the TDVF image to the TD VM.struct kvm_tdx_init_mem_region {        __u64 source_addr;        __u64 gpa;        __u64 nr_pages; };    QEMU SIDE CODE 248     for_each_fw_entry(&amp;tdx-&gt;fw, entry) {                                    249         struct kvm_tdx_init_mem_region mem_region = {                       250             .source_addr = (__u64)entry-&gt;mem_ptr,                           251             .gpa = entry-&gt;address,                                          252             .nr_pages = entry-&gt;size / 4096, 253         };KVM SIDE CODEstatic int tdx_init_mem_region(struct kvm *kvm, struct kvm_tdx_cmd *cmd){        struct kvm_tdx *kvm_tdx = to_kvm_tdx(kvm);        struct kvm_tdx_init_mem_region region;        struct kvm_vcpu *vcpu;        struct page *page;        u64 error_code;        kvm_pfn_t pfn;        int idx, ret = 0;        /* The BSP vCPU must be created before initializing memory regions. */        if (!atomic_read(&amp;kvm-&gt;online_vcpus))                return -EINVAL;        if (cmd-&gt;flags &amp; ~KVM_TDX_MEASURE_MEMORY_REGION)                return -EINVAL;        if (copy_from_user(&amp;region, (void __user *)cmd-&gt;data, sizeof(region)))                return -EFAULT;        ......Before initializing and loading the VCPU MMU, it first copies region informationfrom the user. This is the address region passed from the user process utilizingthe KVM module. The source_addr is the QEMU’s user level address, containing each TDVF’s section code/data. And the gpa is the address of TDVF sectionwhere each section code/data should be loaded to (GPA). Note that TDVF should be loaded into the designated physical address of the TD-VM so that it can start from there. The passed HVA is used to map TDVF to HPA mapped to the HVA.Recall that memslot is generated for the GPA memory regions as a result of ioctl call to KVMmodule from QEMU. Note that it only accepts TD private pages. When the TD-VM requires the shared page, it should invoke MapGPA to convert it.Loading shared EPT and initializing MMUBefore loading the TDVF to TD VM memory, the MMU of the VCPU should be set up.static int tdx_init_mem_region(struct kvm *kvm, struct kvm_tdx_cmd *cmd)        {             ......        vcpu = kvm_get_vcpu(kvm, 0);        if (mutex_lock_killable(&amp;vcpu-&gt;mutex))                return -EINTR;        vcpu_load(vcpu);        idx = srcu_read_lock(&amp;kvm-&gt;srcu);        kvm_mmu_reload(vcpu);I DON”T KNOW WHY VCPU SHOULD BE LOADED Here..static void vt_vcpu_load(struct kvm_vcpu *vcpu, int cpu){        if (is_td_vcpu(vcpu))                return tdx_vcpu_load(vcpu, cpu);        return vmx_vcpu_load(vcpu, cpu);}void tdx_vcpu_load(struct kvm_vcpu *vcpu, int cpu){        struct vcpu_tdx *tdx = to_tdx(vcpu);        vmx_vcpu_pi_load(vcpu, cpu);        if (vcpu-&gt;cpu == cpu)                return;        tdx_flush_vp_on_cpu(vcpu);        local_irq_disable();        /*         * Pairs with the smp_wmb() in tdx_disassociate_vp() to ensure         * vcpu-&gt;cpu is read before tdx-&gt;cpu_list.         */        smp_rmb();        list_add(&amp;tdx-&gt;cpu_list, &amp;per_cpu(associated_tdvcpus, cpu));        local_irq_enable();}Reload MMU (Shared EPT initialization)The primary job of reloading MMU is initializing the SPT, especially when the root_hpa is set as INVALID_PAGE. Because the root_hpa was touched when the MMU is initialized, but the root of the SPT has not been initialized, so reload funcinitializes the SPT for VM.5732 int kvm_mmu_load(struct kvm_vcpu *vcpu)5733 {5734         int r;5735 5736         r = mmu_topup_memory_caches(vcpu, !vcpu-&gt;arch.mmu-&gt;direct_map);5737         if (r)5738                 goto out;5739         r = mmu_alloc_special_roots(vcpu);5740         if (r)              5741                 goto out;5742         if (vcpu-&gt;arch.mmu-&gt;direct_map)5743                 r = mmu_alloc_direct_roots(vcpu);5744         else5745                 r = mmu_alloc_shadow_roots(vcpu); 5746         if (r)5747                 goto out;5748 5749         kvm_mmu_sync_roots(vcpu);5750 5751         kvm_mmu_load_pgd(vcpu);5752         static_call(kvm_x86_tlb_flush_current)(vcpu);5753 out:   5754         return r;5755 }As we covered before, about how the SPT is allocated, the mmu_alloc_direct_roots allocates the SPT for shared pages and private pages. Please refer to [[]] for details.109 static inline void kvm_mmu_load_pgd(struct kvm_vcpu *vcpu)110 {        111         u64 root_hpa = vcpu-&gt;arch.mmu-&gt;root_hpa;112 113         if (!VALID_PAGE(root_hpa))114                 return;115 116         static_call(kvm_x86_load_mmu_pgd)(vcpu, root_hpa,117                                           vcpu-&gt;arch.mmu-&gt;shadow_root_level);118 }  491 static void vt_load_mmu_pgd(struct kvm_vcpu *vcpu, hpa_t root_hpa, 492                             int pgd_level) 493 { 494         if (is_td_vcpu(vcpu)) 495                 return tdx_load_mmu_pgd(vcpu, root_hpa, pgd_level); 496  497         vmx_load_mmu_pgd(vcpu, root_hpa, pgd_level); 498 }1602 static void tdx_load_mmu_pgd(struct kvm_vcpu *vcpu, hpa_t root_hpa,1603                              int pgd_level)1604 {1605         td_vmcs_write64(to_tdx(vcpu), SHARED_EPT_POINTER, root_hpa &amp; PAGE_MASK);1606 }Because current VCPU belongs to the TD-VM, the SPT used for shared pages should be set to shared EPTP. For vanilla VM, only the EPT_POINTER exists. Because TD-VM also utilize the shared EPT, which is identical to EPT_POINTER in vanilla VM, it should be set up during the MMU-setup. Note that the private EPTP can beset only through SEPT related SEAMCALLs.Adding page to TD VMBecause KVM MMU is initialized and the shared EPTP is correctly set to the VMCS of TD-VCPU, we can finally add some memories to the TD-VM. Let’s go back to the tdx_init_mem_region function.Note that the tdvf binary is already loaded into the QEMU address space dedicated for the TD-VM, but to be utilized as private pages of the TD-VM, it should be added to the target TD-VM through the SEAMCALL.Load the TDVF images into TD memorystatic int tdx_init_mem_region(struct kvm *kvm, struct kvm_tdx_cmd *cmd){    struct kvm_tdx *kvm_tdx = to_kvm_tdx(kvm);    struct kvm_tdx_init_mem_region region;    struct kvm_vcpu *vcpu;    struct page *page;    u64 error_code;    kvm_pfn_t pfn;    int idx, ret = 0;    ......    while (region.nr_pages) {            if (signal_pending(current)) {                    ret = -ERESTARTSYS;                    break;            }                if (need_resched())                    cond_resched();            /* Pin the source page. */            ret = get_user_pages_fast(region.source_addr, 1, 0, &amp;page);            if (ret &lt; 0)                     break;            if (ret != 1) {                     ret = -ENOMEM;                    break;            }                kvm_tdx-&gt;source_pa = pfn_to_hpa(page_to_pfn(page)) |                                 (cmd-&gt;flags &amp; KVM_TDX_MEASURE_MEMORY_REGION);            /* TODO: large page support. */            error_code = TDX_SEPT_PFERR;            error_code |= (PG_LEVEL_4K &lt;&lt; PFERR_LEVEL_START_BIT) &amp;                    PFERR_LEVEL_MASK;            pfn = kvm_mmu_map_tdp_page(vcpu, region.gpa, error_code,                                       PG_LEVEL_4K);            if (is_error_noslot_pfn(pfn) || kvm-&gt;vm_bugged)                    ret = -EFAULT;            else                     ret = 0;             put_page(page);            if (ret)                    break;            region.source_addr += PAGE_SIZE;            region.gpa += PAGE_SIZE;            region.nr_pages--;    }    Because the passed memory region belongs to user process, it should be pinned by the KVM module first before being copied to TD-VM pages. Note that it invokes function get_user_pages_fast with region.source_addr which is the HVA of theQEMU. Also the function assumes that all page table associated with user addressis mapped. Unless the page table has not been resolved yet to translate passed user space address to HPA, KVM just returns. After the pinning, each page shouldbe mapped through the kvm_mmu_map_tdp_page. Note that the pinned page’s physicaladdress is stored in kvm_tdx-&gt;source_pa. This page address will be used later tocopy the content from host page to TD VM page by __tdx_sept_set_private_spte.kvm_mmu_map_tdp_pageWe now have pinned HVA and its physical address HPA. Also, we have target GPAwhere the TDVF should be loaded into. Let’s load the memory!kvm_pfn_t kvm_mmu_map_tdp_page(struct kvm_vcpu *vcpu, gpa_t gpa,                               u32 error_code, int max_level){        int r;        struct kvm_page_fault fault = (struct kvm_page_fault) {                .addr = gpa,                .error_code = error_code,                .exec = error_code &amp; PFERR_FETCH_MASK,                .write = error_code &amp; PFERR_WRITE_MASK,                .present = error_code &amp; PFERR_PRESENT_MASK,                .rsvd = error_code &amp; PFERR_RSVD_MASK,                .user = error_code &amp; PFERR_USER_MASK,                .prefetch = false,                .is_tdp = true,                .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),                .is_private = kvm_is_private_gpa(vcpu-&gt;kvm, gpa),        };              if (mmu_topup_memory_caches(vcpu, false))                return KVM_PFN_ERR_FAULT;        /*         * Loop on the page fault path to handle the case where an mmu_notifier         * invalidation triggers RET_PF_RETRY.  In the normal page fault path,         * KVM needs to resume the guest in case the invalidation changed any         * of the page fault properties, i.e. the gpa or error code.  For this         * path, the gpa and error code are fixed by the caller, and the caller         * expects failure if and only if the page fault can't be fixed.         */                     do {                    fault.max_level = max_level;                fault.req_level = PG_LEVEL_4K;                fault.goal_level = PG_LEVEL_4K;                r = direct_page_fault(vcpu, &amp;fault);        } while (r == RET_PF_RETRY &amp;&amp; !is_error_noslot_pfn(fault.pfn));        return fault.pfn;}               Although there is no page fault because we haven’t executed the VCPU yet in TD-VM side. However, it invokes the direct_page_fault function implementing thepage fault handling as if the EPT violation happens on the GPA that should be initialized. Therefore, direct_page_fault handles injected emulated page faultand allocates all SPTE required for mapping target GPA to HPA. Also note thatthe fault-in address is set as gpa which is the GPA of TD VM where the TDVF memory will be copied into later. After the injected page fault is resolved, theTD VM can access the TDVF by the gpa through the generated mapping.static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault){        bool is_tdp_mmu_fault = is_tdp_mmu(vcpu-&gt;arch.mmu);        unsigned long mmu_seq;        int r;        fault-&gt;gfn = gpa_to_gfn(fault-&gt;addr) &amp; ~kvm_gfn_shared_mask(vcpu-&gt;kvm);        fault-&gt;slot = kvm_vcpu_gfn_to_memslot(vcpu, fault-&gt;gfn);        ......        if (is_tdp_mmu_fault)                r = kvm_tdp_mmu_map(vcpu, fault);        else                r = __direct_map(vcpu, fault);Based on the mmu configuration of vcpu, it invokes kvm_tdp_mmu_map or __direct_map function to handle the fault.static int __direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault){        struct kvm_shadow_walk_iterator it;        gfn_t base_gfn = fault-&gt;gfn;        bool is_private = fault-&gt;is_private;        bool is_zapped_pte;        unsigned int pte_access;        int ret;\t......        __direct_populate_nonleaf(vcpu, fault, &amp;it, &amp;base_gfn);To resolve this TDX page fault, we needs to handle two important things. The first is generating S-EPT mapping. The second is adding page to TD-VM.[[https://github.gatech.edu/sslab/tdx/blob/main/img/ADD_PRIVATE_PAGE.png]]Add S-EPT for private page translation (TDH_MEM_SEPT_ADD)To translate private page belong to TD-VM, it needs SPTE for non-leaf and leaf.Whether it is non-leaf or not, for private pages, it needs to invoke SEAMCALL,TDH_MEM_SEPT_ADD because SPTE for private pages are maintained by the TDX module.static void __direct_populate_nonleaf(struct kvm_vcpu *vcpu,                                struct kvm_page_fault *fault,                                struct kvm_shadow_walk_iterator *itp,                                gfn_t *base_gfnp){        bool is_private = fault-&gt;is_private;        struct kvm_shadow_walk_iterator it;        struct kvm_mmu_page *sp;        gfn_t base_gfn;        if (kvm_gfn_shared_mask(vcpu-&gt;kvm))                fault-&gt;max_level = min(                        fault-&gt;max_level,                        max_level_of_valid_page_type(fault-&gt;gfn, fault-&gt;slot));        /*         * Cannot map a private page to higher level if smaller level mapping         * exists. It can be promoted to larger mapping later when all the         * smaller mapping are there.         */        if (is_private) {                for_each_shadow_entry(vcpu, fault-&gt;addr, it) {                        if (is_shadow_present_pte(*it.sptep)) {                                if (!is_last_spte(*it.sptep, it.level) &amp;&amp;                                        fault-&gt;max_level &gt;= it.level)                                        fault-&gt;max_level = it.level - 1;                        } else {                                break;                        }                }        }        kvm_mmu_hugepage_adjust(vcpu, fault);        trace_kvm_mmu_spte_requested(fault);        for_each_shadow_entry(vcpu, fault-&gt;addr, it) {                /*                 * We cannot overwrite existing page tables with an NX                 * large page, as the leaf could be executable.                 */                if (fault-&gt;nx_huge_page_workaround_enabled)                        disallowed_hugepage_adjust(fault, *it.sptep, it.level);                base_gfn = fault-&gt;gfn &amp; ~(KVM_PAGES_PER_HPAGE(it.level) - 1);                if (it.level == fault-&gt;goal_level)                        break;                drop_large_spte(vcpu, it.sptep);                if (is_shadow_present_pte(*it.sptep))                        continue;                sp = kvm_mmu_get_page(vcpu, base_gfn, it.addr, it.level - 1,                                      true, ACC_ALL, is_private);                link_shadow_page(vcpu, it.sptep, sp);                if (fault-&gt;is_tdp &amp;&amp; fault-&gt;huge_page_disallowed &amp;&amp;                    fault-&gt;req_level &gt;= it.level)                        account_huge_nx_page(vcpu-&gt;kvm, sp);                if (is_private)                        kvm_mmu_link_private_sp(vcpu-&gt;kvm, sp);        }        *itp = it;        if (base_gfnp)                *base_gfnp = base_gfn;}Walking shadow page tablesThe main loop body of page table walking is done by for_each_shadow_entry macro.Although the TDX Module manages S-EPT, the host VMM also maintains the mirror ofthe S-EPT in host memory so that it can reduce the burden of TDX Module. For example, to add S-EPT for one physical page, VMM can ask the TDX Module to walkthe secure page table inside the TDX Module, but host VMM does walk the mirroredS-EPT on behalf of the TDX Module and send request to the TDX Module for S-EPTinsertion through the SEAMCALL TDH_MEM_SEPT_ADD.#define for_each_shadow_entry(_vcpu, _addr, _walker)            \\        for (shadow_walk_init(&amp;(_walker), _vcpu, _addr);        \\             shadow_walk_okay(&amp;(_walker));                      \\             shadow_walk_next(&amp;(_walker)))static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,                             struct kvm_vcpu *vcpu, u64 addr){        hpa_t root;        if (tdp_enabled &amp;&amp; kvm_is_private_gpa(vcpu-&gt;kvm, addr))                root = vcpu-&gt;arch.mmu-&gt;private_root_hpa;        else                root = vcpu-&gt;arch.mmu-&gt;root.hpa;        shadow_walk_init_using_root(iterator, vcpu, root, addr);}Based on whether the fault belongs to private or not, it selects different rootpage table, private_root_hpa or root.hpa, respectively.static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterator,                                        struct kvm_vcpu *vcpu, hpa_t root,                                        u64 addr){        iterator-&gt;addr = addr;        iterator-&gt;shadow_addr = root;        iterator-&gt;level = vcpu-&gt;arch.mmu-&gt;root_role.level;        if (iterator-&gt;level &gt;= PT64_ROOT_4LEVEL &amp;&amp;            vcpu-&gt;arch.mmu-&gt;cpu_role.base.level &lt; PT64_ROOT_4LEVEL &amp;&amp;            !vcpu-&gt;arch.mmu-&gt;root_role.direct)                iterator-&gt;level = PT32E_ROOT_LEVEL;        if (iterator-&gt;level == PT32E_ROOT_LEVEL) {                /*                 * prev_root is currently only used for 64-bit hosts. So only                 * the active root_hpa is valid here.                 */                BUG_ON(root != vcpu-&gt;arch.mmu-&gt;root.hpa);                iterator-&gt;shadow_addr                        = vcpu-&gt;arch.mmu-&gt;pae_root[(addr &gt;&gt; 30) &amp; 3];                iterator-&gt;shadow_addr &amp;= PT64_BASE_ADDR_MASK;                --iterator-&gt;level;                if (!iterator-&gt;shadow_addr)                        iterator-&gt;level = 0;        }}After setting up the root page table, it also initialize the iterator based on the MMU settings, and page fault address.static bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator){        if (iterator-&gt;level &lt; PG_LEVEL_4K)                return false;        iterator-&gt;index = SHADOW_PT_INDEX(iterator-&gt;addr, iterator-&gt;level);        iterator-&gt;sptep = ((u64 *)__va(iterator-&gt;shadow_addr)) + iterator-&gt;index;        return true;}After one iteration finishes, it checks if it can further go down, until leaf.The addr field of the iterator points to the fault-in address. The index of the next level page table is calculated based on current level and fault-in addr.Also, the sptep field points to the non-leaf spte or PTE of this level. Walking continues until the level of iterator reaches the leaf page table entry (PG_LEVEL_4K).static void shadow_walk_next(struct kvm_shadow_walk_iterator *iterator){        __shadow_walk_next(iterator, *iterator-&gt;sptep);}static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,                               u64 spte){        if (!is_shadow_present_pte(spte) || is_last_spte(spte, iterator-&gt;level)) {                iterator-&gt;level = 0;                return;        }        iterator-&gt;shadow_addr = spte &amp; PT64_BASE_ADDR_MASK;        --iterator-&gt;level;}The shadow_addr member field of the iterator points to the root address of the next level page table or the leaf PTE.Set-up S-EPTBefore we add the S-EPT associated with the TDVF image, non-leaf S-EPT entries should be added into the TDX memories so that the mapping from the root to the leaf for the S-EPT will be populated. As a result private pages of the TD-VM canbe translated into the HPA smoothly without incurring any page faults.        for_each_shadow_entry(vcpu, fault-&gt;addr, it) {                /*                 * We cannot overwrite existing page tables with an NX                 * large page, as the leaf could be executable.                 */                if (fault-&gt;nx_huge_page_workaround_enabled)                        disallowed_hugepage_adjust(fault, *it.sptep, it.level);                base_gfn = fault-&gt;gfn &amp; ~(KVM_PAGES_PER_HPAGE(it.level) - 1);                if (it.level == fault-&gt;goal_level)                        break;                drop_large_spte(vcpu, it.sptep);                if (is_shadow_present_pte(*it.sptep))                        continue;                sp = kvm_mmu_get_page(vcpu, base_gfn, it.addr, it.level - 1,                                      true, ACC_ALL, is_private);                link_shadow_page(vcpu, it.sptep, sp);                if (fault-&gt;is_tdp &amp;&amp; fault-&gt;huge_page_disallowed &amp;&amp;                    fault-&gt;req_level &gt;= it.level)                        account_huge_nx_page(vcpu-&gt;kvm, sp);                if (is_private)                        kvm_mmu_link_private_sp(vcpu-&gt;kvm, sp);        }Above loop walks the private page table until the current level matches with thelevel of fault-in page. While the sptep entry presents, it continues walking andallocate one page when it does not present. kvm_mmu_get_page function allocatesnew spte and link_shadow_page links the generated page to sptep of current level.Detailed implementation is already covered in [[]]. Let’s see what difference has been introduced due to TDX.static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,                                             gfn_t gfn,                                             gva_t gaddr,                                             unsigned level,                                             int direct,                                             unsigned int access,                                             unsigned int private){        ......\tsp = kvm_mmu_alloc_page(vcpu, direct, private);        sp-&gt;gfn = gfn;         sp-&gt;role = role;                                         /* kvm_mmu_alloc_private_sp() requires valid role. */        if (private)                kvm_mmu_alloc_private_sp(                        vcpu, sp, level == vcpu-&gt;arch.mmu-&gt;root_role.level);/* Valid sp-&gt;role.level is required. */static inline void kvm_mmu_alloc_private_sp(        struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, bool is_root){                               if (is_root)                sp-&gt;private_sp = KVM_MMU_PRIVATE_SP_ROOT;        else                 sp-&gt;private_sp = kvm_mmu_memory_cache_alloc(                        &amp;vcpu-&gt;arch.mmu_private_sp_cache);        /*         * Because mmu_private_sp_cache is topped up before staring kvm page         * fault resolving, the allocation above shouldn't fail.         */        WARN_ON_ONCE(!sp-&gt;private_sp);}kvm_mmu_get_page allocates kvm_mmu_page. The spte page is allocated by the kvm_mmu_alloc_private_sp function if it is private page. One thing added for private SPTE is private_sp field of the kvm_mmu_mpage. To add new S-EPT, through the SEAMCALL, host KVM should provide memory page to TDX so that it canuse this page to fill out S-EPT. If this is not a root, kvm_mmu_memory_cache_alloc function allocates page for S-EPT.After the SPTE page is allocated, link_shadow_page links allocated page in the host KVM side. Recall that KVM maintains a mirrored SPTE for private pages. However, we do need the S-EPT entry in the TDX side also so that the hardware based translation smoothly translate TD-VM private pages to the HPA during its execution. To this end, it additionally invokes the func kvm_mmu_link_private_spand add S-EPT page at TDX side through the TDH_MEM_SEPT_ADD.static inline int kvm_mmu_link_private_sp(struct kvm *kvm,                                        struct kvm_mmu_page *sp) {                           /* Link this sp to its parent spte.  + 1 for parent spte. */        return static_call(kvm_x86_link_private_sp)(                kvm, sp-&gt;gfn, sp-&gt;role.level + 1, sp-&gt;private_sp);}       static int tdx_sept_link_private_sp(struct kvm *kvm, gfn_t gfn,                                    enum pg_level level, void *sept_page){               int tdx_level = pg_level_to_tdx_sept_level(level);        struct kvm_tdx *kvm_tdx = to_kvm_tdx(kvm);         gpa_t gpa = gfn_to_gpa(gfn);        hpa_t hpa = __pa(sept_page);        struct tdx_module_output out;        u64 err;        spin_lock(&amp;kvm_tdx-&gt;seamcall_lock);        err = tdh_mem_sept_add(kvm_tdx-&gt;tdr.pa, gpa, tdx_level, hpa, &amp;out);        spin_unlock(&amp;kvm_tdx-&gt;seamcall_lock);        if (KVM_BUG_ON(err, kvm)) {                pr_tdx_error(TDH_MEM_SEPT_ADD, err, &amp;out);                return -EIO;        }                                  return 0;}       Add private pages (TDH_MEM_PAGE_ADD)static int __direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault){                                       struct kvm_shadow_walk_iterator it;                                             gfn_t base_gfn = fault-&gt;gfn;                                                    bool is_private = fault-&gt;is_private;                                            bool is_zapped_pte;                                                             unsigned int pte_access;                                                        int ret;                 ......        if (!is_private) {                if (!vcpu-&gt;arch.mmu-&gt;no_prefetch)                        direct_pte_prefetch(vcpu, it.sptep);        } else if (!WARN_ON_ONCE(ret != RET_PF_FIXED)) {                if (is_zapped_pte)                        static_call(kvm_x86_unzap_private_spte)(                                vcpu-&gt;kvm, base_gfn, it.level);                else                        static_call(kvm_x86_set_private_spte)(                                vcpu-&gt;kvm, base_gfn, it.level, fault-&gt;pfn);        }        return ret;}After resolving S-EPT mappings, it returns to the __direct_map and invokes kvm_x86_set_private_spte function to covert host VMM pages containing TDVF intoprivate memory of the TD VM.static void __tdx_sept_set_private_spte(struct kvm *kvm, gfn_t gfn,                                        enum pg_level level, kvm_pfn_t pfn){               int tdx_level = pg_level_to_tdx_sept_level(level);        struct kvm_tdx *kvm_tdx = to_kvm_tdx(kvm);        hpa_t hpa = pfn_to_hpa(pfn);        gpa_t gpa = gfn_to_gpa(gfn);        struct tdx_module_output out;        hpa_t source_pa;        u64 err;        int i;                if (WARN_ON_ONCE(is_error_noslot_pfn(pfn) || kvm_is_reserved_pfn(pfn)))                return;                /* Only support 4KB and 2MB pages */        if (KVM_BUG_ON(level &gt; PG_LEVEL_2M, kvm))                return;                        /* To prevent page migration, do nothing on mmu notifier. */        for (i = 0; i &lt; KVM_PAGES_PER_HPAGE(level); i++)                get_page(pfn_to_page(pfn + i));                        /* Build-time faults are induced and handled via TDH_MEM_PAGE_ADD. */        if (likely(is_td_finalized(kvm_tdx))) {                /*                 * For now only 4K and 2M pages are tested by KVM MMU.                 * TODO: support/test 1G large page.                 */                if (KVM_BUG_ON(level &gt; PG_LEVEL_2M, kvm))                        return;                err = tdh_mem_page_aug(kvm_tdx-&gt;tdr.pa, gpa, tdx_level, hpa, &amp;out);                if (KVM_BUG_ON(err, kvm)) {                        pr_tdx_error(TDH_MEM_PAGE_AUG, err, &amp;out);                        tdx_unpin(kvm, gfn, pfn, level);                }                return;        }        /* KVM_INIT_MEM_REGION, tdx_init_mem_region(), supports only 4K page. */        if (KVM_BUG_ON(level != PG_LEVEL_4K, kvm))                return;        /*         * In case of TDP MMU, fault handler can run concurrently.  Note         * 'source_pa' is a TD scope variable, meaning if there are multiple         * threads reaching here with all needing to access 'source_pa', it         * will break.  However fortunately this won't happen, because below         * TDH_MEM_PAGE_ADD code path is only used when VM is being created         * before it is running, using KVM_TDX_INIT_MEM_REGION ioctl (which         * always uses vcpu 0's page table and protected by vcpu-&gt;mutex).         */        if (KVM_BUG_ON(kvm_tdx-&gt;source_pa == INVALID_PAGE, kvm)) {                tdx_unpin(kvm, gfn, pfn, level);                return;        }        source_pa = kvm_tdx-&gt;source_pa &amp; ~KVM_TDX_MEASURE_MEMORY_REGION;        err = tdh_mem_page_add(kvm_tdx-&gt;tdr.pa, gpa, tdx_level, hpa, source_pa, &amp;out);        if (KVM_BUG_ON(err, kvm)) {                pr_tdx_error(TDH_MEM_PAGE_ADD, err, &amp;out);                tdx_unpin(kvm, gfn, pfn, level);        } else if ((kvm_tdx-&gt;source_pa &amp; KVM_TDX_MEASURE_MEMORY_REGION))                tdx_measure_page(kvm_tdx, gpa, KVM_HPAGE_SIZE(level));        kvm_tdx-&gt;source_pa = INVALID_PAGE;}If the target TD-VM has been already finalized, the page can only be inserted as the TDH_MEM_PAGE_AUG SEAMCALL, but if not, TDH_MEM_PAGE_ADD SEAMCALL allows it to have new private page.[[https://github.gatech.edu/sslab/tdx/blob/main/img/TDH_MEM_PAGE_ADD.png]]This SEAMCALL requires four important information about the addresses to add newpage to the TD VM. The first is the EPT mapping information which we alreadyhave as a result of TDH_MEM_SEPT_ADD. The second is HPA of the TDR page. The third one is HPA of the target page to be added to the TD VM. And the last one is the address of the source page containing data/code. Note that the source_papoints to the QEMU page containing TDVF image. gpa is the EPT mapping address inside the TDX Module. Recall that the main loop for S-EPT updates base_gfn to the updates S-EPT entry. hpa is the destination page that will be added to the TD VM (TODO: Need to understand where the hpa parameter comes from.. it shouldhave been allocated by the VMM before..)KVM_TDX_FINALIAZE_VMAfter the initial set of pages is added and extended, the VMM can finalize the TD measurement using the TDH.MR.FINALIZE SEAMCALL. After this SEAMCALL returns successfully, its measurement cannot be modified anymore (except the run-timemeasurement registers). Also, the TD VCPUs can enter to the TD VM through the TDH.VP.ENTER.Miscstatic_callarch/x86/kvm/x86.c  131 #define KVM_X86_OP(func)                                             \\  132         DEFINE_STATIC_CALL_NULL(kvm_x86_##func,                      \\  133                                 *(((struct kvm_x86_ops *)0)-&gt;func));  134 #define KVM_X86_OP_NULL KVM_X86_OP  135 #include &lt;asm/kvm-x86-ops.h&gt;asm/kvm-x86-ops.h/* * KVM_X86_OP() and KVM_X86_OP_NULL() are used to help generate * \"static_call()\"s. They are also intended for use when defining * the vmx/svm kvm_x86_ops. KVM_X86_OP() can be used for those * functions that follow the [svm|vmx]_func_name convention. * KVM_X86_OP_NULL() can leave a NULL definition for the * case where there is no definition or a function name that * doesn't match the typical naming convention is supplied. */KVM_X86_OP_NULL(hardware_enable)KVM_X86_OP_NULL(hardware_disable)KVM_X86_OP_NULL(hardware_unsetup)KVM_X86_OP_NULL(cpu_has_accelerated_tpr)KVM_INTEL_TDX_SEAM_BACKDOOR  This kernel config enables Trusted Domain Extensions backdoor interface for development.Backdoor interface provides raw interface to call TDX SEAM module for user land. This is only for development so that KVM doesn’t guarantee any integrity like cache coherency. To enable this feature, also pass tdx_seam_backdoor to the command line.https://lwn.net/Articles/827925/  SEV currently needs to pin guest memory as it doesn’t support migratingencrypted pages.  Introduce a framework in KVM’s MMU to support pinningpages on demand without requiring additional memory allocations, and with(somewhat hazy) line of sight toward supporting more advanced features forencrypted guest memory, e.g. host page migration.The idea is to use a software available bit in the SPTE to track that apage has been pinned.  The decision to pin a page and the actual pinningmanagment is handled by vendor code via kvm_x86_ops hooks.Introduce a helper to directly (pun intended) fault-in a TDP pagewithout having to go through the full page fault path.  This allowsTDX to get the resulting pfn and also allows the RET_PF_* enums tostay in mmu.c where they belong."
  },
  
  {
    "title": "TD VM Life Cycle Part 2",
    "url": "/posts/TD-VM-LIFECYCLE-2/",
    "categories": "Confidential Computing, Intel TDX",
    "tags": "",
    "date": "2023-04-03 00:00:00 -0400",
    





    
    "snippet": "Deep dive into TD VCPU creation (TDH_VP_CREATE-TDH_VP_INIT)Instantiating TD VCPUAfter the VM has been initialized, note that it has not been finalized yet, it can generate VCPUs assigned to the gen...",
    "content": "Deep dive into TD VCPU creation (TDH_VP_CREATE-TDH_VP_INIT)Instantiating TD VCPUAfter the VM has been initialized, note that it has not been finalized yet, it can generate VCPUs assigned to the generated TD instance. The logistics of TD VCPU generation consists of two parts mainly: generate VCPU (KVM_CREATE_VCPU) and initialize the generated VCPU as TDX VCPU (KVM_TDX_INIT_VCPU) through the SEAMCALL, TDH_VP_INIT.VCPU Creation for TDXThe first step is generating VCPU instance as we do for vanilla VM’s VCPU. It utilizes the same interface from QEMU side, KVM_CREATE_VCPU of the kvm_vm_ioctl.kvm_vm_ioctl_create_vcpu is the main function handling the ioctl and invokes following functions to initialize the VCPU related features including MMU.MMU initialization details are described in [here].10997 int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)......        r = static_call(kvm_x86_vcpu_create)(vcpu);        if (r)                goto free_guest_fpu;        vcpu-&gt;arch.arch_capabilities = kvm_get_arch_capabilities();        vcpu-&gt;arch.msr_platform_info = MSR_PLATFORM_INFO_CPUID_FAULT;        kvm_xen_init_vcpu(vcpu);        kvm_vcpu_mtrr_init(vcpu);        vcpu_load(vcpu);        kvm_set_tsc_khz(vcpu, vcpu-&gt;kvm-&gt;arch.default_tsc_khz);        kvm_vcpu_reset(vcpu, false);        kvm_init_mmu(vcpu);        vcpu_put(vcpu);        return 0;kvm_arch_vcpu_create is the architecture specific VCPU initialization function.struct kvm_x86_ops vt_x86_ops __initdata = {        .vcpu_create = vt_vcpu_create,static int vt_vcpu_create(struct kvm_vcpu *vcpu){        if (is_td_vcpu(vcpu))                return tdx_vcpu_create(vcpu);        return vmx_vcpu_create(vcpu);}And the kvm_x86_vcpu_create function, actually the tdx_vcpu_create function generates the TD VM’s VCPU instances.Trust Domain Virtual Processor State (TDVPS)  Trust Domain Virtual Processor Root (TDVPR) is the 4KB root page of TDVPS. Its physical address serves as a unique identifier of the VCPU (as long as it resides in memory).As the TDR is the id of the TD VM, each TD VCPU requires associated TDX metadatacalled Trust Domain Virtual Processor State (TDVPS). For example, TD VCPU and TD VMCS structure are stored in the TDVPS as shown in the figure.  Trust Domain Virtual Processor eXtension (TDVPX) 4KB pages extend TDVPR to helpprovide enough physical space for the logical TDVPS structure.TDX logically views the TDVPS as a consecutive memory region containing all VMX standard control structure such as TD VMCS and TD VCPU. Especially, TD VCPU Management fields manage the operation of the VCPU. However, physically, it consists of multiple physical pages represented as TDVPR root page and TDVPX child pages.  The required number of 4KB TDVPR/TDVPX pages in TDVPS is enumerated to the VMMby the TDH.SYS.INFO function.TDVPS management in TDX Moduletypedef struct tdvps_management_s{    uint8_t   state; /**&lt; The activity state of the VCPU */    /**     * A boolean flag, indicating whether the TD VCPU has been VMLAUNCH’ed     * on this LP since it has last been associated with this VCPU. If TRUE,     * VM entry should use VMRESUME. Else, VM entry should use VMLAUNCH.     */    bool_t    launched;    /**     * Sequential index of the VCPU in the parent TD. VCPU_INDEX indicates the order     * of VCPU initialization (by TDHVPINIT), starting from 0, and is made available to     * the TD via TDINFO. VCPU_INDEX is in the range 0 to (MAX_VCPUS_PER_TD - 1)     */    uint32_t  vcpu_index;    uint8_t   num_tdvpx; /**&lt; A counter of the number of child TDVPX pages associated with this TDVPR */    uint8_t   reserved_0[1]; /**&lt; Reserved for aligning the next field */    /**     * An array of (TDVPS_PAGES) physical address pointers to the TDVPX pages     *     * PA is without HKID bits     * Page 0 is the PA of the TDVPR page     * Pages 1,2,... are PAs of the TDVPX pages    */    uint64_t  tdvps_pa[MAX_TDVPS_PAGES];    /**     * The (unique hardware-derived identifier) of the logical processor on which this VCPU     * is currently associated (either by TDHVPENTER or by other VCPU-specific SEAMCALL flow).     * A value of 0xffffffff (-1 in signed) indicates that VCPU is not associated with any LP.     * Initialized by TDHVPINIT to the LP_ID on which it ran     */    uint32_t  assoc_lpid;    /**     * The TD's ephemeral private HKID at the last time this VCPU was associated (either     * by TDHVPENTER or by other VCPU-specific SEAMCALL flow) with an LP.     * Initialized by TDHVPINIT to the current TD ephemeral private HKID.     */    uint32_t  assoc_hkid;    /**     * The value of TDCS.TD_EPOCH, sampled at the time this VCPU entered TDX non-root mode     */    uint64_t  vcpu_epoch;    bool_t    cpuid_supervisor_ve;    bool_t    cpuid_user_ve;    bool_t    is_shared_eptp_valid;    uint8_t   reserved_1[5]; /**&lt; Reserved for aligning the next field */    uint64_t  last_exit_tsc;    bool_t    pend_nmi;    uint8_t   reserved_2[7]; /**&lt; Reserved for aligning the next field */    uint64_t  xfam;    uint8_t   last_epf_gpa_list_idx;    uint8_t   possibly_epf_stepping;    uint8_t   reserved_3[150]; /**&lt; Reserved for aligning the next field */    uint64_t   last_epf_gpa_list[EPF_GPA_LIST_SIZE];  // Array of GPAs that caused EPF at this TD vCPU instruction    uint8_t   reserved_4[256]; /**&lt; Reserved for aligning the next field */} tdvps_management_t;Allocating TDVPR page and TDVPX pagesVMM should prepare TDVPR and TDVPX pages before it bounds the pages to the VCPU.tdx_vcpu_create function allocate one TDVPR page and multiple TDVPX pages.634 int tdx_vcpu_create(struct kvm_vcpu *vcpu)635 {......648         ret = tdx_alloc_td_page(&amp;tdx-&gt;tdvpr);649         if (ret)650                 return ret;651 652         tdx-&gt;tdvpx = kcalloc(tdx_caps.tdvpx_nr_pages, sizeof(*tdx-&gt;tdvpx),653                         GFP_KERNEL_ACCOUNT);654         if (!tdx-&gt;tdvpx) {655                 ret = -ENOMEM;656                 goto free_tdvpr;657         }658         for (i = 0; i &lt; tdx_caps.tdvpx_nr_pages; i++) {659                 ret = tdx_alloc_td_page(&amp;tdx-&gt;tdvpx[i]);660                 if (ret)661                         goto free_tdvpx;662         }To utilize the created VCPU, the TDX VM should have a TDVPR page bound to the VCPU assigned to the TD VM. Physical page for TDVPR is allocated when VCPU is created. Also note that it allocates multiple physical pages for TDVPX. Thegenerated TDVPR is used when registering the generated VCPU to specific TD VM through TDH_VP_CREATE, which adds a TDVPR page as a child of a TDR page. Also, TDH.VP.ADDCX adds a TDVPX page as a child of a given TDVPR (tdh_vp_addcx). We will cover when and how the TDVPX pages are initialized based on their semanticsas VMCS for example.Create VCPU for TD VMNote that we haven’t generated any VCPU instance, but TDVPS page required forgenerating TDX VPCU instance. Because the TD VCPU should be belong to one TD VM,it requires TDR page to denote which TD VM will have the access for the newly generated TD VCPU. You will see that TDH_VP_CREATE SEAMCALL receive these two data structures to instantiate new VCPU for TDX.11148 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)......11230         static_call(kvm_x86_vcpu_reset)(vcpu, init_event); 992 static struct kvm_x86_ops vt_x86_ops __initdata = {......1008         .vcpu_reset = vt_vcpu_reset, 169 static void vt_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event) 170 { 171         if (is_td_vcpu(vcpu)) 172                 return tdx_vcpu_reset(vcpu, init_event); 173  174         return vmx_vcpu_reset(vcpu, init_event); 175 }If current kvm_vcpu indicates that it is VCPU for TD-VM, it invokes tdx_vcpu_reset function that calls TDH_VP_CREATE SEAMCALL. 825 void tdx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event) 826 { ...... 839         err = tdh_vp_create(kvm_tdx-&gt;tdr.pa, tdx-&gt;tdvpr.pa); 840         if (WARN_ON_ONCE(err)) { 841                 pr_tdx_error(TDH_VP_CREATE, err, NULL); 842                 goto td_bugged; 843         } 844         tdx_mark_td_page_added(&amp;tdx-&gt;tdvpr); 845  846         for (i = 0; i &lt; tdx_caps.tdvpx_nr_pages; i++) { 847                 err = tdh_vp_addcx(tdx-&gt;tdvpr.pa, tdx-&gt;tdvpx[i].pa); 848                 if (WARN_ON_ONCE(err)) { 849                         pr_tdx_error(TDH_VP_ADDCX, err, NULL); 850                         goto td_bugged; 851                 } 852                 tdx_mark_td_page_added(&amp;tdx-&gt;tdvpx[i]); 853         }The TDH_VP_CREATE SEAMCALL creates the VCPU by registering TDVPR page as a childof TDR. Note that TDR page is owned by the TDX module, so the TDX module should register the TDVPR page as its child. After the TDVPR page is added, because itis a logical concept, additional TDVPX pages should be added in addition to the first TDVPR page by TDH_VP_ADDCX SEAMCALL.TDX_VP_CREATE (TDX Module side)Because the TDVPR page address is passed from VMM, it needs to be converted intoprivate page of the TD VM. We already covered how this conversion is done by theTDX module ([XXX]). After the private mapping is created, the TDVPR page should be initialized and registered as a child of TDR. Through this process, the TDVCPU is created and bound to specific TD VM.api_error_type tdh_vp_create(uint64_t target_tdvpr_pa, uint64_t target_tdr_pa){    ......    // Check, lock and map the new TDVPR page    return_val = check_lock_and_map_explicit_private_4k_hpa(tdvpr_pa,                                                            OPERAND_ID_RCX,                                                            tdr_ptr,                                                            TDX_RANGE_RW,                                                            TDX_LOCK_EXCLUSIVE,                                                            PT_NDA,                                                            &amp;tdvpr_pamt_block,                                                            &amp;tdvpr_pamt_entry_ptr,                                                            &amp;tdvpr_locked_flag,                                                            (void**)&amp;tdvps_ptr);    ......    tdvps_ptr-&gt;management.assoc_lpid = (uint32_t)-1;    tdvps_ptr-&gt;management.tdvps_pa[0] = tdvpr_pa.raw;    // Register the new TDVPR page in its owner TDR    _lock_xadd_64b(&amp;(tdr_ptr-&gt;management_fields.chldcnt), 1);    // Set the new TDVPR page PAMT fields    tdvpr_pamt_entry_ptr-&gt;pt = PT_TDVPR;    set_pamt_entry_owner(tdvpr_pamt_entry_ptr, tdr_pa);TDH_VP_ADDCX (TDX Module side)Physically the TDVPR page consists of multiple TDVPX pages, and the TDH_VP_ADDCXadds the TDVPX page as the child of TDVPR. Note that this function receives the TDVPR and TDVPX page not the TDR page.api_error_type tdh_vp_addcx(uint64_t target_tdvpx_pa, uint64_t target_tdvpr_pa){    ......    // Check and lock the parent TDVPR page    return_val = check_and_lock_explicit_4k_private_hpa(tdvpr_pa,                                                         OPERAND_ID_RDX,                                                         TDX_LOCK_EXCLUSIVE,                                                         PT_TDVPR,                                                         &amp;tdvpr_pamt_block,                                                         &amp;tdvpr_pamt_entry_ptr,                                                         &amp;page_leaf_size,                                                         &amp;tdvpr_locked_flag);    // Get and lock the owner TDR page                       tdr_pa = get_pamt_entry_owner(tdvpr_pamt_entry_ptr);     return_val = lock_and_map_implicit_tdr(tdr_pa,                                                  OPERAND_ID_TDR,                                           TDX_RANGE_RW,                                            TDX_LOCK_SHARED,                                           &amp;tdr_pamt_entry_ptr,                                           &amp;tdr_locked_flag,                                           &amp;tdr_ptr);    ......    // Map the TDVPS structure.  Note that only the 1st page (TDVPR) is    // accessible at this point.    tdvps_ptr = (tdvps_t*)map_pa((void*)(set_hkid_to_pa(tdvpr_pa, td_hkid).full_pa), TDX_RANGE_RW);    ......    // Check, lock and map the new TDVPX page    return_val = check_lock_and_map_explicit_private_4k_hpa(tdvpx_pa,                                                            OPERAND_ID_RCX,                                                            tdr_ptr,                                                            TDX_RANGE_RW,                                                            TDX_LOCK_EXCLUSIVE,                                                            PT_NDA,                                                            &amp;tdvpx_pamt_block,                                                            &amp;tdvpx_pamt_entry_ptr,                                                            &amp;tdvpx_locked_flag,                                                            (void**)&amp;tdvpx_ptr);    ......    // Clear the content of the TDVPX page using direct writes    zero_area_cacheline(tdvpx_ptr, TDX_PAGE_SIZE_IN_BYTES);    // Register the new TDVPX in its parent TDVPS structure    // Note that tdvpx_pa[0] is the PA of TDVPR, so TDVPX    // pages start from index 1    tdvpx_index_num++;    tdvps_ptr-&gt;management.num_tdvpx = (uint8_t)tdvpx_index_num;    tdvps_ptr-&gt;management.tdvps_pa[tdvpx_index_num] = tdvpx_pa.raw;    // Register the new TDVPX page in its owner TDR    _lock_xadd_64b(&amp;(tdr_ptr-&gt;management_fields.chldcnt), 1);    // Set the new TDVPX page PAMT fields    tdvpx_pamt_entry_ptr-&gt;pt = PT_TDVPX;    set_pamt_entry_owner(tdvpx_pamt_entry_ptr, tdr_pa);Because the TDVPR page has been initialized and registered as a private page forthe TD VM before in the TDH_VP_CREATE SEMCALL, its corresponding PAMT block willbe also retrieved as a result of the check_and_lock_explicit_4k_private_hpa func.Because each PAMT entry memorize owner TD VM where the physical address mapped by the PAMT belongs to, it can easily retrieve its owner, the TDR. It also maps TDVPR and TDVPX, but because TDVPX is mapped to private page first time, mappingis done by map_pa and check_lock_and_map_explicit_private_4k_hpa, respectively.It updates the TDVPR and initialize TDVPX page. Note that still each TDVPX pageis not initialized. We will see how each TDVPX page will be initialized to carryVCPU information.Initialize registered VCPU (TDH_VP_INIT)After adding all VCPU related physical pages to the TD VM, it is ready for VCPUinitialization. When the TDX module finish initialization of VCPU of the TD VM, the status of the VM is changed to initialized.2472 static int tdx_vcpu_ioctl(struct kvm_vcpu *vcpu, void __user *argp)......2488         if (cmd.metadata || cmd.id != KVM_TDX_INIT_VCPU)2489                 return -EINVAL;2490 2491         err = tdh_vp_init(tdx-&gt;tdvpr.pa, cmd.data);2492         if (TDX_ERR(err, TDH_VP_INIT, NULL))2493                 return -EIO;2494 2495         tdx-&gt;initialized = true;2496 2497         td_vmcs_write16(tdx, POSTED_INTR_NV, POSTED_INTR_VECTOR);2498         td_vmcs_write64(tdx, POSTED_INTR_DESC_ADDR, __pa(&amp;tdx-&gt;pi_desc));2499         td_vmcs_setbit32(tdx, PIN_BASED_VM_EXEC_CONTROL, PIN_BASED_POSTED_INTR);It invokes tdh_vp_init function which invokes TDH_VP_INIT SEAMCALL. We can say that The VCPU initialization is equal to TDVPR page initialization. Let’s see how TDX Module initializes the TDVPR pages and related data structures used to manage TDVPS.Initialize TDVPS pagetypedef enum{       TDVPS_VE_INFO_PAGE_INDEX = 0,    TDVPS_VMCS_PAGE_INDEX    = 1,    TDVPS_VAPIC_PAGE_INDEX   = 2,    MAX_TDVPS_PAGES          = 6} tdvps_pages_e;typedef struct ALIGN(TDX_PAGE_SIZE_IN_BYTES) tdvps_s            {       tdvps_ve_info_t                ve_info;    uint8_t                        reserved_0[128]; /**&lt; Reserved for aligning the next field */    tdvps_management_t             management;    tdvps_guest_state_t            guest_state;    tdvps_guest_msr_state_t        guest_msr_state;        uint8_t                        reserved_1[2432]; /**&lt; Reserved for aligning the next field */        tdvps_td_vmcs_t                td_vmcs;    uint8_t                        reserved_2[TDX_PAGE_SIZE_IN_BYTES - SIZE_OF_TD_VMCS_IN_BYTES]; /**&lt; Reserved for aligning the next field */    tdvps_vapic_t                  vapic;    tdvps_guest_extension_state_t  guest_extension_state;} tdvps_t;TDVPS page can semantically be divided into 3 different pages as shown in the tdvps_pages_e: VE_INFO, VMCS, VAPIC page. Let’s see how TDX Module functions initialize those three pages as a part of TDH_VP_INIT SEAMCALL.api_error_type tdh_vp_init(uint64_t target_tdvpr_pa, uint64_t td_vcpu_rcx){    ......    // Get the TD's ephemeral HKID    curr_hkid = tdr_ptr-&gt;key_management_fields.hkid;    // Map the multi-page TDVPS structure    tdvps_ptr = map_tdvps(tdvpr_pa, curr_hkid, TDX_RANGE_RW);    ......    /**     *  Initialize the TD VCPU GPRs.  Default GPR value is 0.     *  Initialize the TD VCPU non-GPR register state in TDVPS:     *  CRs, DRs, XCR0, IWK etc.     */    init_vcpu_gprs_and_registers(tdvps_ptr, tdcs_ptr, init_rcx, vcpu_index);    /**     *  Initialize the TD VCPU MSR state in TDVPS     */    init_vcpu_msrs(tdvps_ptr);    /**     *  No need to explicitly initialize TD VCPU extended state pages.     *  Since the pages are initialized to 0 on TDHVPCREATE/TDVPADDCX.     */    // Bit 63 of XCOMP_BV should be set to 1, to indicate compact format.    // Otherwise XSAVES and XRSTORS won't work    tdvps_ptr-&gt;guest_extension_state.xbuf.xsave_header.xcomp_bv = BIT(63);    // Initialize TDVPS.LBR_DEPTH to MAX_LBR_DEPTH supported on the core    if (((ia32_xcr0_t)tdcs_ptr-&gt;executions_ctl_fields.xfam).lbr)    {        tdvps_ptr-&gt;guest_msr_state.ia32_lbr_depth = (uint64_t)get_global_data()-&gt;max_lbr_depth;    }    /**     *  No need to explicitly initialize VAPIC page.     *  Since the pages are initialized to 0 on TDHVPCREATE/TDVPADDCX,     *  VAPIC page is already 0.     */Note that it receives cmd.data which will be set as an initial RCX value of theVCPU. This RCX value will be used when VBIOS initially starts from TD VM._STATIC_INLINE_ void init_vcpu_gprs_and_registers(tdvps_t * tdvps_ptr, tdcs_t * tdcs_ptr, uint64_t init_rcx, uint32_t vcpu_index){    /**     *  GPRs init     */    if (tdcs_ptr-&gt;executions_ctl_fields.gpaw)    {        tdvps_ptr-&gt;guest_state.rbx = MAX_PA_FOR_GPAW;    }    else    {        tdvps_ptr-&gt;guest_state.rbx = MAX_PA_FOR_GPA_NOT_WIDE;    }    // Set RCX and R8 to the input parameter's value    tdvps_ptr-&gt;guest_state.rcx = init_rcx;    tdvps_ptr-&gt;guest_state.r8 = init_rcx;    // CPUID(1).EAX - returns Family/Model/Stepping in EAX - take the saved value by TDHSYSINIT    tdx_debug_assert(get_cpuid_lookup_entry(0x1, 0x0) &lt; MAX_NUM_CPUID_LOOKUP);    tdvps_ptr-&gt;guest_state.rdx = (uint64_t)get_global_data()-&gt;cpuid_values[get_cpuid_lookup_entry(0x1, 0x0)].values.eax;    /**     *  Registers init     */    tdvps_ptr-&gt;guest_state.xcr0 = XCR0_RESET_STATE;    tdvps_ptr-&gt;guest_state.dr6 = DR6_RESET_STATE;    // Set RSI to the VCPU index    tdvps_ptr-&gt;guest_state.rsi = vcpu_index &amp; BITS(31,0);    /**     *  All other GPRs/Registers are set to 0 or     *  that their INIT state is 0     *  Doesn’t include values initialized in VMCS     */}VMCS initializationThe most important page of the TDVPS is VMCS of the TD VCPU. It is literally identical with the VMCS for vanilla VM VCPU. Let’s see how the TDX Module setsup VMCS structure for TD VCPU.    vmcs_pa = set_hkid_to_pa((pa_t)tdvps_ptr-&gt;management.tdvps_pa[TDVPS_VMCS_PAGE_INDEX], curr_hkid);    /**     *  Map the TD VMCS page.     *     *  @note This is the only place the VMCS page is directly accessed.     */    vmcs_ptr = map_pa((void*)vmcs_pa.raw, TDX_RANGE_RW);    vmcs_ptr-&gt;revision.vmcs_revision_identifier =            get_global_data()-&gt;plt_common_config.ia32_vmx_basic.vmcs_revision_id;    // Clear the TD VMCS    ia32_vmclear((void*)vmcs_pa.raw);    /**     *  No need to explicitly initialize VE_INFO.     *  Since the pages are initialized to 0 on TDHVPCREATE/TDVPADDCX,     *  VE_INFO.VALID is already 0.     */    // Mark the VCPU as initialized and ready    tdvps_ptr-&gt;management.state = VCPU_READY_ASYNC;    /**     *  Save the host VMCS fields before going to TD VMCS context     */    save_vmcs_host_fields(&amp;td_vmcs_host_values);    /**     *  Associate the VCPU - no checks required     */    associate_vcpu_initial(tdvps_ptr, tdcs_ptr, tdr_ptr, &amp;td_vmcs_host_values);    td_vmcs_loaded = true;    /**     *  Initialize the TD VMCS fields     */    init_td_vmcs(tdcs_ptr, tdvps_ptr, &amp;td_vmcs_host_values);Because TDX Module doesn’t manage the virtual mapping of all physical pages of the meta data of one TD VM, it should be mapped first and should retrieve a virtual address for TD VMCS page. The management structure maintain physical pages of the TDVPS pages. After the mapping is done, init_td_vmcs initializes VMCS for TD VCPU.void save_vmcs_host_fields(vmcs_host_values_t* host_fields_ptr){       read_vmcs_field_info(VMX_HOST_CR0_ENCODE, &amp;host_fields_ptr-&gt;CR0);    read_vmcs_field_info(VMX_HOST_CR3_ENCODE, &amp;host_fields_ptr-&gt;CR3);    read_vmcs_field_info(VMX_HOST_CR4_ENCODE, &amp;host_fields_ptr-&gt;CR4);    read_vmcs_field_info(VMX_HOST_CS_SELECTOR_ENCODE, &amp;host_fields_ptr-&gt;CS);    read_vmcs_field_info(VMX_HOST_SS_SELECTOR_ENCODE, &amp;host_fields_ptr-&gt;SS);    read_vmcs_field_info(VMX_HOST_FS_SELECTOR_ENCODE, &amp;host_fields_ptr-&gt;FS);    read_vmcs_field_info(VMX_HOST_GS_SELECTOR_ENCODE, &amp;host_fields_ptr-&gt;GS);    read_vmcs_field_info(VMX_HOST_TR_SELECTOR_ENCODE, &amp;host_fields_ptr-&gt;TR);    read_vmcs_field_info(VMX_HOST_IA32_S_CET_ENCODE, &amp;host_fields_ptr-&gt;IA32_S_CET);    read_vmcs_field_info(VMX_HOST_SSP_ENCODE, &amp;host_fields_ptr-&gt;SSP);    read_vmcs_field_info(VMX_HOST_IA32_PAT_FULL_ENCODE, &amp;host_fields_ptr-&gt;IA32_PAT);    read_vmcs_field_info(VMX_HOST_IA32_EFER_FULL_ENCODE, &amp;host_fields_ptr-&gt;IA32_EFER);    read_vmcs_field_info(VMX_HOST_FS_BASE_ENCODE, &amp;host_fields_ptr-&gt;FS_BASE);    read_vmcs_field_info(VMX_HOST_RSP_ENCODE, &amp;host_fields_ptr-&gt;RSP);    read_vmcs_field_info(VMX_HOST_GS_BASE_ENCODE, &amp;host_fields_ptr-&gt;GS_BASE);}            VMCS needs to be configured for two interfaces, host to VM and VM to host. The all required information to control VM to host interface is maintained by the TDX Module. Recall that we are still in the VMX root operation while the TDX Module executes. Also, VMREAD instruction reads from the current VMCS when the processor runs as VMX root operation. If executed in VMX non-root operation, the instruction reads from the VMCS referenced by the VMCS link pointer field inthe current VMCS.void associate_vcpu_initial(tdvps_t * tdvps_ptr,                            tdcs_t * tdcs_ptr,                            tdr_t * tdr_ptr,                            vmcs_host_values_t * host_values){    uint32_t         curr_lp_id = get_local_data()-&gt;lp_info.lp_id;    uint16_t         curr_hkid;    pa_t             vmcs_addr;        tdvps_ptr-&gt;management.assoc_lpid = curr_lp_id;                curr_hkid = tdr_ptr-&gt;key_management_fields.hkid;        // Set the TD VMCS as the current VMCS    vmcs_addr = set_hkid_to_pa((pa_t)tdvps_ptr-&gt;management.tdvps_pa[TDVPS_VMCS_PAGE_INDEX], curr_hkid);            ia32_vmptrld((void*)vmcs_addr.raw);        /**     *  Update multiple TD VMCS physical address fields with the new HKID.     */     init_guest_td_address_fields(tdr_ptr, tdvps_ptr, curr_hkid);        /**     *  Update the TD VMCS LP-dependent host state fields.     *  Applicable fields are HOST_RSP, HOST_SSP and HOST_GS_BASE     */     ia32_vmwrite(host_values-&gt;RSP.encoding, host_values-&gt;RSP.value);    ia32_vmwrite(host_values-&gt;SSP.encoding, host_values-&gt;SSP.value);    ia32_vmwrite(host_values-&gt;GS_BASE.encoding, host_values-&gt;GS_BASE.value);    // Atomically increment the number of associated VCPUs    _lock_xadd_32b(&amp;(tdcs_ptr-&gt;management_fields.num_assoc_vcpus), 1);}To initialize the TD VMCS of the target TD, the VMCS should be first loaded intothe processor. It retrieves the VMCS from the tdvps and run vmptrld inst to switch VCPU of TDX Module to VCPU of TD VM.After switching the VCPU the most of the VCPU initialization is done by the funcinit_td_vmcs. I will not cover the details, but previously stored host registersSEAM VMCS will be written to TD VMCS’s host registers because VM EXIT from the TD should jump to the TDX Module. Also the initialization includes private EPTP.Note that the EPTP address is stored in the TDCS (refer to part 1). However, note that the entire EPTP has not been initialized, but the root.#define VMX_GUEST_EPT_POINTER_FULL_ENCODE  0x201AULL#define VMX_GUEST_EPT_POINTER_HIGH_ENCODE  0x201bULL#define VMX_GUEST_SHARED_EPT_POINTER_FULL_ENCODE  0x203C#define VMX_GUEST_SHARED_EPT_POINTER_HIGH_ENCODE  0x203DAlso, note that there are two different types of EPTP for TD VM. For that VMCS needs to be updated to contain pointers of shared and private EPTP. The private EPTP should be protected from the non-TD software layers, so it should be initialized by the TDX Module and written to VMCS through TDH_VP_INIT. However,the shared EPTP is provided by the Host VMM (see detail in part 3), and it doesn’t need to be secure. The purpose of shared EPTP is for sharing datawith host VMM. Therefore, another SEAMCALL is used to write the VMCS located in the TDX memory (TDH.VP.WR).Read Write to TD VMCS from Host VMMBased on the debugging mode or production mode, TDX allows the VMM to read/writesome pages belong to TD VM, for example, TDVPS and VMCS. For this, TDX provides two SEAMCALL: TDH.VP.RD and TDH.VP.WR. To utilize the two SEAMCALL, KVM definesbelow macro functions.199 #define TDX_BUILD_TDVPS_ACCESSORS(bits, uclass, lclass)                        \\200 static __always_inline u##bits td_##lclass##_read##bits(struct vcpu_tdx *tdx,  \\201                                                         u32 field)             \\202 {                                                                              \\203         struct tdx_ex_ret ex_ret;                                              \\204         u64 err;                                                               \\205                                                                                \\206         tdvps_##lclass##_check(field, bits);                                   \\207         err = tdh_vp_rd(tdx-&gt;tdvpr.pa, TDVPS_##uclass(field), &amp;ex_ret);        \\208         if (unlikely(err)) {                                                   \\209                 pr_err(\"TDH_VP_RD[\"#uclass\".0x%x] failed: %s (0x%llx)\\n\",      \\210                        field, tdx_seamcall_error_name(err), err);              \\211                 return 0;                                                      \\212         }                                                                      \\213         return (u##bits)ex_ret.regs.r8;                                        \\214 }                                                                              \\215 static __always_inline void td_##lclass##_write##bits(struct vcpu_tdx *tdx,    \\216                                                       u32 field, u##bits val)  \\217 {                                                                              \\218         struct tdx_ex_ret ex_ret;                                              \\219         u64 err;                                                               \\220                                                                                \\221         tdvps_##lclass##_check(field, bits);                                   \\222         err = tdh_vp_wr(tdx-&gt;tdvpr.pa, TDVPS_##uclass(field), val,             \\223                       GENMASK_ULL(bits - 1, 0), &amp;ex_ret);                      \\224         if (unlikely(err))                                                     \\225                 pr_err(\"TDH_VP_WR[\"#uclass\".0x%x] = 0x%llx failed: %s (0x%llx)\\n\", \\226                        field, (u64)val, tdx_seamcall_error_name(err), err);    \\227 }                                                                              \\228 static __always_inline void td_##lclass##_setbit##bits(struct vcpu_tdx *tdx,   \\229                                                        u32 field, u64 bit)     \\230 {                                                                              \\231         struct tdx_ex_ret ex_ret;                                              \\232         u64 err;                                                               \\233                                                                                \\234         tdvps_##lclass##_check(field, bits);                                   \\235         err = tdh_vp_wr(tdx-&gt;tdvpr.pa, TDVPS_##uclass(field), bit, bit,        \\236                         &amp;ex_ret);                                              \\237         if (unlikely(err))                                                     \\238                 pr_err(\"TDH_VP_WR[\"#uclass\".0x%x] |= 0x%llx failed: %s (0x%llx)\\n\", \\239                        field, bit, tdx_seamcall_error_name(err), err);         \\240 }                                                                              \\241 static __always_inline void td_##lclass##_clearbit##bits(struct vcpu_tdx *tdx, \\242                                                          u32 field, u64 bit)   \\243 {                                                                              \\244         struct tdx_ex_ret ex_ret;                                              \\245         u64 err;                                                               \\246                                                                                \\247         tdvps_##lclass##_check(field, bits);                                   \\248         err = tdh_vp_wr(tdx-&gt;tdvpr.pa, TDVPS_##uclass(field), 0, bit,          \\249                         &amp;ex_ret);                                              \\250         if (unlikely(err))                                                     \\251                 pr_err(\"TDH_VP_WR[\"#uclass\".0x%x] &amp;= ~0x%llx failed: %s (0x%llx)\\n\", \\252                        field, bit, tdx_seamcall_error_name(err), err);         \\253 }254255 TDX_BUILD_TDVPS_ACCESSORS(16, VMCS, vmcs);256 TDX_BUILD_TDVPS_ACCESSORS(32, VMCS, vmcs);257 TDX_BUILD_TDVPS_ACCESSORS(64, VMCS, vmcs);258 259 TDX_BUILD_TDVPS_ACCESSORS(64, APIC, apic);260 TDX_BUILD_TDVPS_ACCESSORS(64, GPR, gpr);261 TDX_BUILD_TDVPS_ACCESSORS(64, DR, dr);262 TDX_BUILD_TDVPS_ACCESSORS(64, STATE, state);263 TDX_BUILD_TDVPS_ACCESSORS(64, STATE_NON_ARCH, state_non_arch);264 TDX_BUILD_TDVPS_ACCESSORS(64, MSR, msr);265 TDX_BUILD_TDVPS_ACCESSORS(8, MANAGEMENT, management);The defined macro functions are also utilized by the vmread and vmwrite macro functions 19 #define VT_BUILD_VMCS_HELPERS(type, bits, tdbits)                          \\ 20 static __always_inline type vmread##bits(struct kvm_vcpu *vcpu,            \\ 21                                          unsigned long field)              \\ 22 {                                                                          \\ 23         if (unlikely(is_td_vcpu(vcpu))) {                                  \\ 24                 if (KVM_BUG_ON(!is_debug_td(vcpu), vcpu-&gt;kvm))             \\ 25                         return 0;                                          \\ 26                 return td_vmcs_read##tdbits(to_tdx(vcpu), field);          \\ 27         }                                                                  \\ 28         return vmcs_read##bits(field);                                     \\ 29 }                                                                          \\ 30 static __always_inline void vmwrite##bits(struct kvm_vcpu *vcpu,           \\ 31                                           unsigned long field, type value) \\ 32 {                                                                          \\ 33         if (unlikely(is_td_vcpu(vcpu))) {                                  \\ 34                 if (KVM_BUG_ON(!is_debug_td(vcpu), vcpu-&gt;kvm))             \\ 35                         return;                                            \\ 36                 return td_vmcs_write##tdbits(to_tdx(vcpu), field, value);  \\ 37         }                                                                  \\ 38         vmcs_write##bits(field, value);                                    \\ 39 }"
  },
  
  {
    "title": "TD VM Life Cycle Part 1",
    "url": "/posts/TD-VM-LIFECYCLE-1/",
    "categories": "Confidential Computing, Intel TDX",
    "tags": "",
    "date": "2023-04-01 00:00:00 -0400",
    





    
    "snippet": "Deep dive into TD-VM creation (TDH_MNG_CREATE SEAMCALL-TDH_MNG_INIT)This article will follow the steps described in this figure. It is good to checkthis figure when you want to check which part of ...",
    "content": "Deep dive into TD-VM creation (TDH_MNG_CREATE SEAMCALL-TDH_MNG_INIT)This article will follow the steps described in this figure. It is good to checkthis figure when you want to check which part of the TD VM creation you aredealing with. Before we delve into the details, lets first check new data structures required to generate and initialize the TD VM: TDR, TDCS.Trust Domain Root (TDR)  TDR is the root control structure of a guest TD. As designed, TDR is encrypted using the Intel TDX global private HKID. It holds a minimal set of state variables that enable guest TD control even during times when the TD’s privateHKID is not known, or when the TD’s key management state does not permit accessto memory encrypted using the TD’s private key. It is designed to be the firstTD page to be allocated and the last to be removed. Its physical address servesas a unique identifier of the TD, as long as any TD page or control structure resides in memoryThe host VMM creates a new guest TD by TDH.MNG.CREATE SEAMCALL which initialize a TD Root(TDR) control structure. TDR is the memory page that can distinguish one TD VM from the others. Also, it is the most important page in TD VM creation because it is creating identity of TD VM instance.Trust Domain Control Structure (TDCS)[[https://github.gatech.edu/sslab/tdx/blob/main/img/TD_CONTROL_STRUCTURE.png]]  TDCS is the main control structure of a guest TD. As designed, TDCS is encrypted using the guest TD’s ephemeral private key. TDCS is a multi-page logical structure composed of multiple TDCX physical pages. At a high level, TDCS holds the following information:      EPTP: a pointer (HPA) to the TD’s secure EPT root page and EPT attributes.    Fields related to TD measurement.    MSR bitmaps: limiting capabilities of all TD’s VCPUs.    Fields controlling the TD operation as a whole (e.g., the number of VCPUs  currently running).    Fields controlling the TD’s execution control (CPU features available to the  TD, etc.).    A page filled with zeros: used in cases where the Intel TDX module needs a  read-only constant-0 page encrypted with the TD’s private key.  Also, each TD VM requires the HKID to tag memory accesses and encrypt/decrypt private memory of the TD. Recall that the key should be within the private key range to not be exposed to the host VMM (refer to XXX)Create TD VM (KVM_CREATE_VM (QEMU) -&gt; TDH_MNG_CREATE (TDX Module)static struct kvm_x86_ops vt_x86_ops __initdata = {        ......        .vm_init = vt_vm_init,}int kvm_arch_init_vm(struct kvm *kvm, unsigned long type){        int ret;        if (!static_call(kvm_x86_is_vm_type_supported)(type))                return -EINVAL;        ......        return static_call(kvm_x86_vm_init)(kvm);}As part of the kvm_create_vm, the main function of creating all guest VM, thekvm_arch_init_vm function is invoked to initiate generated KVM structure. This further invokes vt_vm_init assigned as vm_init operation of kvm_x86_ops. 123 static int vt_vm_init(struct kvm *kvm) 124 { 125         if (kvm-&gt;arch.vm_type == KVM_X86_TDX_VM) 126                 return tdx_vm_init(kvm); 127  128         return vmx_vm_init(kvm); 129 }Based on the VM type, whether it is TD VM or vanilla VM, it invokes different initialization functions. Before the TD VM initialization through SEAMCALL (KVM_TDX_INIT_VM), KVM needs to prepare TDR and TDCS structures to instantiateand initialize the TD.static int tdx_vm_init(struct kvm *kvm)                                         {                     /* TODO: test 1GB support and remove tdp_max_page_level */        kvm-&gt;arch.tdp_max_page_level = PG_LEVEL_2M;        /* vCPUs can't be created until after KVM_TDX_INIT_VM. */        kvm-&gt;max_vcpus = 0;         kvm_tdx-&gt;hkid = tdx_keyid_alloc();        if (kvm_tdx-&gt;hkid &lt; 0)                 return -EBUSY;       kvm_tdx-&gt;misc_cg = get_current_misc_cg();       ret = misc_cg_try_charge(MISC_CG_RES_TDX, kvm_tdx-&gt;misc_cg, 1);       if (ret)               goto free_hkid;        ret = tdx_alloc_td_page(&amp;kvm_tdx-&gt;tdr);        if (ret)                goto free_hkid;        kvm_tdx-&gt;tdcs = kcalloc(tdx_caps.tdcs_nr_pages, sizeof(*kvm_tdx-&gt;tdcs),                                GFP_KERNEL_ACCOUNT);        if (!kvm_tdx-&gt;tdcs)                goto free_tdr;        for (i = 0; i &lt; tdx_caps.tdcs_nr_pages; i++) {                ret = tdx_alloc_td_page(&amp;kvm_tdx-&gt;tdcs[i]);                if (ret)                        goto free_tdcs;        }    KVM introduces the kvm_tdx structure to manage each TD instance. Although theTDR and TDCS pages are filled out by the TDX Module, the physical pages used forthose two structures should be ready by the host KVM and passed to the TDX Module during the VM creation and initialization.        mutex_lock(&amp;tdx_lock);        err = tdh_mng_create(kvm_tdx-&gt;tdr.pa, kvm_tdx-&gt;hkid);        mutex_unlock(&amp;tdx_lock);        if (WARN_ON_ONCE(err)) {                pr_tdx_error(TDH_MNG_CREATE, err, NULL);                ret = -EIO;                goto free_tdcs;        }        tdx_mark_td_page_added(&amp;kvm_tdx-&gt;tdr);After the physical page allocations, it invokes TDH_MNG_CREATE SEAMCALL with passing physical address of TDR page and the HKID as its input.TDX Module sidePrimary job of the TDX module for TDH_MNG_CREATE SEAMCALL is mapping the passedTDR page as private and verifies that the passed key can be exclusively used for private memory of the TD VM. Let’s see how it maps the physical page passed fromthe VMM, which is not secure, to the private page of the TD VM.api_error_type tdh_mng_create(uint64_t target_tdr_pa, hkid_api_input_t hkid_info){    ......    return_val = check_lock_and_map_explicit_tdr(tdr_pa,                                                 OPERAND_ID_RCX,                                                 TDX_RANGE_RW,                                                 TDX_LOCK_EXCLUSIVE,                                                 PT_NDA,                                                 &amp;tdr_pamt_block,                                                 &amp;tdr_pamt_entry_ptr,                                                 &amp;tdr_locked_flag,                                                 &amp;tdr_ptr);    ......}api_error_type check_lock_and_map_explicit_tdr(        pa_t tdr_hpa,        uint64_t operand_id,        mapping_type_t mapping_type,        lock_type_t lock_type,        page_type_t expected_pt,        pamt_block_t* pamt_block,        pamt_entry_t** pamt_entry,        bool_t* is_locked,        tdr_t** tdr_p        ){    return check_lock_and_map_explicit_private_4k_hpa(tdr_hpa, operand_id, NULL, mapping_type,            lock_type, expected_pt, pamt_block, pamt_entry, is_locked, (void**)tdr_p);}   api_error_type check_lock_and_map_explicit_private_4k_hpa(        pa_t hpa,        uint64_t operand_id,        tdr_t* tdr_p,        mapping_type_t mapping_type,        lock_type_t lock_type,        page_type_t expected_pt,        pamt_block_t* pamt_block,        pamt_entry_t** pamt_entry,        bool_t* is_locked,        void**         la        ){           api_error_type errc;    page_size_t leaf_size;            errc = check_and_lock_explicit_4k_private_hpa( hpa, operand_id,             lock_type, expected_pt, pamt_block, pamt_entry, &amp;leaf_size, is_locked);    if (errc != TDX_SUCCESS)    {           return errc;    }           pa_t hpa_with_hkid;        errc = check_and_assign_hkid_to_hpa(tdr_p, hpa, &amp;hpa_with_hkid);        if (errc != TDX_SUCCESS)    {        TDX_ERROR(\"check_and_assign_hkid_to_hpa failure\\n\");        *is_locked = false;        pamt_unwalk(hpa, *pamt_block, *pamt_entry, lock_type, leaf_size);        return api_error_with_operand_id(errc, operand_id);    }    *la = map_pa((void*)hpa_with_hkid.full_pa, mapping_type);    return TDX_SUCCESS;}The main function for mapping TDR is check_lock_and_map_explicit_private_4k_hpa.It consists of three parts, setting PAMT for the TDR page, assign HKID to the TDR hpa, and mapping the physical page. To map TDR hpa, TDX module utilizes theglobal HKID of the TDX Module instead of the HKID of the target TD that owns TDR.The key difference is the first part, setting PAMT. And the last part, mappingphysical page, we already covered [here.]Retrieving PAMT entry mapped to TDRapi_error_code_e non_shared_hpa_metadata_check_and_lock(        pa_t hpa,        lock_type_t lock_type,        page_type_t expected_pt,        pamt_block_t* pamt_block,        pamt_entry_t** pamt_entry,        page_size_t*   leaf_size,        bool_t walk_to_leaf_size        ){    // 1) Check that the operand’s HPA is within a TDMR (Trust Domain Memory Range) which is covered by a PAMT.    if (!pamt_get_block(hpa, pamt_block))    {        TDX_ERROR(\"pamt_get_block error hpa = 0x%llx\\n\", hpa.raw);        return TDX_OPERAND_ADDR_RANGE_ERROR;    }    page_size_t requested_leaf_size = *leaf_size;    // 2) Find the PAMT entry for the page and verify that its metadata is as expected.    pamt_entry_t* pamt_entry_lp = pamt_walk(hpa, *pamt_block, lock_type, leaf_size, walk_to_leaf_size);    if (pamt_entry_lp == NULL)    {        TDX_ERROR(\"pamt_walk error\\n\");        return TDX_OPERAND_BUSY;    }    if (walk_to_leaf_size &amp;&amp; (requested_leaf_size != *leaf_size))    {        TDX_ERROR(\"PAMT entry level = %d , Expected level = %d\\n\", *leaf_size, requested_leaf_size);        pamt_unwalk(hpa, *pamt_block, pamt_entry_lp, lock_type, *leaf_size);        return TDX_PAGE_METADATA_INCORRECT;    }    if (pamt_entry_lp-&gt;pt != expected_pt)    {        TDX_ERROR(\"pamt_entry_lp-&gt;pt = %d , expected_pt = %d\\n\", pamt_entry_lp-&gt;pt, expected_pt);        pamt_unwalk(hpa, *pamt_block, pamt_entry_lp, lock_type, *leaf_size);        return TDX_PAGE_METADATA_INCORRECT;    }    *pamt_entry = pamt_entry_lp;    return TDX_SUCCESS;}Regarding setting PAMT, it should first check the physical address of the TDR iswithin one of the initialized TDMR. If the check passes, the base addresses of the three different levels of PAMT will be retrieved from the TDMR that covers the TDR PA. The next step is walking the PAMT table and retrieve the PAMT entrymapped with the TDR PA.pamt_entry_t* pamt_walk(pa_t pa, pamt_block_t pamt_block, lock_type_t leaf_lock_type,                        page_size_t* leaf_size, bool_t walk_to_leaf_size){    pamt_entry_t* pamt_1gb = map_pa_with_global_hkid(pamt_block.pamt_1gb_p, TDX_RANGE_RW);    pamt_entry_t* pamt_2mb = map_pa_with_global_hkid(&amp;pamt_block.pamt_2mb_p[pa.pamt_2m.idx], TDX_RANGE_RW);    pamt_entry_t* pamt_4kb = map_pa_with_global_hkid(&amp;pamt_block.pamt_4kb_p[pa.pamt_4k.idx], TDX_RANGE_RW);    pamt_entry_t* ret_entry_pp = NULL;    pamt_entry_t* ret_entry_lp = NULL;    page_size_t target_size = walk_to_leaf_size ? *leaf_size : PT_4KB;    // Acquire PAMT 1GB entry lock as shared    if (acquire_sharex_lock_sh(&amp;pamt_1gb-&gt;entry_lock) != LOCK_RET_SUCCESS)    {        goto EXIT;    }    // Return pamt_1g entry if it is currently a leaf entry    if ((pamt_1gb-&gt;pt == PT_REG) || (target_size == PT_1GB))    {        // Promote PAMT lock to exclusive if needed        if ((leaf_lock_type == TDX_LOCK_EXCLUSIVE) &amp;&amp; !promote_sharex_lock(&amp;pamt_1gb-&gt;entry_lock))        {            goto EXIT_FAILURE_RELEASE_ROOT;        }        *leaf_size = PT_1GB;        ret_entry_pp = pamt_block.pamt_1gb_p;        goto EXIT;    }    // Acquire PAMT 2MB entry lock as shared    if (acquire_sharex_lock_sh(&amp;pamt_2mb-&gt;entry_lock) != LOCK_RET_SUCCESS)    {        goto EXIT_FAILURE_RELEASE_ROOT;    }    // Return pamt_2m entry if it is leaf    if ((pamt_2mb-&gt;pt == PT_REG) || (target_size == PT_2MB))    {        // Promote PAMT lock to exclusive if needed        if ((leaf_lock_type == TDX_LOCK_EXCLUSIVE) &amp;&amp; !promote_sharex_lock(&amp;pamt_2mb-&gt;entry_lock))        {            goto EXIT_FAILURE_RELEASE_ALL;        }        *leaf_size = PT_2MB;        ret_entry_pp = &amp;pamt_block.pamt_2mb_p[pa.pamt_2m.idx];        goto EXIT;    }    // Acquire PAMT 4KB entry lock as shared/exclusive based on the lock flag    if (acquire_sharex_lock(&amp;pamt_4kb-&gt;entry_lock, leaf_lock_type) != LOCK_RET_SUCCESS)    {        goto EXIT_FAILURE_RELEASE_ALL;    }    *leaf_size = PT_4KB;    ret_entry_pp = &amp;pamt_block.pamt_4kb_p[pa.pamt_4k.idx];    goto EXIT;EXIT_FAILURE_RELEASE_ALL:    // Release PAMT 2MB shared lock    release_sharex_lock_sh(&amp;pamt_2mb-&gt;entry_lock);EXIT_FAILURE_RELEASE_ROOT:    // Release PAMT 1GB shared lock    release_sharex_lock_sh(&amp;pamt_1gb-&gt;entry_lock);EXIT:    free_la(pamt_1gb);    free_la(pamt_2mb);    free_la(pamt_4kb);    if (ret_entry_pp != NULL)    {        ret_entry_lp = map_pa_with_global_hkid(ret_entry_pp,                (leaf_lock_type == TDX_LOCK_EXCLUSIVE) ? TDX_RANGE_RW : TDX_RANGE_RO);    }    return ret_entry_lp;}Based on the target page size, different level of PATM will be returned. Becausethe TDR page is a single 4KB page, it does need to walk the PAMT table three times (1GB, 2MB, 4KB) and then return the leaf node. After finding the PAMT entry, its virtual address is mapped and returned.Rest of the TDH_MNG_CREATE on TDX Modulepi_error_type tdh_mng_create(uint64_t target_tdr_pa, hkid_api_input_t hkid_info)    ......    // Mark the HKID entry in the KOT as assigned    global_data-&gt;kot.entries[td_hkid].state = (uint8_t)KOT_STATE_HKID_ASSIGNED;    // Set HKID in the TKT entry    tdr_ptr-&gt;key_management_fields.hkid = td_hkid;    tdr_ptr-&gt;management_fields.lifecycle_state = TD_HKID_ASSIGNED;    // Set the new TDR page PAMT fields    tdr_pamt_entry_ptr-&gt;pt = PT_TDR;    tdr_pamt_entry_ptr-&gt;owner = 0;Now the PAMT entry for the TDR is accessible, so it sets the PAMT for TDR (e.g.,PT_TDR). Also, it checks whether the passed HKID is eligible for use of TD, for example, whether it belongs to the private HKID range. Also, it needs to check KOT to confirm that HKID can be exclusively used. If it is valid HKID, then the passed HKID is stored in the TDR.Program MKTME for TD (TDH_MNG_KEY_CONFIG)To encrypt/decrypt the private pages of the TD, TDX Module should program the HKID and encryption key into the MKTME. Because MKTME exists per package, the TDH.MNG.KEY.CONFIG SEAMCALL should be invoked on each package. When the last package finished programming MKTME, the state of the TD is changed to UNINITIALIZED.Add TDCS Pages for TD (TDH_MNG_ADDCX)After configuring the key, TDCX page should be added for the TD. Newly addedTDCX page is utilized for constructing TDCS. Recall that TDCS is a logical concept and consists of multiple physical TDCX pages. Also, when the TDCS pagesare mapped in the TDX module, it is accessible from the TDR of the specific VM.It would be good to think of TDCS as a child of TDR. To add the TDCX pages, theKVM host prepares physical page that can be mapped ad TDCX by the TDX Module. TDX module checks the validity of this physical page and maps with the HKID assigned for the target TD, not the global HKID of the TDX Module.        for (i = 0; i &lt; tdx_caps.tdcs_nr_pages; i++) {                err = tdh_mng_addcx(kvm_tdx-&gt;tdr.pa, kvm_tdx-&gt;tdcs[i].pa);                if (WARN_ON_ONCE(err)) {                        pr_tdx_error(TDH_MNG_ADDCX, err, NULL);                        ret = -EIO;                        goto teardown;                }                tdx_mark_td_page_added(&amp;kvm_tdx-&gt;tdcs[i]);        }Also, the number of TDCX pages required for the TD is enumerated by the TDH.SYS.INFO, which means multiple TDH_MNG_ADDCX SEAMCALL can be called to add TDCX pages.Initialize TDCS (TDH_MNG_INIT)Previously we added TDCX pages to the TD, but note that TDCX has logically meaningful usage, Trust Domain Control Structure. To utilize the TDCX pages as TDCS, it should be initialized, and the TDH_MNG_INIT SEAMCALL does this job. This initialization includes set an EPML4 page in one of the previously added TDCX pages as the root page of the secure EPT (TDCS.EPTP).Host KVM passes TD_PARAMS to TDX Modulestruct td_params {        u64 attributes;         u64 xfam;        u32 max_vcpus;        u32 reserved0;                u64 eptp_controls;        u64 exec_controls;        u16 tsc_frequency;        u8  reserved1[38];                u64 mrconfigid[6];        u64 mrowner[6];        u64 mrownerconfig[6];        u64 reserved2[4];                        union {                         struct tdx_cpuid_value cpuid_values[0];                u8 reserved3[768];        };      } __packed __aligned(1024);  TD_PARAMS is provided as an input to TDH.MNG.INIT, and some of its fields are included in the TD report. The format of this structure is valid for a specificMAJOR_VERSION of the Intel TDX module, as reported by TDH.SYS.INFOstatic int tdx_td_init(struct kvm *kvm, struct kvm_tdx_cmd *cmd) {        ......        td_params = kzalloc(sizeof(struct td_params), GFP_KERNEL);        if (!td_params) {                ret = -ENOMEM;                goto out;        }        ret = setup_tdparams(kvm, td_params, init_vm);        if (ret)                goto out;        err = tdh_mng_init(kvm_tdx-&gt;tdr.pa, __pa(td_params), &amp;out);        if (WARN_ON_ONCE(err)) {                pr_tdx_error(TDH_MNG_INIT, err, &amp;out);                ret = -EIO;                goto out;        }        kvm_tdx-&gt;tsc_offset = td_tdcs_exec_read64(kvm_tdx, TD_TDCS_EXEC_TSC_OFFSET);        kvm_tdx-&gt;attributes = td_params-&gt;attributes;        kvm_tdx-&gt;xfam = td_params-&gt;xfam;        kvm_tdx-&gt;tsc_khz = TDX_TSC_25MHZ_TO_KHZ(td_params-&gt;tsc_frequency);\tkvm-&gt;max_vcpus = td_params-&gt;max_vcpus;        if (td_params-&gt;exec_controls &amp; TDX_EXEC_CONTROL_MAX_GPAW)                kvm-&gt;arch.gfn_shared_mask = gpa_to_gfn(BIT_ULL(51));        else                kvm-&gt;arch.gfn_shared_mask = gpa_to_gfn(BIT_ULL(47));}The main job of tdx_td_init function is invoking TDH.MNG.INIT SEAMCALL. For successful initialization of the TD VM, it requires TD_PARAMS which contains allinformation such as measurement, tsc frequency, &amp;c. Most of the information to build TD_PARAMS are provided by the QEMU. Let’s take a look at how the TDX Module initializes the rest of the data structures.Map TDCS pages for initializationTo initializes the TDCS, the previously added TDCX pages should be accessible as the form of TDCS. Note that TDR maintains physical addresses of the TDCX.map_implicit_tdcs function maps the TDCX pages and return the tdcs_t pointer. Note that it doesn’t map the 4th page because it is used for PASID usage.typedef struct ALIGN(TDX_PAGE_SIZE_IN_BYTES) tdcs_s{       /**     * TDCX First page - Management structures            */    tdcs_management_fields_t               management_fields;    tdcs_execution_control_fields_t        executions_ctl_fields;    /**     * Needs to be 128bit (16 byte) aligned for atomic cmpxchg     */    tdcs_epoch_tracking_fields_t ALIGN(16) epoch_tracking;    tdcs_measurement_fields_t              measurement_fields;        uint64_t                     notify_enables; // Enable guest notification of events        /**     * TDCX 2nd page - MSR Bitmaps     */    uint8_t ALIGN(TDX_PAGE_SIZE_IN_BYTES)                          MSR_BITMAPS[TDX_PAGE_SIZE_IN_BYTES]; /**&lt; TD-scope RDMSR/WRMSR exit control bitmaps */        /**      * TDCX 3rd page - Secure EPT Root Page     */    uint8_t ALIGN(TDX_PAGE_SIZE_IN_BYTES)                          sept_root_page[TDX_PAGE_SIZE_IN_BYTES];        /**     * TDCX 4th page - Zero Page     */     uint8_t ALIGN(TDX_PAGE_SIZE_IN_BYTES)                          zero_page[TDX_PAGE_SIZE_IN_BYTES];} tdcs_t;As shown in the above code, the TDCS consists of four TDCX pages and each fieldis initialized during handling the TDH_MNG_INIT SEAMCALL.Initialize TDCS fields    /**      *  Read the TD configuration input and set TDCS fields     */    uint16_t virt_tsc_freq;    return_val = read_and_set_td_configurations(tdcs_ptr,                                                td_params_ptr,                                                MAX_PA,                                                tdr_ptr-&gt;management_fields.tdcx_pa[SEPT_ROOT_PAGE_INDEX],                                                &amp;virt_tsc_freq);read_and_set_td_configurations configures most of the TDCS member fields, so it is hard to cover everything. Let’s focus on some important TDCS member fieldssuch as MSR bitmaps, TDCS measurement, and EPTP pointer controllingguest TD’s GPA-&gt;HPA translation.    // Read and verify EPTP_CONTROLS    target_eptp.raw = td_params_ptr-&gt;eptp_controls.raw;    if ((target_eptp.fields.ept_ps_mt != MT_WB) ||        (target_eptp.fields.ept_pwl &lt; LVL_PML4) ||        (target_eptp.fields.ept_pwl &gt; LVL_PML5) ||        (target_eptp.fields.enable_ad_bits != 0) ||        (target_eptp.fields.enable_sss_control != 0) ||        (target_eptp.fields.reserved_0 != 0) ||        (target_eptp.fields.base_pa != 0) ||        (target_eptp.fields.reserved_1 != 0))    {        return_val = api_error_with_operand_id(TDX_OPERAND_INVALID, OPERAND_ID_EPTP_CONTROLS);        goto EXIT;    }    ,,,,,,    /**      *  The PA field of EPTP points to the Secure EPT root page in TDCS,     *  which has already been initialized to 0 during TDADDCX     */    target_eptp.fields.base_pa = sept_root_pa.page_4k_num;        tdcs_ptr-&gt;executions_ctl_fields.eptp.raw = target_eptp.raw;Based on the provided TD_PARAMS, it sets control bits of the EPTP first. After validating the control bits, it sets the EPTP root page table address, which isthe third TDCS page (tdcx_pa[SEPT_ROOT_PAGE_INDEX]). The populated EPTP info will be maintained in the first TDCS page, especially, executions_ctl_field.eptp.KVM_TDX_CAPABILITIESCurrently, KVM_TDX_CAPABILITIES is the only ioctl function supported through thetdx device ioctl.int tdx_dev_ioctl(void __user *argp){        struct kvm_tdx_capabilities __user *user_caps;        struct kvm_tdx_capabilities caps;        struct kvm_tdx_cmd cmd;        BUILD_BUG_ON(sizeof(struct kvm_tdx_cpuid_config) !=                     sizeof(struct tdx_cpuid_config));        if (copy_from_user(&amp;cmd, argp, sizeof(cmd)))                return -EFAULT;        if (cmd.flags || cmd.error || cmd.unused)                return -EINVAL;        /*         * Currently only KVM_TDX_CAPABILITIES is defined for system-scoped         * mem_enc_ioctl().         */        if (cmd.id != KVM_TDX_CAPABILITIES)                return -EINVAL;        user_caps = (void __user *)cmd.data;        if (copy_from_user(&amp;caps, user_caps, sizeof(caps)))                return -EFAULT;        if (caps.nr_cpuid_configs &lt; tdx_caps.nr_cpuid_configs)                return -E2BIG;        caps = (struct kvm_tdx_capabilities) {                .attrs_fixed0 = tdx_caps.attrs_fixed0,                .attrs_fixed1 = tdx_caps.attrs_fixed1,                .xfam_fixed0 = tdx_caps.xfam_fixed0,                .xfam_fixed1 = tdx_caps.xfam_fixed1,                .nr_cpuid_configs = tdx_caps.nr_cpuid_configs,                .padding = 0,        };        if (copy_to_user(user_caps, &amp;caps, sizeof(caps)))                return -EFAULT;        if (copy_to_user(user_caps-&gt;cpuid_configs, &amp;tdx_caps.cpuid_configs,                         tdx_caps.nr_cpuid_configs *                         sizeof(struct tdx_cpuid_config)))                return -EFAULT;        return 0;}KVM retrieve and store TDX capabilities informationTo return the proper information about TDX, KVM module should have invoked TDCALL to tdx module and memorize all required information beforehand./* Capabilities of KVM + the TDX module. */static struct tdx_capabilities tdx_caps;int __init tdx_module_setup(void){        const struct tdsysinfo_struct *tdsysinfo;        int ret = 0;        BUILD_BUG_ON(sizeof(*tdsysinfo) != 1024);        BUILD_BUG_ON(TDX_MAX_NR_CPUID_CONFIGS != 37);        ret = tdx_init();        if (ret) {                pr_info(\"Failed to initialize TDX module.\\n\");                return ret;        }        tdx_global_keyid = tdx_get_global_keyid();        tdsysinfo = tdx_get_sysinfo();        if (tdsysinfo-&gt;num_cpuid_config &gt; TDX_MAX_NR_CPUID_CONFIGS)                return -EIO;        tdx_caps = (struct tdx_capabilities) {                .tdcs_nr_pages = tdsysinfo-&gt;tdcs_base_size / PAGE_SIZE,                /*                 * TDVPS = TDVPR(4K page) + TDVPX(multiple 4K pages).                 * -1 for TDVPR.                 */                .tdvpx_nr_pages = tdsysinfo-&gt;tdvps_base_size / PAGE_SIZE - 1,                .attrs_fixed0 = tdsysinfo-&gt;attributes_fixed0,                .attrs_fixed1 = tdsysinfo-&gt;attributes_fixed1,                .xfam_fixed0 =  tdsysinfo-&gt;xfam_fixed0,                .xfam_fixed1 = tdsysinfo-&gt;xfam_fixed1,                .nr_cpuid_configs = tdsysinfo-&gt;num_cpuid_config,        };        if (!memcpy(tdx_caps.cpuid_configs, tdsysinfo-&gt;cpuid_configs,                        tdsysinfo-&gt;num_cpuid_config *                        sizeof(struct tdx_cpuid_config)))                return -EIO;        return 0;}struct tdx_capabilities {        u8 tdcs_nr_pages;        u8 tdvpx_nr_pages;        u64 attrs_fixed0;        u64 attrs_fixed1;        u64 xfam_fixed0;        u64 xfam_fixed1;        u32 nr_cpuid_configs;        struct tdx_cpuid_config cpuid_configs[TDX_MAX_NR_CPUID_CONFIGS];};tdx_capabilities struct selectively contains the information provided through tdsysinfo.tdsysinfo can be retrieved from the TDX module through the tdcall as shown in the below. Host KVM retrieves the tdsysinfo as a result of TDH_SYS_INFO SEAMCALL at the time of init_tdx_module."
  },
  
  {
    "title": "Gem5 Interrupt Handling O3",
    "url": "/posts/gem5-interrupt-handling-o3/",
    "categories": "",
    "tags": "",
    "date": "2021-12-25 00:00:00 -0500",
    





    
    "snippet": "Interrupt makes the commit stage stop further instructions fetching 813 template &lt;class Impl&gt; 814 void 815 DefaultCommit&lt;Impl&gt;::commit() 816 { 817     if (FullSystem) { 818         // C...",
    "content": "Interrupt makes the commit stage stop further instructions fetching 813 template &lt;class Impl&gt; 814 void 815 DefaultCommit&lt;Impl&gt;::commit() 816 { 817     if (FullSystem) { 818         // Check if we have a interrupt and get read to handle it 819         if (cpu-&gt;checkInterrupts(cpu-&gt;tcBase(0))) 820             propagateInterrupt(); 821     } 788 template &lt;class Impl&gt; 789 void 790 DefaultCommit&lt;Impl&gt;::propagateInterrupt() 791 { 792     // Don't propagate intterupts if we are currently handling a trap or 793     // in draining and the last observable instruction has been committed. 794     if (commitStatus[0] == TrapPending || interrupt || trapSquash[0] || 795             tcSquash[0] || drainImminent) 796         return; 797  798     // Process interrupts if interrupts are enabled, not in PAL 799     // mode, and no other traps or external squashes are currently 800     // pending. 801     // @todo: Allow other threads to handle interrupts. 802  803     // Get any interrupt that happened 804     interrupt = cpu-&gt;getInterrupts(); 805  806     // Tell fetch that there is an interrupt pending.  This 807     // will make fetch wait until it sees a non PAL-mode PC, 808     // at which point it stops fetching instructions. 809     if (interrupt != NoFault) 810         toIEW-&gt;commitInfo[0].interruptPending = true; 811 }If the commit stage found that the interrupt needs to be handled,through the getInterrupts function,it should first send signal to IEW stage and prevent further instruction fetching until the interrupt is resolved.Handle interrupt at the time of instruction commitAfter commit stage checks possible squash on the threads and presence of interrupt, it tries to commit the instructions. 976 template &lt;class Impl&gt; 977 void 978 DefaultCommit&lt;Impl&gt;::commitInsts() 979 { 980     //////////////////////////////////// 981     // Handle commit 982     // Note that commit will be handled prior to putting new 983     // instructions in the ROB so that the ROB only tries to commit 984     // instructions it has in this current cycle, and not instructions 985     // it is writing in during this cycle.  Can't commit and squash 986     // things at the same time... 987     //////////////////////////////////// 988  989     DPRINTF(Commit, \"Trying to commit instructions in the ROB.\\n\"); 990  991     unsigned num_committed = 0; 992  993     DynInstPtr head_inst; 994  995     // Commit as many instructions as possible until the commit bandwidth 996     // limit is reached, or it becomes impossible to commit any more. 997     while (num_committed &lt; commitWidth) { 998         // Check for any interrupt that we've already squashed for 999         // and start processing it.1000         if (interrupt != NoFault)1001             handleInterrupt();If there were pending interrupt, the instructions cannot be committed. 734 template &lt;class Impl&gt; 735 void 736 DefaultCommit&lt;Impl&gt;::handleInterrupt() 737 { 738     // Verify that we still have an interrupt to handle 739     if (!cpu-&gt;checkInterrupts(cpu-&gt;tcBase(0))) { 740         DPRINTF(Commit, \"Pending interrupt is cleared by master before \" 741                 \"it got handled. Restart fetching from the orig path.\\n\"); 742         toIEW-&gt;commitInfo[0].clearInterrupt = true; 743         interrupt = NoFault; 744         avoidQuiesceLiveLock = true; 745         return; 746     } 747  748     // Wait until all in flight instructions are finished before enterring 749     // the interrupt. 750     if (canHandleInterrupts &amp;&amp; cpu-&gt;instList.empty()) { 751         // Squash or record that I need to squash this cycle if 752         // an interrupt needed to be handled. 753         DPRINTF(Commit, \"Interrupt detected.\\n\"); 754  755         // Clear the interrupt now that it's going to be handled 756         toIEW-&gt;commitInfo[0].clearInterrupt = true; 757  758         assert(!thread[0]-&gt;noSquashFromTC); 759         thread[0]-&gt;noSquashFromTC = true; 760  761         if (cpu-&gt;checker) { 762             cpu-&gt;checker-&gt;handlePendingInt(); 763         } 764  765         // CPU will handle interrupt. Note that we ignore the local copy of 766         // interrupt. This is because the local copy may no longer be the 767         // interrupt that the interrupt controller thinks is being handled. 768         cpu-&gt;processInterrupts(cpu-&gt;getInterrupts()); 769  770         thread[0]-&gt;noSquashFromTC = false; 771  772         commitStatus[0] = TrapPending; 773  774         interrupt = NoFault; 775  776         // Generate trap squash event. 777         generateTrapEvent(0, interrupt); 778  779         avoidQuiesceLiveLock = false; 780     } else { 781         DPRINTF(Commit, \"Interrupt pending: instruction is %sin \" 782                 \"flight, ROB is %sempty\\n\", 783                 canHandleInterrupts ? \"not \" : \"\", 784                 cpu-&gt;instList.empty() ? \"\" : \"not \" ); 785     } 786 }Architecture dependent interrupt checking614 bool615 X86ISA::Interrupts::checkInterrupts(ThreadContext *tc) const616 {617     RFLAGS rflags = tc-&gt;readMiscRegNoEffect(MISCREG_RFLAGS);618     if (pendingUnmaskableInt) {619         DPRINTF(LocalApic, \"Reported pending unmaskable interrupt.\\n\");620         return true;621     }622     if (rflags.intf) {623         if (pendingExtInt) {624             DPRINTF(LocalApic, \"Reported pending external interrupt.\\n\");625             return true;626         }627         if (IRRV &gt; ISRV &amp;&amp; bits(IRRV, 7, 4) &gt;628                bits(regs[APIC_TASK_PRIORITY], 7, 4)) {629             DPRINTF(LocalApic, \"Reported pending regular interrupt.\\n\");630             return true;631         }632     }633     return false;634 }Processing interrupt through invoke 886 template &lt;class Impl&gt; 887 void     888 FullO3CPU&lt;Impl&gt;::processInterrupts(const Fault &amp;interrupt) 889 {        890     // Check for interrupts here.  For now can copy the code that 891     // exists within isa_fullsys_traits.hh.  Also assume that thread 0 892     // is the one that handles the interrupts. 893     // @todo: Possibly consolidate the interrupt checking code. 894     // @todo: Allow other threads to handle interrupts. 895          896     assert(interrupt != NoFault); 897     this-&gt;interrupts[0]-&gt;updateIntrInfo(this-&gt;threadContexts[0]); 898          899     DPRINTF(O3CPU, \"Interrupt %s being handled\\n\", interrupt-&gt;name()); 900     this-&gt;trap(interrupt, 0, nullptr); 901 }        902 903 template &lt;class Impl&gt; 904 void 905 FullO3CPU&lt;Impl&gt;::trap(const Fault &amp;fault, ThreadID tid, 906                       const StaticInstPtr &amp;inst) 907 {        908     // Pass the thread's TC into the invoke method. 909     fault-&gt;invoke(this-&gt;threadContexts[tid], inst); 910 }      Generate trap event 526 template &lt;class Impl&gt; 527 void 528 DefaultCommit&lt;Impl&gt;::generateTrapEvent(ThreadID tid, Fault inst_fault) 529 { 530     DPRINTF(Commit, \"Generating trap event for [tid:%i]\\n\", tid); 531  532     EventFunctionWrapper *trap = new EventFunctionWrapper( 533         [this, tid]{ processTrapEvent(tid); }, 534         \"Trap\", true, Event::CPU_Tick_Pri); 535  536     Cycles latency = dynamic_pointer_cast&lt;SyscallRetryFault&gt;(inst_fault) ? 537                      cpu-&gt;syscallRetryLatency : trapLatency; 538  539     cpu-&gt;schedule(trap, cpu-&gt;clockEdge(latency)); 540     trapInFlight[tid] = true; 541     thread[tid]-&gt;trapPending = true; 542 }"
  },
  
  {
    "title": "Basecpu",
    "url": "/posts/basecpu/",
    "categories": "GEM5",
    "tags": "",
    "date": "2021-08-08 00:00:00 -0400",
    





    
    "snippet": "gem5/src/cpu/BaseCPU.py 65 if buildEnv['TARGET_ISA'] == 'alpha': 66     from m5.objects.AlphaTLB import AlphaDTB as ArchDTB, AlphaITB as ArchITB 67     from m5.objects.AlphaInterrupts import AlphaI...",
    "content": "gem5/src/cpu/BaseCPU.py 65 if buildEnv['TARGET_ISA'] == 'alpha': 66     from m5.objects.AlphaTLB import AlphaDTB as ArchDTB, AlphaITB as ArchITB 67     from m5.objects.AlphaInterrupts import AlphaInterrupts as ArchInterrupts 68     from m5.objects.AlphaISA import AlphaISA as ArchISA 69     ArchISAsParam = VectorParam.AlphaISA 70 elif buildEnv['TARGET_ISA'] == 'sparc': 71     from m5.objects.SparcTLB import SparcTLB as ArchDTB, SparcTLB as ArchITB 72     from m5.objects.SparcInterrupts import SparcInterrupts as ArchInterrupts 73     from m5.objects.SparcISA import SparcISA as ArchISA 74     ArchISAsParam = VectorParam.SparcISA 75 elif buildEnv['TARGET_ISA'] == 'x86': 76     from m5.objects.X86TLB import X86TLB as ArchDTB, X86TLB as ArchITB 77     from m5.objects.X86LocalApic import X86LocalApic as ArchInterrupts 78     from m5.objects.X86ISA import X86ISA as ArchISA 79     ArchISAsParam = VectorParam.X86ISA 80 elif buildEnv['TARGET_ISA'] == 'mips': 81     from m5.objects.MipsTLB import MipsTLB as ArchDTB, MipsTLB as ArchITB 82     from m5.objects.MipsInterrupts import MipsInterrupts as ArchInterrupts 83     from m5.objects.MipsISA import MipsISA as ArchISA 84     ArchISAsParam = VectorParam.MipsISA 85 elif buildEnv['TARGET_ISA'] == 'arm': 86     from m5.objects.ArmTLB import ArmDTB as ArchDTB, ArmITB as ArchITB 87     from m5.objects.ArmInterrupts import ArmInterrupts as ArchInterrupts 88     from m5.objects.ArmISA import ArmISA as ArchISA 89     ArchISAsParam = VectorParam.ArmISA 90 elif buildEnv['TARGET_ISA'] == 'power': 91     from m5.objects.PowerTLB import PowerTLB as ArchDTB, PowerTLB as ArchITB 92     from m5.objects.PowerInterrupts import PowerInterrupts as ArchInterrupts 93     from m5.objects.PowerISA import PowerISA as ArchISA 94     ArchISAsParam = VectorParam.PowerISA 95 elif buildEnv['TARGET_ISA'] == 'riscv': 96     from m5.objects.RiscvTLB import RiscvTLB as ArchDTB, RiscvTLB as ArchITB 97     from m5.objects.RiscvInterrupts import RiscvInterrupts as ArchInterrupts 98     from m5.objects.RiscvISA import RiscvISA as ArchISA 99     ArchISAsParam = VectorParam.RiscvISA100 else:101     print(\"Don't know what object types to use for ISA %s\" %102             buildEnv['TARGET_ISA'])103     sys.exit(1)104 gem5/src/cpu/BaseCPU.py105 class BaseCPU(ClockedObject):106     type = 'BaseCPU'107     abstract = True108     cxx_header = \"cpu/base.hh\"......143     system = Param.System(Parent.any, \"system object\")144     cpu_id = Param.Int(-1, \"CPU identifier\")145     socket_id = Param.Unsigned(0, \"Physical Socket identifier\")146     numThreads = Param.Unsigned(1, \"number of HW thread contexts\")147     pwr_gating_latency = Param.Cycles(300,148         \"Latency to enter power gating state when all contexts are suspended\")149 150     power_gating_on_idle = Param.Bool(False, \"Control whether the core goes \"\\151         \"to the OFF power state after all thread are disabled for \"\\152         \"pwr_gating_latency cycles\")153 154     function_trace = Param.Bool(False, \"Enable function trace\")155     function_trace_start = Param.Tick(0, \"Tick to start function trace\")156 157     checker = Param.BaseCPU(NULL, \"checker CPU\")158 159     syscallRetryLatency = Param.Cycles(10000, \"Cycles to wait until retry\")160 161     do_checkpoint_insts = Param.Bool(True,162         \"enable checkpoint pseudo instructions\")163     do_statistics_insts = Param.Bool(True,164         \"enable statistics pseudo instructions\")165 166     profile = Param.Latency('0ns', \"trace the kernel stack\")167     do_quiesce = Param.Bool(True, \"enable quiesce instructions\")168 169     wait_for_remote_gdb = Param.Bool(False,170         \"Wait for a remote GDB connection\");171 172     workload = VectorParam.Process([], \"processes to run\")173 174     dtb = Param.BaseTLB(ArchDTB(), \"Data TLB\")175     itb = Param.BaseTLB(ArchITB(), \"Instruction TLB\")As shown in the line 174-175, to instantiate the TLBs for data and instruction,it invokes the constructor of the BaseTLB class. Note that this is not the python class yet, but a python class!Also, X86TLB has been imported as ArchDTB and ArchITB.Therefore, on the above code, BaseTLB instances are populated with X86TLB class.class BaseTLB(SimObject):    type = 'BaseTLB'    abstract = True    cxx_header = \"arch/generic/tlb.hh\"    # Ports to connect with other TLB levels    slave  = VectorSlavePort(\"Port closer to the CPU side\")    master = MasterPort(\"Port closer to memory side\")class X86TLB(BaseTLB):    type = 'X86TLB'    cxx_class = 'X86ISA::TLB'    cxx_header = 'arch/x86/tlb.hh'    size = Param.Unsigned(64, \"TLB size\")    system = Param.System(Parent.any, \"system object\")    walker = Param.X86PagetableWalker(\\            X86PagetableWalker(), \"page table walker\")Because BaseTLB class doesn’t have any constructor definition dedicated forX86TLB class, it will invoke the SimObject’s constructor.Generated Paramsgem5/build/X86/params/BaseTLB.hh  1 #ifndef __PARAMS__BaseTLB__  2 #define __PARAMS__BaseTLB__  3   4 class BaseTLB;  5   6 #include \"params/SimObject.hh\"  7   8 struct BaseTLBParams  9     : public SimObjectParams 10 { 11     unsigned int port_master_connection_count; 12     unsigned int port_slave_connection_count; 13 }; 14  15 #endif // __PARAMS__BaseTLB__gem5/build/X86/params/X86TLB.hh#ifndef __PARAMS__X86TLB__#define __PARAMS__X86TLB__namespace X86ISA {class TLB;} // namespace X86ISA#include &lt;cstddef&gt;#include \"base/types.hh\"#include &lt;cstddef&gt;#include \"params/System.hh\"#include &lt;cstddef&gt;#include \"params/X86PagetableWalker.hh\"#include \"params/BaseTLB.hh\"struct X86TLBParams    : public BaseTLBParams{    X86ISA::TLB * create();    unsigned size;    System * system;    X86ISA::Walker * walker;};#endif // __PARAMS__X86TLB__How to generate params?gem5/src/python/m5/params.py2074 # Port description object.  Like a ParamDesc object, this represents a2075 # logical port in the SimObject class, not a particular port on a2076 # SimObject instance.  The latter are represented by PortRef objects.2077 class Port(object):2078     # Port(\"role\", \"description\")2079 2080     _compat_dict = { }2081 2082     @classmethod2083     def compat(cls, role, peer):2084         cls._compat_dict.setdefault(role, set()).add(peer)2085         cls._compat_dict.setdefault(peer, set()).add(role)2086 2087     @classmethod2088     def is_compat(cls, one, two):2089         for port in one, two:2090             if not port.role in Port._compat_dict:2091                 fatal(\"Unrecognized role '%s' for port %s\\n\", port.role, port)2092         return one.role in Port._compat_dict[two.role]2093 2094     def __init__(self, role, desc, is_source=False):2095         self.desc = desc2096         self.role = role2097         self.is_source = is_source2098 2099     # Generate a PortRef for this port on the given SimObject with the2100     # given name2101     def makeRef(self, simobj):2102         return PortRef(simobj, self.name, self.role, self.is_source)2103 2104     # Connect an instance of this port (on the given SimObject with2105     # the given name) with the port described by the supplied PortRef2106     def connect(self, simobj, ref):2107         self.makeRef(simobj).connect(ref)2108 2109     # No need for any pre-declarations at the moment as we merely rely2110     # on an unsigned int.2111     def cxx_predecls(self, code):2112         pass2113 2114     def pybind_predecls(self, code):2115         cls.cxx_predecls(self, code)2116 2117     # Declare an unsigned int with the same name as the port, that2118     # will eventually hold the number of connected ports (and thus the2119     # number of elements for a VectorPort).2120     def cxx_decl(self, code):2121         code('unsigned int port_$_connection_count;')"
  },
  
  {
    "title": "O3 Cache Block",
    "url": "/posts/O3-cache-block/",
    "categories": "",
    "tags": "",
    "date": "2021-06-11 00:00:00 -0400",
    





    
    "snippet": "Cache internal class hierarchies in GEM5  92 /**  93  * A basic cache interface. Implements some common functions for speed.  94  */  95 class BaseCache : public ClockedObject  96 {...... 349     /...",
    "content": "Cache internal class hierarchies in GEM5  92 /**  93  * A basic cache interface. Implements some common functions for speed.  94  */  95 class BaseCache : public ClockedObject  96 {...... 349     /** Tag and data Storage */ 350     BaseTags *tags; 70 /** 71  * A common base class of Cache tagstore objects. 72  */ 73 class BaseTags : public ClockedObject 74 {...... 88     /** Indexing policy */ 89     BaseIndexingPolicy *indexingPolicy;......102     /** The data blocks, 1 per cache block. */103     std::unique_ptr&lt;uint8_t[]&gt; dataBlks;The main cache structure called BaseCache contains member field tags (object of BaseTags)The BaseTags class contains the actual data blocks for maintaining data to the cache.Also, it has BaseIndexingPolicy that determines which entry should be selected or evicted as a result of cache access operations (including cache read and write).60 /** 61  * A common base class for indexing table locations. Classes that inherit 62  * from it determine hash functions that should be applied based on the set 63  * and way. These functions are then applied to re-map the original values. 64  * @sa  \\ref gem5MemorySystem \"gem5 Memory System\" 65  */ 66 class BaseIndexingPolicy : public SimObject 67 { 68   protected: 69     /** 70      * The associativity. 71      */ 72     const unsigned assoc; 73  74     /** 75      * The number of sets in the cache. 76      */ 77     const uint32_t numSets; 78  79     /** 80      * The amount to shift the address to get the set. 81      */ 82     const int setShift; 83  84     /** 85      * Mask out all bits that aren't part of the set index. 86      */ 87     const unsigned setMask; 88  89     /** 90      * The cache sets. 91      */ 92     std::vector&lt;std::vector&lt;ReplaceableEntry*&gt;&gt; sets;Note that the BaseIndexingPolicy contains sets consisting of multiple entries of ReplaceableEntry. The CacheBlk will be stored in the sets member fieldbecause the CacheBlk is a child of ReplaceableEntry class. Also, sets can be indexed with set and way to provide nomenclature of the cache.CacheBlk the basic unit of each cache block entry 65 /** 66  * A Basic Cache block. 67  * Contains information regarding its coherence, prefetching status, as 68  * well as a pointer to its data. 69  */ 70 class CacheBlk : public TaggedEntry 41 /** 42  * A tagged entry is an entry containing a tag. Each tag is accompanied by a 43  * secure bit, which informs whether it belongs to a secure address space. 44  * A tagged entry's contents are only relevant if it is marked as valid. 45  */ 46 class TaggedEntry : public ReplaceableEntry 53 /** 54  * A replaceable entry is a basic entry in a 2d table-like structure that needs 55  * to have replacement functionality. This entry is located in a specific row 56  * and column of the table (set and way in cache nomenclature), which are 57  * stored within the entry itself. 58  * 59  * It contains the replacement data pointer, which must be instantiated by the 60  * replacement policy before being used. 61  * @sa Replacement Policies 62  */ 63 class ReplaceableEntryThe cache block maintains three important data related with one cache entry:coherence information, prefetching status, and pointer to the data.Therefore, let’s take a look at the structure and interface required for managing those information.Remaining question: Then where the data exactly is stored? in the CacheBlk object? or in the dataBlks member field of the tags?Cache managementInitializing cache blocks  79 BaseCache::BaseCache(const BaseCacheParams &amp;p, unsigned blk_size)  80     : ClockedObject(p),  81       cpuSidePort (p.name + \".cpu_side_port\", this, \"CpuSidePort\"),  82       memSidePort(p.name + \".mem_side_port\", this, \"MemSidePort\"),  83       mshrQueue(\"MSHRs\", p.mshrs, 0, p.demand_mshr_reserve, p.name),  84       writeBuffer(\"write buffer\", p.write_buffers, p.mshrs, p.name),  85       tags(p.tags),  86       compressor(p.compressor),  87       prefetcher(p.prefetcher),  88       writeAllocator(p.write_allocator),  89       writebackClean(p.writeback_clean),  90       tempBlockWriteback(nullptr),  91       writebackTempBlockAtomicEvent([this]{ writebackTempBlockAtomic(); },  92                                     name(), false,  93                                     EventBase::Delayed_Writeback_Pri),  94       blkSize(blk_size),  95       lookupLatency(p.tag_latency),  96       dataLatency(p.data_latency),  97       forwardLatency(p.tag_latency),  98       fillLatency(p.data_latency),  99       responseLatency(p.response_latency), 100       sequentialAccess(p.sequential_access), 101       numTarget(p.tgts_per_mshr), 102       forwardSnoops(true), 103       clusivity(p.clusivity), 104       isReadOnly(p.is_read_only), 105       replaceExpansions(p.replace_expansions), 106       moveContractions(p.move_contractions), 107       blocked(0), 108       order(0), 109       noTargetMSHR(nullptr), 110       missCount(p.max_miss_count), 111       addrRanges(p.addr_ranges.begin(), p.addr_ranges.end()), 112       system(p.system), 113       stats(*this) 114 { 115     // the MSHR queue has no reserve entries as we check the MSHR 116     // queue on every single allocation, whereas the write queue has 117     // as many reserve entries as we have MSHRs, since every MSHR may 118     // eventually require a writeback, and we do not check the write 119     // buffer before committing to an MSHR 120  121     // forward snoops is overridden in init() once we can query 122     // whether the connected requestor is actually snooping or not 123  124     tempBlock = new TempCacheBlk(blkSize); 125  126     tags-&gt;tagsInit(); 127     if (prefetcher) 128         prefetcher-&gt;setCache(this); 129  130     fatal_if(compressor &amp;&amp; !dynamic_cast&lt;CompressedTags*&gt;(tags), 131         \"The tags of compressed cache %s must derive from CompressedTags\", 132         name()); 133     warn_if(!compressor &amp;&amp; dynamic_cast&lt;CompressedTags*&gt;(tags), 134         \"Compressed cache %s does not have a compression algorithm\", name()); 135     if (compressor)When the BaseCache is constructed, it initializes its member field tags by passing the parametersrequired for initializing the BaseTags class.Because we are currently dealing with the BaseSetAssoc class which inherits the BaseTags class,its constructor will be invoked first instead of the BaseTags’s constructor. 55 BaseSetAssoc::BaseSetAssoc(const Params &amp;p) 56     :BaseTags(p), allocAssoc(p.assoc), blks(p.size / p.block_size), 57      sequentialAccess(p.sequential_access), 58      replacementPolicy(p.replacement_policy) 59 { 60     // There must be a indexing policy 61     fatal_if(!p.indexing_policy, \"An indexing policy is required\"); 62  63     // Check parameters 64     if (blkSize &lt; 4 || !isPowerOf2(blkSize)) { 65         fatal(\"Block size must be at least 4 and a power of 2\"); 66     } 67 } 68 69 void 70 BaseSetAssoc::tagsInit() 71 { 72     // Initialize all blocks 73     for (unsigned blk_index = 0; blk_index &lt; numBlocks; blk_index++) { 74         // Locate next cache block 75         CacheBlk* blk = &amp;blks[blk_index]; 76  77         // Link block to indexing policy 78         indexingPolicy-&gt;setEntry(blk, blk_index); 79  80         // Associate a data chunk to the block 81         blk-&gt;data = &amp;dataBlks[blkSize*blk_index]; 82  83         // Associate a replacement data entry to the block 84         blk-&gt;replacementData = replacementPolicy-&gt;instantiateEntry(); 85     } 86 }When the BaseSetAssoc is populated, it initializes the blk member field with (p.size / p.block_size) entries. After that, the constructor of the BaseCache invokes the tagsInit functionimplemented in the BaseSetAssoc class. As shown in the above code, it select one CacheBlk from the blk memeber field of the BaseSetAssoc. 80 void 81 BaseIndexingPolicy::setEntry(ReplaceableEntry* entry, const uint64_t index) 82 { 83     // Calculate set and way from entry index 84     const std::lldiv_t div_result = std::div((long long)index, assoc); 85     const uint32_t set = div_result.quot; 86     const uint32_t way = div_result.rem; 87  88     // Sanity check 89     assert(set &lt; numSets); 90  91     // Assign a free pointer 92     sets[set][way] = entry; 93  94     // Inform the entry its position 95     entry-&gt;setPosition(set, way); 96 }The setEntry function of the indexingPolicy will be invoked next so that it associates the CacheBlk in the Tag class to the sets member field of the indexingPolicyMap datablk to cacheblkWhen the BaseSetAssoc object is constructed, it invoked its parent’s constructor BaseTags with the parameter. 61 BaseTags::BaseTags(const Params &amp;p) 62     : ClockedObject(p), blkSize(p.block_size), blkMask(blkSize - 1), 63       size(p.size), lookupLatency(p.tag_latency), 64       system(p.system), indexingPolicy(p.indexing_policy), 65       warmupBound((p.warmup_percentage/100.0) * (p.size / p.block_size)), 66       warmedUp(false), numBlocks(p.size / p.block_size), 67       dataBlks(new uint8_t[p.size]), // Allocate data storage in one big chunk 68       stats(*this) 69 { 70     registerExitCallback([this]() { cleanupRefs(); }); 71 }You might remember that the BaseTags class contains dataBlks member field.Here the constructor initializes the dataBlks to have (p.size / p.block_size) entries for storing the cache data. Note that the number of entries of the dataBlks and the blk are samebecause one dataBlk is associated with one CacheBlk. 69 void 70 BaseSetAssoc::tagsInit() 71 { 72     // Initialize all blocks 73     for (unsigned blk_index = 0; blk_index &lt; numBlocks; blk_index++) { 74         // Locate next cache block 75         CacheBlk* blk = &amp;blks[blk_index]; 76 77         // Link block to indexing policy 78         indexingPolicy-&gt;setEntry(blk, blk_index); 79 80         // Associate a data chunk to the block 81         blk-&gt;data = &amp;dataBlks[blkSize*blk_index];When you go back to the tagsInit, you can find that one dataBlks having blkSize size is mapped to the one CacheBlk.Because this CacheBlk is mapped to particular way of one set,the data will be also associated with that cache entry.To summary, the BaseCache maintains the tag.And the tag provides cache storage and cacheblks to the BaseIndexingPolicy object.Conceptually, the cache is maintained by the BaseIndexingPolicy class which maintains the cachewith set and ways indexing."
  },
  
  {
    "title": "O3 Cache Recv",
    "url": "/posts/O3-cache-recv/",
    "categories": "",
    "tags": "",
    "date": "2021-06-10 00:00:00 -0400",
    





    
    "snippet": "Cache receive2510 bool2511 BaseCache::MemSidePort::recvTimingResp(PacketPtr pkt)2512 {2513     cache-&gt;recvTimingResp(pkt);2514     return true;2515 }BaseCache::recvTimingResp 419 void 420 BaseCa...",
    "content": "Cache receive2510 bool2511 BaseCache::MemSidePort::recvTimingResp(PacketPtr pkt)2512 {2513     cache-&gt;recvTimingResp(pkt);2514     return true;2515 }BaseCache::recvTimingResp 419 void 420 BaseCache::recvTimingResp(PacketPtr pkt) 421 { 422     assert(pkt-&gt;isResponse()); 423  424     // all header delay should be paid for by the crossbar, unless 425     // this is a prefetch response from above 426     panic_if(pkt-&gt;headerDelay != 0 &amp;&amp; pkt-&gt;cmd != MemCmd::HardPFResp, 427              \"%s saw a non-zero packet delay\\n\", name()); 428  429     const bool is_error = pkt-&gt;isError(); 430  431     if (is_error) { 432         DPRINTF(Cache, \"%s: Cache received %s with error\\n\", __func__, 433                 pkt-&gt;print()); 434     } 435  436     DPRINTF(Cache, \"%s: Handling response %s\\n\", __func__, 437             pkt-&gt;print()); 438  439     // if this is a write, we should be looking at an uncacheable 440     // write 441     if (pkt-&gt;isWrite()) { 442         assert(pkt-&gt;req-&gt;isUncacheable()); 443         handleUncacheableWriteResp(pkt); 444         return; 445     } 446  447     // we have dealt with any (uncacheable) writes above, from here on 448     // we know we are dealing with an MSHR due to a miss or a prefetch 449     MSHR *mshr = dynamic_cast&lt;MSHR*&gt;(pkt-&gt;popSenderState()); 450     assert(mshr); 451  452     if (mshr == noTargetMSHR) { 453         // we always clear at least one target 454         clearBlocked(Blocked_NoTargets); 455         noTargetMSHR = nullptr; 456     } 457  458     // Initial target is used just for stats 459     const QueueEntry::Target *initial_tgt = mshr-&gt;getTarget(); 460     const Tick miss_latency = curTick() - initial_tgt-&gt;recvTime; 461     if (pkt-&gt;req-&gt;isUncacheable()) { 462         assert(pkt-&gt;req-&gt;requestorId() &lt; system-&gt;maxRequestors()); 463         stats.cmdStats(initial_tgt-&gt;pkt) 464             .mshrUncacheableLatency[pkt-&gt;req-&gt;requestorId()] += miss_latency; 465     } else { 466         assert(pkt-&gt;req-&gt;requestorId() &lt; system-&gt;maxRequestors()); 467         stats.cmdStats(initial_tgt-&gt;pkt) 468             .mshrMissLatency[pkt-&gt;req-&gt;requestorId()] += miss_latency; 469     }Filling the cache with the fetched data (recvTimingResp) 471     PacketList writebacks; 472  473     bool is_fill = !mshr-&gt;isForward &amp;&amp; 474         (pkt-&gt;isRead() || pkt-&gt;cmd == MemCmd::UpgradeResp || 475          mshr-&gt;wasWholeLineWrite); 476  477     // make sure that if the mshr was due to a whole line write then 478     // the response is an invalidation 479     assert(!mshr-&gt;wasWholeLineWrite || pkt-&gt;isInvalidate()); 480  481     CacheBlk *blk = tags-&gt;findBlock(pkt-&gt;getAddr(), pkt-&gt;isSecure()); 482  483     if (is_fill &amp;&amp; !is_error) { 484         DPRINTF(Cache, \"Block for addr %#llx being updated in Cache\\n\", 485                 pkt-&gt;getAddr()); 486  487         const bool allocate = (writeAllocator &amp;&amp; mshr-&gt;wasWholeLineWrite) ? 488             writeAllocator-&gt;allocate() : mshr-&gt;allocOnFill(); 489         blk = handleFill(pkt, blk, writebacks, allocate); 490         assert(blk != nullptr); 491         ppFill-&gt;notify(pkt); 492     }First of all, it needs to search the current cache to find out the blockmapped to the current request’s address.handleFill1433 CacheBlk*1434 BaseCache::handleFill(PacketPtr pkt, CacheBlk *blk, PacketList &amp;writebacks,1435                       bool allocate)1436 {1437     assert(pkt-&gt;isResponse());1438     Addr addr = pkt-&gt;getAddr();1439     bool is_secure = pkt-&gt;isSecure();1440     const bool has_old_data = blk &amp;&amp; blk-&gt;isValid();1441     const std::string old_state = blk ? blk-&gt;print() : \"\";1442 1443     // When handling a fill, we should have no writes to this line.1444     assert(addr == pkt-&gt;getBlockAddr(blkSize));1445     assert(!writeBuffer.findMatch(addr, is_secure));1446 1447     if (!blk) {1448         // better have read new data...1449         assert(pkt-&gt;hasData() || pkt-&gt;cmd == MemCmd::InvalidateResp);1450 1451         // need to do a replacement if allocating, otherwise we stick1452         // with the temporary storage1453         blk = allocate ? allocateBlock(pkt, writebacks) : nullptr;1454 1455         if (!blk) {1456             // No replaceable block or a mostly exclusive1457             // cache... just use temporary storage to complete the1458             // current request and then get rid of it1459             blk = tempBlock;1460             tempBlock-&gt;insert(addr, is_secure);1461             DPRINTF(Cache, \"using temp block for %#llx (%s)\\n\", addr,1462                     is_secure ? \"s\" : \"ns\");1463         }1464     } else {1465         // existing block... probably an upgrade1466         // don't clear block status... if block is already dirty we1467         // don't want to lose that1468     }When the blk is nullptr, it allocates new block depending on the allocate flag.When the flat is set, it allocates new block in the cache by invoking allocateBlock function. Note that writebacks is passed together with the packet. When the cache has no slot for the new data, then it evicts the pre-allocated one and will be handled by the writebacks. If there is no require for allocating new block, it assigns tempBlock instead of allocating new one.Note that it is required because the block is necessary in any case to process the current request.The allocate flag is determined mostly based on the inclusiveness ofcurrent cache level. When current cache level is exclusive to the lower cache level,it doesn’t need to allocate cache line for the current cache level and just forward the request to the upper level.Note that even for the exclusive cache, if one cache level is higher than the other, the request from more higher level cache or the memory should goes through the current cache level.setCoherenceBits:???1469 1470     // Block is guaranteed to be valid at this point1471     assert(blk-&gt;isValid());1472     assert(blk-&gt;isSecure() == is_secure);1473     assert(regenerateBlkAddr(blk) == addr);1474 1475     blk-&gt;setCoherenceBits(CacheBlk::ReadableBit);1476 1477     // sanity check for whole-line writes, which should always be1478     // marked as writable as part of the fill, and then later marked1479     // dirty as part of satisfyRequest1480     if (pkt-&gt;cmd == MemCmd::InvalidateResp) {1481         assert(!pkt-&gt;hasSharers());1482     }1483 1484     // here we deal with setting the appropriate state of the line,1485     // and we start by looking at the hasSharers flag, and ignore the1486     // cacheResponding flag (normally signalling dirty data) if the1487     // packet has sharers, thus the line is never allocated as Owned1488     // (dirty but not writable), and always ends up being either1489     // Shared, Exclusive or Modified, see Packet::setCacheResponding1490     // for more details1491     if (!pkt-&gt;hasSharers()) {1492         // we could get a writable line from memory (rather than a1493         // cache) even in a read-only cache, note that we set this bit1494         // even for a read-only cache, possibly revisit this decision1495         blk-&gt;setCoherenceBits(CacheBlk::WritableBit);1496 1497         // check if we got this via cache-to-cache transfer (i.e., from a1498         // cache that had the block in Modified or Owned state)1499         if (pkt-&gt;cacheResponding()) {1500             // we got the block in Modified state, and invalidated the1501             // owners copy1502             blk-&gt;setCoherenceBits(CacheBlk::DirtyBit);1503 1504             chatty_assert(!isReadOnly, \"Should never see dirty snoop response \"1505                           \"in read-only cache %s\\n\", name());1506 1507         }1508     }1509 1510     DPRINTF(Cache, \"Block addr %#llx (%s) moving from %s to %s\\n\",1511             addr, is_secure ? \"s\" : \"ns\", old_state, blk-&gt;print());1512 Filling the block and returning the filled block1513     // if we got new data, copy it in (checking for a read response1514     // and a response that has data is the same in the end)1515     if (pkt-&gt;isRead()) {1516         // sanity checks1517         assert(pkt-&gt;hasData());1518         assert(pkt-&gt;getSize() == blkSize);1519 1520         updateBlockData(blk, pkt, has_old_data);1521     }1522     // The block will be ready when the payload arrives and the fill is done1523     blk-&gt;setWhenReady(clockEdge(fillLatency) + pkt-&gt;headerDelay +1524                       pkt-&gt;payloadDelay);1525 1526     return blk;1527 } 694 void 695 BaseCache::updateBlockData(CacheBlk *blk, const PacketPtr cpkt, 696     bool has_old_data) 697 { 698     DataUpdate data_update(regenerateBlkAddr(blk), blk-&gt;isSecure()); 699     if (ppDataUpdate-&gt;hasListeners()) { 700         if (has_old_data) { 701             data_update.oldData = std::vector&lt;uint64_t&gt;(blk-&gt;data, 702                 blk-&gt;data + (blkSize / sizeof(uint64_t))); 703         } 704     } 705  706     // Actually perform the data update 707     if (cpkt) { 708         cpkt-&gt;writeDataToBlock(blk-&gt;data, blkSize); 709     } 710  711     if (ppDataUpdate-&gt;hasListeners()) { 712         if (cpkt) { 713             data_update.newData = std::vector&lt;uint64_t&gt;(blk-&gt;data, 714                 blk-&gt;data + (blkSize / sizeof(uint64_t))); 715         } 716         ppDataUpdate-&gt;notify(data_update); 717     } 718 }The actual data write is done by the updateBlockData function.Because the received packet contains the actual data that should be filled in the cache block, it copies the data from the packet to the cache block.271     /**  272      * Set tick at which block's data will be available for access. The new273      * tick must be chronologically sequential with respect to previous274      * accesses.275      *   276      * @param tick New data ready tick.277      */  278     void setWhenReady(const Tick tick)279     {        280         assert(tick &gt;= _tickInserted);281         whenReady = tick;282     }    Also, it needs to set the when the block will becomes ready by invoking setWhenReady function.Promote MSHR and service its targets (recvTimingResp) 493  494     if (blk &amp;&amp; blk-&gt;isValid() &amp;&amp; pkt-&gt;isClean() &amp;&amp; !pkt-&gt;isInvalidate()) { 495         // The block was marked not readable while there was a pending 496         // cache maintenance operation, restore its flag. 497         blk-&gt;setCoherenceBits(CacheBlk::ReadableBit); 498  499         // This was a cache clean operation (without invalidate) 500         // and we have a copy of the block already. Since there 501         // is no invalidation, we can promote targets that don't 502         // require a writable copy 503         mshr-&gt;promoteReadable(); 504     } 505  506     if (blk &amp;&amp; blk-&gt;isSet(CacheBlk::WritableBit) &amp;&amp; 507         !pkt-&gt;req-&gt;isCacheInvalidate()) { 508         // If at this point the referenced block is writable and the 509         // response is not a cache invalidate, we promote targets that 510         // were deferred as we couldn't guarrantee a writable copy 511         mshr-&gt;promoteWritable(); 512     }serviceMSHRTargets (recvTimingResp) 514     serviceMSHRTargets(mshr, pkt, blk);Although it has updated the cache block, still the targets of the MSHR entries are waiting the data block is coming to the cache.The main job of the serviceMSHRTargets function is looping targets of the MSHR entries associates with currently received response packet. Because there are three different sources for the targets, it should be handled differently. 683 void 684 Cache::serviceMSHRTargets(MSHR *mshr, const PacketPtr pkt, CacheBlk *blk) 685 { 686     QueueEntry::Target *initial_tgt = mshr-&gt;getTarget(); 687     // First offset for critical word first calculations 688     const int initial_offset = initial_tgt-&gt;pkt-&gt;getOffset(blkSize); 689  690     const bool is_error = pkt-&gt;isError(); 691     // allow invalidation responses originating from write-line 692     // requests to be discarded 693     bool is_invalidate = pkt-&gt;isInvalidate() &amp;&amp; 694         !mshr-&gt;wasWholeLineWrite; 695  696     MSHR::TargetList targets = mshr-&gt;extractServiceableTargets(pkt); 697     for (auto &amp;target: targets) { 698         Packet *tgt_pkt = target.pkt; 699         switch (target.source) { 700           case MSHR::Target::FromCPU: 701             Tick completion_time; 702             // Here we charge on completion_time the delay of the xbar if the 703             // packet comes from it, charged on headerDelay. 704             completion_time = pkt-&gt;headerDelay; 705  706             // Software prefetch handling for cache closest to core 707             if (tgt_pkt-&gt;cmd.isSWPrefetch()) { 708                 if (tgt_pkt-&gt;needsWritable()) { 709                     // All other copies of the block were invalidated and we 710                     // have an exclusive copy. 711  712                     // The coherence protocol assumes that if we fetched an 713                     // exclusive copy of the block, we have the intention to 714                     // modify it. Therefore the MSHR for the PrefetchExReq has 715                     // been the point of ordering and this cache has commited 716                     // to respond to snoops for the block. 717                     // 718                     // In most cases this is true anyway - a PrefetchExReq 719                     // will be followed by a WriteReq. However, if that 720                     // doesn't happen, the block is not marked as dirty and 721                     // the cache doesn't respond to snoops that has committed 722                     // to do so. 723                     // 724                     // To avoid deadlocks in cases where there is a snoop 725                     // between the PrefetchExReq and the expected WriteReq, we 726                     // proactively mark the block as Dirty. 727                     assert(blk); 728                     blk-&gt;setCoherenceBits(CacheBlk::DirtyBit); 729  730                     panic_if(isReadOnly, \"Prefetch exclusive requests from \" 731                             \"read-only cache %s\\n\", name()); 732                 } 733  734                 // a software prefetch would have already been ack'd 735                 // immediately with dummy data so the core would be able to 736                 // retire it. This request completes right here, so we 737                 // deallocate it. 738                 delete tgt_pkt; 739                 break; // skip response 740             } 741  742             // unlike the other packet flows, where data is found in other 743             // caches or memory and brought back, write-line requests always 744             // have the data right away, so the above check for \"is fill?\" 745             // cannot actually be determined until examining the stored MSHR 746             // state. We \"catch up\" with that logic here, which is duplicated 747             // from above. 748             if (tgt_pkt-&gt;cmd == MemCmd::WriteLineReq) { 749                 assert(!is_error); 750                 assert(blk); 751                 assert(blk-&gt;isSet(CacheBlk::WritableBit)); 752             } 753  754             // Here we decide whether we will satisfy the target using 755             // data from the block or from the response. We use the 756             // block data to satisfy the request when the block is 757             // present and valid and in addition the response in not 758             // forwarding data to the cache above (we didn't fill 759             // either); otherwise we use the packet data. 760             if (blk &amp;&amp; blk-&gt;isValid() &amp;&amp; 761                 (!mshr-&gt;isForward || !pkt-&gt;hasData())) { 762                 satisfyRequest(tgt_pkt, blk, true, mshr-&gt;hasPostDowngrade()); 763  764                 // How many bytes past the first request is this one 765                 int transfer_offset = 766                     tgt_pkt-&gt;getOffset(blkSize) - initial_offset; 767                 if (transfer_offset &lt; 0) { 768                     transfer_offset += blkSize; 769                 } 770  771                 // If not critical word (offset) return payloadDelay. 772                 // responseLatency is the latency of the return path 773                 // from lower level caches/memory to an upper level cache or 774                 // the core. 775                 completion_time += clockEdge(responseLatency) + 776                     (transfer_offset ? pkt-&gt;payloadDelay : 0); 777  778                 assert(!tgt_pkt-&gt;req-&gt;isUncacheable()); 779  780                 assert(tgt_pkt-&gt;req-&gt;requestorId() &lt; system-&gt;maxRequestors()); 781                 stats.cmdStats(tgt_pkt) 782                     .missLatency[tgt_pkt-&gt;req-&gt;requestorId()] += 783                     completion_time - target.recvTime; 784             } else if (pkt-&gt;cmd == MemCmd::UpgradeFailResp) { 785                 // failed StoreCond upgrade 786                 assert(tgt_pkt-&gt;cmd == MemCmd::StoreCondReq || 787                        tgt_pkt-&gt;cmd == MemCmd::StoreCondFailReq || 788                        tgt_pkt-&gt;cmd == MemCmd::SCUpgradeFailReq); 789                 // responseLatency is the latency of the return path 790                 // from lower level caches/memory to an upper level cache or 791                 // the core. 792                 completion_time += clockEdge(responseLatency) + 793                     pkt-&gt;payloadDelay; 794                 tgt_pkt-&gt;req-&gt;setExtraData(0); 795             } else { 796                 if (is_invalidate &amp;&amp; blk &amp;&amp; blk-&gt;isValid()) { 797                     // We are about to send a response to a cache above 798                     // that asked for an invalidation; we need to 799                     // invalidate our copy immediately as the most 800                     // up-to-date copy of the block will now be in the 801                     // cache above. It will also prevent this cache from 802                     // responding (if the block was previously dirty) to 803                     // snoops as they should snoop the caches above where 804                     // they will get the response from. 805                     invalidateBlock(blk); 806                 } 807                 // not a cache fill, just forwarding response 808                 // responseLatency is the latency of the return path 809                 // from lower level cahces/memory to the core. 810                 completion_time += clockEdge(responseLatency) + 811                     pkt-&gt;payloadDelay; 812                 if (!is_error) { 813                     if (pkt-&gt;isRead()) { 814                         // sanity check 815                         assert(pkt-&gt;matchAddr(tgt_pkt)); 816                         assert(pkt-&gt;getSize() &gt;= tgt_pkt-&gt;getSize()); 817  818                         tgt_pkt-&gt;setData(pkt-&gt;getConstPtr&lt;uint8_t&gt;()); 819                     } else { 820                         // MSHR targets can read data either from the 821                         // block or the response pkt. If we can't get data 822                         // from the block (i.e., invalid or has old data) 823                         // or the response (did not bring in any data) 824                         // then make sure that the target didn't expect 825                         // any. 826                         assert(!tgt_pkt-&gt;hasRespData()); 827                     } 828                 } 829  830                 // this response did not allocate here and therefore 831                 // it was not consumed, make sure that any flags are 832                 // carried over to cache above 833                 tgt_pkt-&gt;copyResponderFlags(pkt); 834             } 835             tgt_pkt-&gt;makeTimingResponse(); 836             // if this packet is an error copy that to the new packet 837             if (is_error) 838                 tgt_pkt-&gt;copyError(pkt); 839             if (tgt_pkt-&gt;cmd == MemCmd::ReadResp &amp;&amp; 840                 (is_invalidate || mshr-&gt;hasPostInvalidate())) { 841                 // If intermediate cache got ReadRespWithInvalidate, 842                 // propagate that.  Response should not have 843                 // isInvalidate() set otherwise. 844                 tgt_pkt-&gt;cmd = MemCmd::ReadRespWithInvalidate; 845                 DPRINTF(Cache, \"%s: updated cmd to %s\\n\", __func__, 846                         tgt_pkt-&gt;print()); 847             } 848             // Reset the bus additional time as it is now accounted for 849             tgt_pkt-&gt;headerDelay = tgt_pkt-&gt;payloadDelay = 0; 850             cpuSidePort.schedTimingResp(tgt_pkt, completion_time); 851             break;For the FromCPU case, there are two main conditions that we need to take care. First of all, when the blk associated with current cache block response is available, then it will invoke satisfyRequest function. However, when the blk points to nullptr, then it just copies data from the response packet to the packet selected among the targets. Regardless of the availability of the blk, it invokes schedTimingResp through the cpuSidePort to send the response packet to the upper cache or processor. Note that this response packet deliver one of targets of the resolved MSHR. At the time of exit of the loop, all packers associated with the resolved MSHR entrywill be handled. 853           case MSHR::Target::FromPrefetcher: 854             assert(tgt_pkt-&gt;cmd == MemCmd::HardPFReq); 855             if (blk) 856                 blk-&gt;setPrefetched(); 857             delete tgt_pkt; 858             break; 859  860           case MSHR::Target::FromSnoop: 861             // I don't believe that a snoop can be in an error state 862             assert(!is_error); 863             // response to snoop request 864             DPRINTF(Cache, \"processing deferred snoop...\\n\"); 865             // If the response is invalidating, a snooping target can 866             // be satisfied if it is also invalidating. If the reponse is, not 867             // only invalidating, but more specifically an InvalidateResp and 868             // the MSHR was created due to an InvalidateReq then a cache above 869             // is waiting to satisfy a WriteLineReq. In this case even an 870             // non-invalidating snoop is added as a target here since this is 871             // the ordering point. When the InvalidateResp reaches this cache, 872             // the snooping target will snoop further the cache above with the 873             // WriteLineReq. 874             assert(!is_invalidate || pkt-&gt;cmd == MemCmd::InvalidateResp || 875                    pkt-&gt;req-&gt;isCacheMaintenance() || 876                    mshr-&gt;hasPostInvalidate()); 877             handleSnoop(tgt_pkt, blk, true, true, mshr-&gt;hasPostInvalidate()); 878             break; 879  880           default: 881             panic(\"Illegal target-&gt;source enum %d\\n\", target.source); 882         } 883     } 884  885     maintainClusivity(targets.hasFromCache, blk); 886  887     if (blk &amp;&amp; blk-&gt;isValid()) { 888         // an invalidate response stemming from a write line request 889         // should not invalidate the block, so check if the 890         // invalidation should be discarded 891         if (is_invalidate || mshr-&gt;hasPostInvalidate()) { 892             invalidateBlock(blk); 893         } else if (mshr-&gt;hasPostDowngrade()) { 894             blk-&gt;clearCoherenceBits(CacheBlk::WritableBit); 895         } 896     } 897 }## Finishing MSHR resolving (recvTimingResp) 516     if (mshr-&gt;promoteDeferredTargets()) { 517         // avoid later read getting stale data while write miss is 518         // outstanding.. see comment in timingAccess() 519         if (blk) { 520             blk-&gt;clearCoherenceBits(CacheBlk::ReadableBit); 521         } 522         mshrQueue.markPending(mshr); 523         schedMemSideSendEvent(clockEdge() + pkt-&gt;payloadDelay); 524     } else { 525         // while we deallocate an mshr from the queue we still have to 526         // check the isFull condition before and after as we might 527         // have been using the reserved entries already 528         const bool was_full = mshrQueue.isFull(); 529         mshrQueue.deallocate(mshr); 530         if (was_full &amp;&amp; !mshrQueue.isFull()) { 531             clearBlocked(Blocked_NoMSHRs); 532         } 533  534         // Request the bus for a prefetch if this deallocation freed enough 535         // MSHRs for a prefetch to take place 536         if (prefetcher &amp;&amp; mshrQueue.canPrefetch() &amp;&amp; !isBlocked()) { 537             Tick next_pf_time = std::max(prefetcher-&gt;nextPrefetchReadyTime(), 538                                          clockEdge()); 539             if (next_pf_time != MaxTick) 540                 schedMemSideSendEvent(next_pf_time); 541         } 542     } 543  544     // if we used temp block, check to see if its valid and then clear it out 545     if (blk == tempBlock &amp;&amp; tempBlock-&gt;isValid()) { 546         evictBlock(blk, writebacks); 547     } 548  549     const Tick forward_time = clockEdge(forwardLatency) + pkt-&gt;headerDelay; 550     // copy writebacks to write buffer 551     doWritebacks(writebacks, forward_time); 552  553     DPRINTF(CacheVerbose, \"%s: Leaving with %s\\n\", __func__, pkt-&gt;print()); 554     delete pkt; 555 }After processing all targets of the currently selected MSHR entry, we should promote deferred targets or deallocate the MSHR entry.Although we finish processing the targets of the selected MSHR, there could be deferred targets for that MSHR entry.In that case, those targets should be moved to the MSHR, and the selected MSHR should not be freed. However, if there is no deferred targets, then the selected MSHR can be freed. Also, if the cache was blocked because of full of MSHR, it clear blocking. Furthermore, if possible, it generates prefetch request and send it to the memory. After the deallocation, the evicted packet should be written backs to the higher level cache or the memory. The doWritebacks function handles this write back operations.Also, when the current block is tempBlock and no cache entry has been allocatedfor the current response, it should evict the current block.\\XXX{I don’t know why tempBlock need to be evicted here..? Cause it didn’t generate new cache block..}"
  },
  
  {
    "title": "O3 Cpu Commit",
    "url": "/posts/O3-CPU-commit/",
    "categories": "GEM5, Pipeline, O3",
    "tags": "",
    "date": "2021-06-04 00:00:00 -0400",
    





    
    "snippet": "Memory read and write of the O3 CPUMemory read621 LSQUnit&lt;Impl&gt;::read(LSQRequest *req, int load_idx)622 {623     LQEntry&amp; load_req = loadQueue[load_idx];624     const DynInstPtr&amp; load...",
    "content": "Memory read and write of the O3 CPUMemory read621 LSQUnit&lt;Impl&gt;::read(LSQRequest *req, int load_idx)622 {623     LQEntry&amp; load_req = loadQueue[load_idx];624     const DynInstPtr&amp; load_inst = load_req.instruction();625 626     load_req.setRequest(req);627     assert(load_inst);628 629     assert(!load_inst-&gt;isExecuted());630 631     // Make sure this isn't a strictly ordered load632     // A bit of a hackish way to get strictly ordered accesses to work633     // only if they're at the head of the LSQ and are ready to commit634     // (at the head of the ROB too).635 636     if (req-&gt;mainRequest()-&gt;isStrictlyOrdered() &amp;&amp;637         (load_idx != loadQueue.head() || !load_inst-&gt;isAtCommit())) {638         // Tell IQ/mem dep unit that this instruction will need to be639         // rescheduled eventually640         iewStage-&gt;rescheduleMemInst(load_inst);641         load_inst-&gt;clearIssued();642         load_inst-&gt;effAddrValid(false);643         ++lsqRescheduledLoads;644         DPRINTF(LSQUnit, \"Strictly ordered load [sn:%lli] PC %s\\n\",645                 load_inst-&gt;seqNum, load_inst-&gt;pcState());646 647         // Must delete request now that it wasn't handed off to648         // memory.  This is quite ugly.  @todo: Figure out the proper649         // place to really handle request deletes.650         load_req.setRequest(nullptr);651         req-&gt;discard();652         return std::make_shared&lt;GenericISA::M5PanicFault&gt;(653             \"Strictly ordered load [sn:%llx] PC %s\\n\",654             load_inst-&gt;seqNum, load_inst-&gt;pcState());655     }656 657     DPRINTF(LSQUnit, \"Read called, load idx: %i, store idx: %i, \"658             \"storeHead: %i addr: %#x%s\\n\",659             load_idx - 1, load_inst-&gt;sqIt._idx, storeQueue.head() - 1,660             req-&gt;mainRequest()-&gt;getPaddr(), req-&gt;isSplit() ? \" split\" : \"\");661 662     if (req-&gt;mainRequest()-&gt;isLLSC()) {663         // Disable recording the result temporarily.  Writing to misc664         // regs normally updates the result, but this is not the665         // desired behavior when handling store conditionals.666         load_inst-&gt;recordResult(false);667         TheISA::handleLockedRead(load_inst.get(), req-&gt;mainRequest());668         load_inst-&gt;recordResult(true);669     }670 671     if (req-&gt;mainRequest()-&gt;isMmappedIpr()) {672         assert(!load_inst-&gt;memData);673         load_inst-&gt;memData = new uint8_t[MaxDataBytes];674 675         ThreadContext *thread = cpu-&gt;tcBase(lsqID);676         PacketPtr main_pkt = new Packet(req-&gt;mainRequest(), MemCmd::ReadReq);677 678         main_pkt-&gt;dataStatic(load_inst-&gt;memData);679 680         Cycles delay = req-&gt;handleIprRead(thread, main_pkt);681 682         WritebackEvent *wb = new WritebackEvent(load_inst, main_pkt, this);683         cpu-&gt;schedule(wb, cpu-&gt;clockEdge(delay));684         return NoFault;685     }686 687     // Check the SQ for any previous stores that might lead to forwarding......840     // If there's no forwarding case, then go access memory841     DPRINTF(LSQUnit, \"Doing memory access for inst [sn:%lli] PC %s\\n\",842             load_inst-&gt;seqNum, load_inst-&gt;pcState());843 844     // Allocate memory if this is the first time a load is issued.845     if (!load_inst-&gt;memData) {846         load_inst-&gt;memData = new uint8_t[req-&gt;mainRequest()-&gt;getSize()];847     }848 849     // For now, load throughput is constrained by the number of850     // load FUs only, and loads do not consume a cache port (only851     // stores do).852     // @todo We should account for cache port contention853     // and arbitrate between loads and stores.854 855     // if we the cache is not blocked, do cache access856     if (req-&gt;senderState() == nullptr) {857         LQSenderState *state = new LQSenderState(858                 loadQueue.getIterator(load_idx));859         state-&gt;isLoad = true;860         state-&gt;inst = load_inst;861         state-&gt;isSplit = req-&gt;isSplit();862         req-&gt;senderState(state);863     }864     req-&gt;buildPackets();865     req-&gt;sendPacketToCache();866     if (!req-&gt;isSent())867         iewStage-&gt;blockMemInst(load_inst);868 869     return NoFault;870 }If the current instruction has not initiated the memory load operation before,then it allocates a memory and make the memData of the instruction points to this allocated memory to store the actual data read from cache or memory.After that, it generates senderState object if it doesn’t have.The state object contains information such as whether this request is load or store, the instruction that initiated the memory operation, and information about whether the request is a split or single access. After the senderState is generated, it is stored in the request object.Note that here the req is the object of LSQRequest.Remember that the req is the same object used for the TLB resolution.Because this object contains all information required for resolving one memory operationincluding TLB, cache ports, etc, by invoking proper function,CPU can handle read/write operations.Build packet1032 template&lt;class Impl&gt;1033 void1034 LSQ&lt;Impl&gt;::SingleDataRequest::buildPackets()1035 {  1036     assert(_senderState);1037     /* Retries do not create new packets. */1038     if (_packets.size() == 0) {1039         _packets.push_back(1040                 isLoad()1041                     ?  Packet::createRead(request())1042                     :  Packet::createWrite(request()));1043         _packets.back()-&gt;dataStatic(_inst-&gt;memData);1044         _packets.back()-&gt;senderState = _senderState;1045     }1046     assert(_packets.size() == 1);1047 } 276 /** 277  * A Packet is used to encapsulate a transfer between two objects in 278  * the memory system (e.g., the L1 and L2 cache).  (In contrast, a 279  * single Request travels all the way from the requestor to the 280  * ultimate destination and back, possibly being conveyed by several 281  * different Packets along the way.) 282  */ 283 class Packet : public Printable 284 { 285   public: 286     typedef uint32_t FlagsType; 287     typedef gem5::Flags&lt;FlagsType&gt; Flags;...... 368   private: 369    /** 370     * A pointer to the data being transferred. It can be different 371     * sizes at each level of the hierarchy so it belongs to the 372     * packet, not request. This may or may not be populated when a 373     * responder receives the packet. If not populated memory should 374     * be allocated. 375     */ 376     PacketDataPtr data;...... 846     /** 847      * Constructor. Note that a Request object must be constructed 848      * first, but the Requests's physical address and size fields need 849      * not be valid. The command must be supplied. 850      */ 851     Packet(const RequestPtr &amp;_req, MemCmd _cmd) 852         :  cmd(_cmd), id((PacketId)_req.get()), req(_req), 853            data(nullptr), addr(0), _isSecure(false), size(0), 854            _qosValue(0), 855            htmReturnReason(HtmCacheFailure::NO_FAIL), 856            htmTransactionUid(0), 857            headerDelay(0), snoopDelay(0), 858            payloadDelay(0), senderState(NULL) 859     { 860         flags.clear(); 861         if (req-&gt;hasPaddr()) { 862             addr = req-&gt;getPaddr(); 863             flags.set(VALID_ADDR); 864             _isSecure = req-&gt;isSecure(); 865         } 866  867         /** 868          * hardware transactional memory 869          * 870          * This is a bit of a hack! 871          * Technically the address of a HTM command is set to zero 872          * but is not valid. The reason that we pretend it's valid is 873          * to void the getAddr() function from failing. It would be 874          * cumbersome to add control flow in many places to check if the 875          * packet represents a HTM command before calling getAddr(). 876          */ 877         if (req-&gt;isHTMCmd()) { 878             flags.set(VALID_ADDR); 879             assert(addr == 0x0); 880         } 881         if (req-&gt;hasSize()) { 882             size = req-&gt;getSize(); 883             flags.set(VALID_SIZE); 884         } 885     }......1002     /**1003      * Constructor-like methods that return Packets based on Request objects.1004      * Fine-tune the MemCmd type if it's not a vanilla read or write.1005      */1006     static PacketPtr1007     createRead(const RequestPtr &amp;req)1008     {1009         return new Packet(req, makeReadCmd(req));1010     }1011 1012     static PacketPtr1013     createWrite(const RequestPtr &amp;req)1014     {1015         return new Packet(req, makeWriteCmd(req));1016     }buildPackets function generates new packet that will be sent to the cache.The generated packet is maintained in the internal vector called _packets. Also, it sets the buffer allocated for storing the data, _inst-&gt;memData to internal data member field of the packet. Also, the senderState is stored. 386     /** 387      * A virtual base opaque structure used to hold state associated 388      * with the packet (e.g., an MSHR), specific to a SimObject that 389      * sees the packet. A pointer to this state is returned in the 390      * packet's response so that the SimObject in question can quickly 391      * look up the state needed to process it. A specific subclass 392      * would be derived from this to carry state specific to a 393      * particular sending device. 394      * 395      * As multiple SimObjects may add their SenderState throughout the 396      * memory system, the SenderStates create a stack, where a 397      * SimObject can add a new Senderstate, as long as the 398      * predecessing SenderState is restored when the response comes 399      * back. For this reason, the predecessor should always be 400      * populated with the current SenderState of a packet before 401      * modifying the senderState field in the request packet. 402      */ 403     struct SenderState 404     { 405         SenderState* predecessor; 406         SenderState() : predecessor(NULL) {} 407         virtual ~SenderState() {} 408     };attribute of the packetmem/packet.hh 209     bool 210     testCmdAttrib(MemCmd::Attribute attrib) const 211     { 212         return commandInfo[cmd].attributes[attrib] != 0; 213     } 214  215   public: 216  217     bool isRead() const            { return testCmdAttrib(IsRead); } 218     bool isWrite() const           { return testCmdAttrib(IsWrite); } 219     bool isUpgrade() const         { return testCmdAttrib(IsUpgrade); } 220     bool isRequest() const         { return testCmdAttrib(IsRequest); } 221     bool isResponse() const        { return testCmdAttrib(IsResponse); } 222     bool needsWritable() const     { return testCmdAttrib(NeedsWritable); } 223     bool needsResponse() const     { return testCmdAttrib(NeedsResponse); } 224     bool isInvalidate() const      { return testCmdAttrib(IsInvalidate); } 225     bool isEviction() const        { return testCmdAttrib(IsEviction); } 226     bool isClean() const           { return testCmdAttrib(IsClean); } 227     bool fromCache() const         { return testCmdAttrib(FromCache); }mem/packet.cc 64 const MemCmd::CommandInfo 65 MemCmd::commandInfo[] = 66 { 67     /* InvalidCmd */ 68     { {}, InvalidCmd, \"InvalidCmd\" }, 69     /* ReadReq - Read issued by a non-caching agent such as a CPU or 70      * device, with no restrictions on alignment. */ 71     { {IsRead, IsRequest, NeedsResponse}, ReadResp, \"ReadReq\" }, 72     /* ReadResp */ 73     { {IsRead, IsResponse, HasData}, InvalidCmd, \"ReadResp\" }, 74     /* ReadRespWithInvalidate */ 75     { {IsRead, IsResponse, HasData, IsInvalidate}, 76             InvalidCmd, \"ReadRespWithInvalidate\" }, 77     /* WriteReq */ 78     { {IsWrite, NeedsWritable, IsRequest, NeedsResponse, HasData}, 79             WriteResp, \"WriteReq\" }, 80     /* WriteResp */ 81     { {IsWrite, IsResponse}, InvalidCmd, \"WriteResp\" }, 82     /* WriteCompleteResp - The WriteCompleteResp command is needed 83      * because in the GPU memory model we use a WriteResp to indicate 84      * that a write has reached the cache controller so we can free 85      * resources at the coalescer. Later, when the write succesfully 86      * completes we send a WriteCompleteResp to the CU so its wait 87      * counters can be updated. Wait counters in the CU is how memory 88      * dependences are handled in the GPU ISA. */ 89     { {IsWrite, IsResponse}, InvalidCmd, \"WriteCompleteResp\" },send packet to the cache1083 template&lt;class Impl&gt;1084 void1085 LSQ&lt;Impl&gt;::SingleDataRequest::sendPacketToCache()1086 {  1087     assert(_numOutstandingPackets == 0);1088     if (lsqUnit()-&gt;trySendPacket(isLoad(), _packets.at(0)))1089         _numOutstandingPackets = 1;1090 }  1083 template &lt;class Impl&gt;1084 bool1085 LSQUnit&lt;Impl&gt;::trySendPacket(bool isLoad, PacketPtr data_pkt)1086 {  1087     bool ret = true;1088     bool cache_got_blocked = false;1089         1090     auto state = dynamic_cast&lt;LSQSenderState*&gt;(data_pkt-&gt;senderState);1091                 1092     if (!lsq-&gt;cacheBlocked() &amp;&amp;1093         lsq-&gt;cachePortAvailable(isLoad)) {1094         if (!dcachePort-&gt;sendTimingReq(data_pkt)) {1095             ret = false;1096             cache_got_blocked = true;1097         } 1098     } else {1099         ret = false;1100     }   1101     1102     if (ret) {1103         if (!isLoad) {1104             isStoreBlocked = false;1105         }1106         lsq-&gt;cachePortBusy(isLoad);1107         state-&gt;outstanding++;                1108         state-&gt;request()-&gt;packetSent();1109     } else {1110         if (cache_got_blocked) {1111             lsq-&gt;cacheBlocked(true);1112             ++lsqCacheBlocked;1113         }1114         if (!isLoad) {1115             assert(state-&gt;request() == storeWBIt-&gt;request());1116             isStoreBlocked = true;1117         }1118         state-&gt;request()-&gt;packetNotSent();1119     }1120     return ret;1121 }This packet will be sent to the cache through the cache port connected to the LSQ. It first checks whether the cache is currently blocked.If it is not blocked and there are available read port for the cache,then it sends the request packet through the dcachePort. It can initiate memory access by sending request packet through a sendTimingReq method.Because CPU goes through the data cache before touching the physical memory, the sendTimingReq is invoked on the DcachePort.gem5/src/mem/port.hh444 inline bool445 MasterPort::sendTimingReq(PacketPtr pkt)446 {447     return TimingRequestProtocol::sendReq(_slavePort, pkt);448 }mem/protocol/timing.cc 47 /* The request protocol. */ 48  49 bool 50 TimingRequestProtocol::sendReq(TimingResponseProtocol *peer, PacketPtr pkt) 51 { 52     assert(pkt-&gt;isRequest()); 53     return peer-&gt;recvTimingReq(pkt); 54 }The sendTimingReq function is very simple. Just invoke the recvTimingReq function of the peer connected to the dcachePortas a slave. Because the cache unit is connected to the dcachePort on the other side of the CPU,we will take a look at the recvTimingReq implementation of the cache unit.Cache, Cache, Cache!recvTimingReq of the BaseCache: how to process the cache access?2448 bool2449 BaseCache::CpuSidePort::recvTimingReq(PacketPtr pkt)2450 {2451     assert(pkt-&gt;isRequest());2452 2453     if (cache-&gt;system-&gt;bypassCaches()) {2454         // Just forward the packet if caches are disabled.2455         // @todo This should really enqueue the packet rather2456         GEM5_VAR_USED bool success = cache-&gt;memSidePort.sendTimingReq(pkt);2457         assert(success);2458         return true;2459     } else if (tryTiming(pkt)) {2460         cache-&gt;recvTimingReq(pkt);2461         return true;2462     }2463     return false;2464 }First of all, the cache port connected to the CPU side will be in charge of handling timing request generated from CPU side. Because the BaseCache contains dedicated port for communicating with the CPU side,called CpuSidePort, its recvTimingReq function will be invoked.However, the main cache operations are done by the BaseCache’s recvTimingReq 349 void 350 BaseCache::recvTimingReq(PacketPtr pkt) 351 {    352     // anything that is merely forwarded pays for the forward latency and 353     // the delay provided by the crossbar 354     Tick forward_time = clockEdge(forwardLatency) + pkt-&gt;headerDelay; 355      356     Cycles lat; 357     CacheBlk *blk = nullptr; 358     bool satisfied = false; 359     {    360         PacketList writebacks; 361         // Note that lat is passed by reference here. The function 362         // access() will set the lat value. 363         satisfied = access(pkt, blk, lat, writebacks); 364          365         // After the evicted blocks are selected, they must be forwarded 366         // to the write buffer to ensure they logically precede anything 367         // happening below 368         doWritebacks(writebacks, clockEdge(lat + forwardLatency)); 369     } 370     Because the recvTimingReq is pretty complex and long, I will explain important parts one by one. First of all, it invokes the access functionto access the cache entry if the data mapped to the request address exists in the cache. After that, it invokes doWritebacks function to write backs evicted entries if exist. Btw, why the access generates victim entry and write back is required?I will show you the answer soon.access function, another long journey in the midst of recvTimingReqUnfortunately, the access function is more complex function than the recvTimingReq cause it emulates actual cache accesses in the GEM5 cache. Let’s take a look at its implementation one by one.access1: check if the cache block exist in current cache1152 bool1153 BaseCache::access(PacketPtr pkt, CacheBlk *&amp;blk, Cycles &amp;lat,1154                   PacketList &amp;writebacks)1155 {1156     // sanity check1157     assert(pkt-&gt;isRequest());1158 1159     chatty_assert(!(isReadOnly &amp;&amp; pkt-&gt;isWrite()),1160                   \"Should never see a write in a read-only cache %s\\n\",1161                   name());1162 1163     // Access block in the tags1164     Cycles tag_latency(0);1165     blk = tags-&gt;accessBlock(pkt, tag_latency);1166 1167     DPRINTF(Cache, \"%s for %s %s\\n\", __func__, pkt-&gt;print(),1168             blk ? \"hit \" + blk-&gt;print() : \"miss\");1169 The first job done by the access function is retrieving the CacheBlk associated with current request address. Because the tags member field manages all CacheBlk of the cache,it invokes the accessBlock function of the tags.117     /**118      * Access block and update replacement data. May not succeed, in which case119      * nullptr is returned. This has all the implications of a cache access and120      * should only be used as such. Returns the tag lookup latency as a side121      * effect.122      *123      * @param pkt The packet holding the address to find.124      * @param lat The latency of the tag lookup.125      * @return Pointer to the cache block if found.126      */127     CacheBlk* accessBlock(const PacketPtr pkt, Cycles &amp;lat) override128     {129         CacheBlk *blk = findBlock(pkt-&gt;getAddr(), pkt-&gt;isSecure());130 131         // Access all tags in parallel, hence one in each way.  The data side132         // either accesses all blocks in parallel, or one block sequentially on133         // a hit.  Sequential access with a miss doesn't access data.134         stats.tagAccesses += allocAssoc;135         if (sequentialAccess) {136             if (blk != nullptr) {137                 stats.dataAccesses += 1;138             }139         } else {140             stats.dataAccesses += allocAssoc;141         }142 143         // If a cache hit144         if (blk != nullptr) {145             // Update number of references to accessed block146             blk-&gt;increaseRefCount();147 148             // Update replacement data of accessed block149             replacementPolicy-&gt;touch(blk-&gt;replacementData, pkt);150         }151 152         // The tag lookup latency is the same for a hit or a miss153         lat = lookupLatency;154 155         return blk;156     } 79 CacheBlk* 80 BaseTags::findBlock(Addr addr, bool is_secure) const 81 { 82     // Extract block tag 83     Addr tag = extractTag(addr); 84  85     // Find possible entries that may contain the given address 86     const std::vector&lt;ReplaceableEntry*&gt; entries = 87         indexingPolicy-&gt;getPossibleEntries(addr); 88  89     // Search for block 90     for (const auto&amp; location : entries) { 91         CacheBlk* blk = static_cast&lt;CacheBlk*&gt;(location); 92         if (blk-&gt;matchTag(tag, is_secure)) { 93             return blk; 94         } 95     } 96  97     // Did not find block 98     return nullptr; 99 }Because the CacheBlk is associated with one address based on the Tag value, by checking the tag value of way entries in one set mapped to current request’s address,it can find whether the cache already contains the cache blockmapped to current request address.Also, note that it can return nullptrwhen there is no cache hit.Therefore, by checking the returned CacheBlk as a result of the findBlock function,it can distinguish cache hit and miss. When the cache hit happens, it invokes touch function of the replacementPolicyto update the replacement policy associated with current CacheBlk.access2: handling cache maintenance packetLet’s go back to the access function. After the accessBlock function returns, it checks types of the packet.1170     if (pkt-&gt;req-&gt;isCacheMaintenance()) {1171         // A cache maintenance operation is always forwarded to the1172         // memory below even if the block is found in dirty state.1173 1174         // We defer any changes to the state of the block until we1175         // create and mark as in service the mshr for the downstream1176         // packet.1177 1178         // Calculate access latency on top of when the packet arrives. This1179         // takes into account the bus delay.1180         lat = calculateTagOnlyLatency(pkt-&gt;headerDelay, tag_latency);1181 1182         return false;1183     }```cpp1001     /**1002      * Accessor functions to determine whether this request is part of1003      * a cache maintenance operation. At the moment three operations1004      * are supported:1005 1006      * 1) A cache clean operation updates all copies of a memory1007      * location to the point of reference,1008      * 2) A cache invalidate operation invalidates all copies of the1009      * specified block in the memory above the point of reference,1010      * 3) A clean and invalidate operation is a combination of the two1011      * operations.1012      * @{ */1013     bool isCacheClean() const { return _flags.isSet(CLEAN); }1014     bool isCacheInvalidate() const { return _flags.isSet(INVALIDATE); }1015     bool isCacheMaintenance() const { return _flags.isSet(CLEAN|INVALIDATE); }1016     /** @} */Currently, GEM5 provide three different requests for cache maintenance:cache clean, cache invalidate, and clean and invalidate. Here is a good definition about invalidate and clean event in general.  Invalidate simply marks a cache line as “invalid”, meaning you won’t hit upon.Clean causes the contents of the cache line to be written back to memory (or the next level of cache), but only if the cache line is “dirty”.That is, the cache line holds the latest copy of that memory.Clean &amp; Invalidate, as the name suggests, does both.Dirty lines normally get back to memory through evictions. When the line is selected to be evicted, there is a check to see if it’s dirty.If yes, it gets written back to memory.Cleaning is way to force this to happen at a particular time.For example, because something else is going to read the buffer.In theory, if you invalidated a dirty line you could loose data.As an invalid line won’t get written back to memory automatically through eviction.In practice many cores will treat Invalidate as Clean&amp;Invalidate - but you shouldn’t rely on that.If the line is potentially dirty, and you care about the data, you should use Clean&amp;Invalidate rather than Invalidate.Because the cache maintenance request is related with cache flushing and coherency, it should be specially handled by the cache unit. When the packet is sent to the cache for its maintenance it returns immediately from the access function and set the satisfied variable as false, which indicates the miss event happens.access3: handling eviction request packet1185     if (pkt-&gt;isEviction()) {1186         // We check for presence of block in above caches before issuing1187         // Writeback or CleanEvict to write buffer. Therefore the only1188         // possible cases can be of a CleanEvict packet coming from above1189         // encountering a Writeback generated in this cache peer cache and1190         // waiting in the write buffer. Cases of upper level peer caches1191         // generating CleanEvict and Writeback or simply CleanEvict and1192         // CleanEvict almost simultaneously will be caught by snoops sent out1193         // by crossbar.1194         WriteQueueEntry *wb_entry = writeBuffer.findMatch(pkt-&gt;getAddr(),1195                                                           pkt-&gt;isSecure());1196         if (wb_entry) {1197             assert(wb_entry-&gt;getNumTargets() == 1);1198             PacketPtr wbPkt = wb_entry-&gt;getTarget()-&gt;pkt;1199             assert(wbPkt-&gt;isWriteback());1200 1201             if (pkt-&gt;isCleanEviction()) {1202                 // The CleanEvict and WritebackClean snoops into other1203                 // peer caches of the same level while traversing the1204                 // crossbar. If a copy of the block is found, the1205                 // packet is deleted in the crossbar. Hence, none of1206                 // the other upper level caches connected to this1207                 // cache have the block, so we can clear the1208                 // BLOCK_CACHED flag in the Writeback if set and1209                 // discard the CleanEvict by returning true.1210                 wbPkt-&gt;clearBlockCached();1211 1212                 // A clean evict does not need to access the data array1213                 lat = calculateTagOnlyLatency(pkt-&gt;headerDelay, tag_latency);1214 1215                 return true;1216             } else {1217                 assert(pkt-&gt;cmd == MemCmd::WritebackDirty);1218                 // Dirty writeback from above trumps our clean1219                 // writeback... discard here1220                 // Note: markInService will remove entry from writeback buffer.1221                 markInService(wb_entry);1222                 delete wbPkt;1223             }1224         }1225     } 91     { {IsWrite, IsRequest, IsEviction, HasData, FromCache}, 92             InvalidCmd, \"WritebackDirty\" }, 93     /* WritebackClean - This allows the upstream cache to writeback a 94      * line to the downstream cache without it being considered 95      * dirty. */ 96     { {IsWrite, IsRequest, IsEviction, HasData, FromCache}, 97             InvalidCmd, \"WritebackClean\" },101     /* CleanEvict */102     { {IsRequest, IsEviction, FromCache}, InvalidCmd, \"CleanEvict\" },access4: handle writeback packets1227     // The critical latency part of a write depends only on the tag access1228     if (pkt-&gt;isWrite()) {1229         lat = calculateTagOnlyLatency(pkt-&gt;headerDelay, tag_latency);1230     }1231 1232     // Writeback handling is special case.  We can write the block into1233     // the cache without having a writeable copy (or any copy at all).1234     if (pkt-&gt;isWriteback()) {1235         assert(blkSize == pkt-&gt;getSize());1236 1237         // we could get a clean writeback while we are having1238         // outstanding accesses to a block, do the simple thing for1239         // now and drop the clean writeback so that we do not upset1240         // any ordering/decisions about ownership already taken1241         if (pkt-&gt;cmd == MemCmd::WritebackClean &amp;&amp;1242             mshrQueue.findMatch(pkt-&gt;getAddr(), pkt-&gt;isSecure())) {1243             DPRINTF(Cache, \"Clean writeback %#llx to block with MSHR, \"1244                     \"dropping\\n\", pkt-&gt;getAddr());1245 1246             // A writeback searches for the block, then writes the data.1247             // As the writeback is being dropped, the data is not touched,1248             // and we just had to wait for the time to find a match in the1249             // MSHR. As of now assume a mshr queue search takes as long as1250             // a tag lookup for simplicity.1251             return true;1252         }1253 1254         const bool has_old_data = blk &amp;&amp; blk-&gt;isValid();1255         if (!blk) {1256             // need to do a replacement1257             blk = allocateBlock(pkt, writebacks);1258             if (!blk) {1259                 // no replaceable block available: give up, fwd to next level.1260                 incMissCount(pkt);1261                 return false;1262             }1263 1264             blk-&gt;setCoherenceBits(CacheBlk::ReadableBit);1265         } else if (compressor) {1266             // This is an overwrite to an existing block, therefore we need1267             // to check for data expansion (i.e., block was compressed with1268             // a smaller size, and now it doesn't fit the entry anymore).1269             // If that is the case we might need to evict blocks.1270             if (!updateCompressionData(blk, pkt-&gt;getConstPtr&lt;uint64_t&gt;(),1271                 writebacks)) {1272                 invalidateBlock(blk);1273                 return false;1274             }1275         }1276 1277         // only mark the block dirty if we got a writeback command,1278         // and leave it as is for a clean writeback1279         if (pkt-&gt;cmd == MemCmd::WritebackDirty) {1280             // TODO: the coherent cache can assert that the dirty bit is set1281             blk-&gt;setCoherenceBits(CacheBlk::DirtyBit);1282         }1283         // if the packet does not have sharers, it is passing1284         // writable, and we got the writeback in Modified or Exclusive1285         // state, if not we are in the Owned or Shared state1286         if (!pkt-&gt;hasSharers()) {1287             blk-&gt;setCoherenceBits(CacheBlk::WritableBit);1288         }1289         // nothing else to do; writeback doesn't expect response1290         assert(!pkt-&gt;needsResponse());1291 1292         updateBlockData(blk, pkt, has_old_data);1293         DPRINTF(Cache, \"%s new state is %s\\n\", __func__, blk-&gt;print());1294         incHitCount(pkt);1295 1296         // When the packet metadata arrives, the tag lookup will be done while1297         // the payload is arriving. Then the block will be ready to access as1298         // soon as the fill is done1299         blk-&gt;setWhenReady(clockEdge(fillLatency) + pkt-&gt;headerDelay +1300             std::max(cyclesToTicks(tag_latency), (uint64_t)pkt-&gt;payloadDelay));1301 1302         return true;1303     } else if (pkt-&gt;cmd == MemCmd::CleanEvict) {The GEM5 defines the condition for writeback as below. 229     /** 230      * A writeback is an eviction that carries data. 231      */ 232     bool isWriteback() const       { return testCmdAttrib(IsEviction) &amp;&amp; 233                                             testCmdAttrib(HasData); }When the request packet sets IsEviction and HasData, it means that current request packet invoked the access functionwas the writeback request packet. Below code specified the commands that satisfy above condition. 91     { {IsWrite, IsRequest, IsEviction, HasData, FromCache}, 92             InvalidCmd, \"WritebackDirty\" }, 93     /* WritebackClean - This allows the upstream cache to writeback a 94      * line to the downstream cache without it being considered 95      * dirty. */ 96     { {IsWrite, IsRequest, IsEviction, HasData, FromCache}, 97             InvalidCmd, \"WritebackClean\" },When those conditions are met, the access function handles writeback packet.\\XXX{need more explain for this case}access5: handle CleanEvict and writeClean packets1303     } else if (pkt-&gt;cmd == MemCmd::CleanEvict) {1304         // A CleanEvict does not need to access the data array1305         lat = calculateTagOnlyLatency(pkt-&gt;headerDelay, tag_latency);1306 1307         if (blk) {1308             // Found the block in the tags, need to stop CleanEvict from1309             // propagating further down the hierarchy. Returning true will1310             // treat the CleanEvict like a satisfied write request and delete1311             // it.1312             return true;1313         }1314         // We didn't find the block here, propagate the CleanEvict further1315         // down the memory hierarchy. Returning false will treat the CleanEvict1316         // like a Writeback which could not find a replaceable block so has to1317         // go to next level.1318         return false;1319     } else if (pkt-&gt;cmd == MemCmd::WriteClean) {1320         // WriteClean handling is a special case. We can allocate a1321         // block directly if it doesn't exist and we can update the1322         // block immediately. The WriteClean transfers the ownership1323         // of the block as well.1324         assert(blkSize == pkt-&gt;getSize());1325 1326         const bool has_old_data = blk &amp;&amp; blk-&gt;isValid();1327         if (!blk) {1328             if (pkt-&gt;writeThrough()) {1329                 // if this is a write through packet, we don't try to1330                 // allocate if the block is not present1331                 return false;1332             } else {1333                 // a writeback that misses needs to allocate a new block1334                 blk = allocateBlock(pkt, writebacks);1335                 if (!blk) {1336                     // no replaceable block available: give up, fwd to1337                     // next level.1338                     incMissCount(pkt);1339                     return false;1340                 }1341 1342                 blk-&gt;setCoherenceBits(CacheBlk::ReadableBit);1343             }1344         } else if (compressor) {1345             // This is an overwrite to an existing block, therefore we need1346             // to check for data expansion (i.e., block was compressed with1347             // a smaller size, and now it doesn't fit the entry anymore).1348             // If that is the case we might need to evict blocks.1349             if (!updateCompressionData(blk, pkt-&gt;getConstPtr&lt;uint64_t&gt;(),1350                 writebacks)) {1351                 invalidateBlock(blk);1352                 return false;1353             }1354         }1355 1356         // at this point either this is a writeback or a write-through1357         // write clean operation and the block is already in this1358         // cache, we need to update the data and the block flags1359         assert(blk);1360         // TODO: the coherent cache can assert that the dirty bit is set1361         if (!pkt-&gt;writeThrough()) {1362             blk-&gt;setCoherenceBits(CacheBlk::DirtyBit);1363         }1364         // nothing else to do; writeback doesn't expect response1365         assert(!pkt-&gt;needsResponse());1366 1367         updateBlockData(blk, pkt, has_old_data);1368         DPRINTF(Cache, \"%s new state is %s\\n\", __func__, blk-&gt;print());1369 1370         incHitCount(pkt);1371 1372         // When the packet metadata arrives, the tag lookup will be done while1373         // the payload is arriving. Then the block will be ready to access as1374         // soon as the fill is done1375         blk-&gt;setWhenReady(clockEdge(fillLatency) + pkt-&gt;headerDelay +1376             std::max(cyclesToTicks(tag_latency), (uint64_t)pkt-&gt;payloadDelay));1377 1378         // If this a write-through packet it will be sent to cache below1379         return !pkt-&gt;writeThrough();access6: handle normal read or write request to existing block with adequate properties1380     } else if (blk &amp;&amp; (pkt-&gt;needsWritable() ?1381             blk-&gt;isSet(CacheBlk::WritableBit) :1382             blk-&gt;isSet(CacheBlk::ReadableBit))) {1383         // OK to satisfy access1384         incHitCount(pkt);1385 1386         // Calculate access latency based on the need to access the data array1387         if (pkt-&gt;isRead()) {1388             lat = calculateAccessLatency(blk, pkt-&gt;headerDelay, tag_latency);1389 1390             // When a block is compressed, it must first be decompressed1391             // before being read. This adds to the access latency.1392             if (compressor) {1393                 lat += compressor-&gt;getDecompressionLatency(blk);1394             }1395         } else {1396             lat = calculateTagOnlyLatency(pkt-&gt;headerDelay, tag_latency);1397         }1398 1399         satisfyRequest(pkt, blk);1400         maintainClusivity(pkt-&gt;fromCache(), blk);1401 1402         return true;1403     }1404 To handle the read and write access to existing cache block,it first should check the properties of the existing cache block such as writable and readable bit.If those conditions of the cached block met requirement of the current request’s type such as read or write,then it can be handled in the above condition statement. Note that it returns true at the end because the request can be handled with the cached block, which means cache hit.Also, it invokes the satisfyRequest function to \\XXX{do what?}The satisfyRequest function is the virtual function of the BaseCacheand implemented also by its children class Cache class.There are two main places that satisfyRequest is invoked,the access function and serviceMSHRTarget.access7: other cases, mainly cache misses1405     // Can't satisfy access normally... either no block (blk == nullptr)1406     // or have block but need writable1407 1408     incMissCount(pkt);1409 1410     lat = calculateAccessLatency(blk, pkt-&gt;headerDelay, tag_latency);1411 1412     if (!blk &amp;&amp; pkt-&gt;isLLSC() &amp;&amp; pkt-&gt;isWrite()) {1413         // complete miss on store conditional... just give up now1414         pkt-&gt;req-&gt;setExtraData(0);1415         return true;1416     }1417 1418     return false;1419 }This cases includes cache misses, write request to non-writable block,or read request to non-readable block, etc.When all conditions doesn’t match with the current’ request,then it should be handled by the rest of the recvTimingReq function, particularly cache miss handling.Note that it returns false.Revisiting revTimingReq of the BaseCache to handle cache hit and miss 349 void 350 BaseCache::recvTimingReq(PacketPtr pkt) ...... 371     // Here we charge the headerDelay that takes into account the latencies 372     // of the bus, if the packet comes from it. 373     // The latency charged is just the value set by the access() function. 374     // In case of a hit we are neglecting response latency. 375     // In case of a miss we are neglecting forward latency. 376     Tick request_time = clockEdge(lat); 377     // Here we reset the timing of the packet. 378     pkt-&gt;headerDelay = pkt-&gt;payloadDelay = 0; 379      380     if (satisfied) { 381         // notify before anything else as later handleTimingReqHit might turn 382         // the packet in a response 383         ppHit-&gt;notify(pkt); 384          385         if (prefetcher &amp;&amp; blk &amp;&amp; blk-&gt;wasPrefetched()) { 386             DPRINTF(Cache, \"Hit on prefetch for addr %#x (%s)\\n\", 387                     pkt-&gt;getAddr(), pkt-&gt;isSecure() ? \"s\" : \"ns\"); 388             blk-&gt;clearPrefetched(); 389         } 390          391         handleTimingReqHit(pkt, blk, request_time); 392     } else { 393         handleTimingReqMiss(pkt, blk, forward_time, request_time); 394          395         ppMiss-&gt;notify(pkt); 396     } 397      398     if (prefetcher) { 399         // track time of availability of next prefetch, if any 400         Tick next_pf_time = prefetcher-&gt;nextPrefetchReadyTime(); 401         if (next_pf_time != MaxTick) { 402             schedMemSideSendEvent(next_pf_time); 403         } 404     } 405 }After executing the access function that asks caches if the requested data exists in the cache, it returns value to indicate whether there was an item in the cache or not.The satisfied variable contains the return value of the access.Therefore, based on the satisfied condition,it should handle cache hit and miss event differently.When the cache hit happens 223 void 224 BaseCache::handleTimingReqHit(PacketPtr pkt, CacheBlk *blk, Tick request_time) 225 { 226     if (pkt-&gt;needsResponse()) { 227         // These delays should have been consumed by now 228         assert(pkt-&gt;headerDelay == 0); 229         assert(pkt-&gt;payloadDelay == 0); 230  231         pkt-&gt;makeTimingResponse(); 232  233         // In this case we are considering request_time that takes 234         // into account the delay of the xbar, if any, and just 235         // lat, neglecting responseLatency, modelling hit latency 236         // just as the value of lat overriden by access(), which calls 237         // the calculateAccessLatency() function. 238         cpuSidePort.schedTimingResp(pkt, request_time); 239     } else { 240         DPRINTF(Cache, \"%s satisfied %s, no response needed\\n\", __func__, 241                 pkt-&gt;print()); 242  243         // queue the packet for deletion, as the sending cache is 244         // still relying on it; if the block is found in access(), 245         // CleanEvict and Writeback messages will be deleted 246         // here as well 247         pendingDelete.reset(pkt); 248     } 249 }Based on the request type of the memory operation, it may or may not require response.Therefore, it first checks whether the packet requires response with the needsResponse method. When it requires response, it invokes schedTimingResp of the cpuSidePort. 93     void schedTimingResp(PacketPtr pkt, Tick when) 94     { respQueue.schedSendTiming(pkt, when); }The schedTimingResp function is defined in the QueuedResponsePort classwhich is one of the ancestor class of the CpuSidePort class. Also, schedSendTiming is defined as the member function of the RespPacketQueue which is the type of the respQueue. The PacketQueue class defines the schedSendTiming method, and the RespPacketQueue inherits PacketQueue.106 void107 PacketQueue::schedSendTiming(PacketPtr pkt, Tick when)108 {109     DPRINTF(PacketQueue, \"%s for %s address %x size %d when %lu ord: %i\\n\",110             __func__, pkt-&gt;cmdString(), pkt-&gt;getAddr(), pkt-&gt;getSize(), when,111             forceOrder);112 113     // we can still send a packet before the end of this tick114     assert(when &gt;= curTick());115 116     // express snoops should never be queued117     assert(!pkt-&gt;isExpressSnoop());118 119     // add a very basic sanity check on the port to ensure the120     // invisible buffer is not growing beyond reasonable limits121     if (!_disableSanityCheck &amp;&amp; transmitList.size() &gt; 128) {122         panic(\"Packet queue %s has grown beyond 128 packets\\n\",123               name());124     }125 126     // we should either have an outstanding retry, or a send event127     // scheduled, but there is an unfortunate corner case where the128     // x86 page-table walker and timing CPU send out a new request as129     // part of the receiving of a response (called by130     // PacketQueue::sendDeferredPacket), in which we end up calling131     // ourselves again before we had a chance to update waitingOnRetry132     // assert(waitingOnRetry || sendEvent.scheduled());133 134     // this belongs in the middle somewhere, so search from the end to135     // order by tick; however, if forceOrder is set, also make sure136     // not to re-order in front of some existing packet with the same137     // address138     auto it = transmitList.end();139     while (it != transmitList.begin()) {140         --it;141         if ((forceOrder &amp;&amp; it-&gt;pkt-&gt;matchAddr(pkt)) || it-&gt;tick &lt;= when) {142             // emplace inserts the element before the position pointed to by143             // the iterator, so advance it one step144             transmitList.emplace(++it, when, pkt);145             return;146         }147     }148     // either the packet list is empty or this has to be inserted149     // before every other packet150     transmitList.emplace_front(when, pkt);151     schedSendEvent(when);152 }transmitList maintains all the packets need to be sent to other end of the port 68     /** A deferred packet, buffered to transmit later. */ 69     class DeferredPacket 70     { 71       public: 72         Tick tick;      ///&lt; The tick when the packet is ready to transmit 73         PacketPtr pkt;  ///&lt; Pointer to the packet to transmit 74         DeferredPacket(Tick t, PacketPtr p) 75             : tick(t), pkt(p) 76         {} 77     }; 78  79     typedef std::list&lt;DeferredPacket&gt; DeferredPacketList; 80  81     /** A list of outgoing packets. */ 82     DeferredPacketList transmitList; 83 tranmitList contains all the deferrredPackets that are waiting to be sent.Therefore, it contains the packet itself and when should it be sent.Note that when which is the Tick is required because GEM5 is emulator not the hardware. Anyway the maintained packets will be sent when the schedSendEvent fires. Note that it is scheduled to be fired at when clock cycle through schedSendEvent function.schedSendEvent function schedules event to handle the deferred packet154 void155 PacketQueue::schedSendEvent(Tick when)156 {157     // if we are waiting on a retry just hold off158     if (waitingOnRetry) {159         DPRINTF(PacketQueue, \"Not scheduling send as waiting for retry\\n\");160         assert(!sendEvent.scheduled());161         return;162     }163 164     if (when != MaxTick) {165         // we cannot go back in time, and to be consistent we stick to166         // one tick in the future167         when = std::max(when, curTick() + 1);168         // @todo Revisit the +1169 170         if (!sendEvent.scheduled()) {171             em.schedule(&amp;sendEvent, when);172         } else if (when &lt; sendEvent.when()) {173             // if the new time is earlier than when the event174             // currently is scheduled, move it forward175             em.reschedule(&amp;sendEvent, when);176         }177     } else {178         // we get a MaxTick when there is no more to send, so if we're179         // draining, we may be done at this point180         if (drainState() == DrainState::Draining &amp;&amp;181             transmitList.empty() &amp;&amp; !sendEvent.scheduled()) {182 183             DPRINTF(Drain, \"PacketQueue done draining,\"184                     \"processing drain event\\n\");185             signalDrainDone();186         }187     }188 }The most important things done by the schedSendEvent is the scheduling event to make it fire at the exact time specified by the GEM5 emulator. As shown in Line 170-176,it first checks whether the sendEvent is already scheduled before.If there is no scheduled event, then it schedule the event with schedule function.Note that the em member field points to the BaseCache. Also, if there is already pre-scheduled event for the sendEvent and if the current event should be raised before the pre-scheduled one,then it reschedule the event. BTW, if there were events that should be handled later then newly scheduled event,how those events can be processed!?To understand the how the deferred packet will be processed and resolve question, let’s take a look at the function invoked when the scheduled event raises.processSendEvent: event to handle deferred packet processing 50 PacketQueue::PacketQueue(EventManager&amp; _em, const std::string&amp; _label, 51                          const std::string&amp; _sendEventName, 52                          bool force_order, 53                          bool disable_sanity_check) 54     : em(_em), sendEvent([this]{ processSendEvent(); }, _sendEventName), 55       _disableSanityCheck(disable_sanity_check), 56       forceOrder(force_order), 57       label(_label), waitingOnRetry(false) 58 { 59 }......220 void 221 PacketQueue::processSendEvent()222 {223     assert(!waitingOnRetry);224     sendDeferredPacket();225 }I can easily find that the sendEvent is initialized with processSendEventin the constructor of the PacketQueue. Therefore, when the sendEvent fires, it invokes the processSendEvent function.Note that it further invokes sendDeferredPacket function of the PacketQueue.sendDeferredPacket handles deferred packet processing at right time190 void 191 PacketQueue::sendDeferredPacket()192 {193     // sanity checks194     assert(!waitingOnRetry);195     assert(deferredPacketReady());196 197     DeferredPacket dp = transmitList.front();198 199     // take the packet of the list before sending it, as sending of200     // the packet in some cases causes a new packet to be enqueued201     // (most notaly when responding to the timing CPU, leading to a 202     // new request hitting in the L1 icache, leading to a new203     // response)204     transmitList.pop_front();205 206     // use the appropriate implementation of sendTiming based on the207     // type of queue208     waitingOnRetry = !sendTiming(dp.pkt);209 210     // if we succeeded and are not waiting for a retry, schedule the211     // next send 212     if (!waitingOnRetry) {213         schedSendEvent(deferredPacketReadyTime());214     } else {215         // put the packet back at the front of the list 216         transmitList.emplace_front(dp);217     }    218 }You might remember that the transmitList contains all the packet and when should it be fired.And because the sendDeferredPacket is the function that process the packet in the transmitListat the right time specified. Therefore, the sendDeferredPacket extracts the packet from the transmitList (line 197-204).After getting the packet to send, it invokes sendTiming function to actually send the packet to the unit that waits for the response. However, you can find that sendTiming function is not implemented on the PacketQueue, and implemented as a virtual function, which means it should invoke its child’s sendTiming.Remind that the schedTimingResp of the cpuSidePort makes us to all the way down to here. Also the respQueue used to schedule sendTiming event was the RespPacketQueue object.And the RespPacketQueue inherits PacketQueue, which means it has the sendTiming function.275 bool276 RespPacketQueue::sendTiming(PacketPtr pkt)277 {278     return cpuSidePort.sendTimingResp(pkt);279 }Finally it invokes sendTimingResp function of the cpuSidePort to send packet to the CPU.Yeah… It is kind of a long detour to get to the sendTimingResp.The important reason of this complicated process for handling packets is because it wants to decouple the CpuSidePort from the managing response packets.After the cache generates the response packet, instead of directly invoking the sendTimingResp function of the cpuSidePort it let the PacketQueue handles all relevant operations to manage response packets.Anyway, after sendTimingResp is invoked, it returns the waitingOnRetry which indicates whether the CPU is currently not available for receiving the response packet from the cache. In that case, the waitingOnRetry field is set and should send the packet once againwhen the CPU send the retry message to the cache at some point.169     /**170      * Get the next packet ready time.171      */172     Tick deferredPacketReadyTime() const173     { return transmitList.empty() ? MaxTick : transmitList.front().tick; }Now this is the time for answering previous question: after one packet is processed,if there are remaining packets need to be sent at some later point, what should we do?Yeah the deferredPacketReadyTime checks the transmitList and returns the tick if deferred packet still remains. This tick is passed to the schedSendEvent function, and will schedule the sendEvent. That’s it!waitingOnRetry\\TODO{need to explain some particular details regarding waitingOnRetry}When the cache miss happensWhen the access function cannot return cache block associated with current request, the satisfied condition is set as false.Therefore, the handleTimingReqMiss function is executed to fetch cache block from the upper level cache or memory. 323 void 324 Cache::handleTimingReqMiss(PacketPtr pkt, CacheBlk *blk, Tick forward_time, 325                            Tick request_time) 326 { 327     if (pkt-&gt;req-&gt;isUncacheable()) { 328         // ignore any existing MSHR if we are dealing with an 329         // uncacheable request 330  331         // should have flushed and have no valid block 332         assert(!blk || !blk-&gt;isValid()); 333  334         stats.cmdStats(pkt).mshrUncacheable[pkt-&gt;req-&gt;requestorId()]++; 335  336         if (pkt-&gt;isWrite()) { 337             allocateWriteBuffer(pkt, forward_time); 338         } else { 339             assert(pkt-&gt;isRead()); 340  341             // uncacheable accesses always allocate a new MSHR 342  343             // Here we are using forward_time, modelling the latency of 344             // a miss (outbound) just as forwardLatency, neglecting the 345             // lookupLatency component. 346             allocateMissBuffer(pkt, forward_time); 347         } 348  349         return; 350     } 351  352     Addr blk_addr = pkt-&gt;getBlockAddr(blkSize); 353  354     MSHR *mshr = mshrQueue.findMatch(blk_addr, pkt-&gt;isSecure()); 355  356     // Software prefetch handling: 357     // To keep the core from waiting on data it won't look at 358     // anyway, send back a response with dummy data. Miss handling 359     // will continue asynchronously. Unfortunately, the core will 360     // insist upon freeing original Packet/Request, so we have to 361     // create a new pair with a different lifecycle. Note that this 362     // processing happens before any MSHR munging on the behalf of 363     // this request because this new Request will be the one stored 364     // into the MSHRs, not the original. 365     if (pkt-&gt;cmd.isSWPrefetch()) { 366         assert(pkt-&gt;needsResponse()); 367         assert(pkt-&gt;req-&gt;hasPaddr()); 368         assert(!pkt-&gt;req-&gt;isUncacheable()); 369  370         // There's no reason to add a prefetch as an additional target 371         // to an existing MSHR. If an outstanding request is already 372         // in progress, there is nothing for the prefetch to do. 373         // If this is the case, we don't even create a request at all. 374         PacketPtr pf = nullptr; 375  376         if (!mshr) { 377             // copy the request and create a new SoftPFReq packet 378             RequestPtr req = std::make_shared&lt;Request&gt;(pkt-&gt;req-&gt;getPaddr(), 379                                                     pkt-&gt;req-&gt;getSize(), 380                                                     pkt-&gt;req-&gt;getFlags(), 381                                                     pkt-&gt;req-&gt;requestorId()); 382             pf = new Packet(req, pkt-&gt;cmd); 383             pf-&gt;allocate(); 384             assert(pf-&gt;matchAddr(pkt)); 385             assert(pf-&gt;getSize() == pkt-&gt;getSize()); 386         } 387  388         pkt-&gt;makeTimingResponse(); 389  390         // request_time is used here, taking into account lat and the delay 391         // charged if the packet comes from the xbar. 392         cpuSidePort.schedTimingResp(pkt, request_time); 393  394         // If an outstanding request is in progress (we found an 395         // MSHR) this is set to null 396         pkt = pf; 397     } 398  399     BaseCache::handleTimingReqMiss(pkt, mshr, blk, forward_time, request_time); 400 }When cache miss happens, the first thing to do is searching the MSHR entry.The findMatch function of the mshrQueue containing all the previous MSHR entries will be invoked to search if there is MSHR entry associated with the current request.Whether it has matching MSHR entry or not,it invokes the handleTimingReqMiss of the BaseCache to further handles the cache miss.Briefly speaking, this function handles cache miss based on whether the MSHR entry exists or not.Because this function is quite long, I will split it in two parts: when MSHR exists and when MSHR doesn’t existing.When MSHR does exist 251 void 252 BaseCache::handleTimingReqMiss(PacketPtr pkt, MSHR *mshr, CacheBlk *blk, 253                                Tick forward_time, Tick request_time) 254 { 255     if (writeAllocator &amp;&amp; 256         pkt &amp;&amp; pkt-&gt;isWrite() &amp;&amp; !pkt-&gt;req-&gt;isUncacheable()) { 257         writeAllocator-&gt;updateMode(pkt-&gt;getAddr(), pkt-&gt;getSize(), 258                                    pkt-&gt;getBlockAddr(blkSize)); 259     } 260  261     if (mshr) { 262         /// MSHR hit 263         /// @note writebacks will be checked in getNextMSHR() 264         /// for any conflicting requests to the same block 265          266         //@todo remove hw_pf here 267          268         // Coalesce unless it was a software prefetch (see above). 269         if (pkt) { 270             assert(!pkt-&gt;isWriteback()); 271             // CleanEvicts corresponding to blocks which have 272             // outstanding requests in MSHRs are simply sunk here 273             if (pkt-&gt;cmd == MemCmd::CleanEvict) { 274                 pendingDelete.reset(pkt); 275             } else if (pkt-&gt;cmd == MemCmd::WriteClean) { 276                 // A WriteClean should never coalesce with any 277                 // outstanding cache maintenance requests. 278                  279                 // We use forward_time here because there is an 280                 // uncached memory write, forwarded to WriteBuffer. 281                 allocateWriteBuffer(pkt, forward_time); 282             } else { 283                 DPRINTF(Cache, \"%s coalescing MSHR for %s\\n\", __func__, 284                         pkt-&gt;print()); 285                  286                 assert(pkt-&gt;req-&gt;requestorId() &lt; system-&gt;maxRequestors()); 287                 stats.cmdStats(pkt).mshrHits[pkt-&gt;req-&gt;requestorId()]++; 288                  289                 // We use forward_time here because it is the same 290                 // considering new targets. We have multiple 291                 // requests for the same address here. It 292                 // specifies the latency to allocate an internal 293                 // buffer and to schedule an event to the queued 294                 // port and also takes into account the additional 295                 // delay of the xbar. 296                 mshr-&gt;allocateTarget(pkt, forward_time, order++, 297                                      allocOnFill(pkt-&gt;cmd)); 298                 if (mshr-&gt;getNumTargets() == numTarget) { 299                     noTargetMSHR = mshr; 300                     setBlocked(Blocked_NoTargets); 301                     // need to be careful with this... if this mshr isn't 302                     // ready yet (i.e. time &gt; curTick()), we don't want to 303                     // move it ahead of mshrs that are ready 304                     // mshrQueue.moveToFront(mshr); 305                 } 306             } 307         }You have to understand that one MSHR entry can track multiple memory requests associated with the address handled by the particular MSHR entry. Therefore, the first job needs to be done is registering the missed request to the MSHR entry as its target. Based on the type of the memory request,it might not add the missed request as the targets of the MSHR entry.However, in most of the cases, when the L1 cache miss happens, it will be added to the found MSHR entry by invoking allocateTarget function of the MSHR entry.allocateTarget associates the missed requests to the found MSHR entry372 /*          373  * Adds a target to an MSHR374  */         375 void        376 MSHR::allocateTarget(PacketPtr pkt, Tick whenReady, Counter _order,377                      bool alloc_on_fill)378 {           379     // assume we'd never issue a prefetch when we've got an380     // outstanding miss381     assert(pkt-&gt;cmd != MemCmd::HardPFReq);382                 383     // if there's a request already in service for this MSHR, we will384     // have to defer the new target until after the response if any of385     // the following are true:386     // - there are other targets already deferred387     // - there's a pending invalidate to be applied after the response388     //   comes back (but before this target is processed)389     // - the MSHR's first (and only) non-deferred target is a cache390     //   maintenance packet391     // - the new target is a cache maintenance packet (this is probably392     //   overly conservative but certainly safe)393     // - this target requires a writable block and either we're not394     //   getting a writable block back or we have already snooped395     //   another read request that will downgrade our writable block396     //   to non-writable (Shared or Owned)397     PacketPtr tgt_pkt = targets.front().pkt;398     if (pkt-&gt;req-&gt;isCacheMaintenance() ||399         tgt_pkt-&gt;req-&gt;isCacheMaintenance() ||400         !deferredTargets.empty() ||401         (inService &amp;&amp;402          (hasPostInvalidate() ||403           (pkt-&gt;needsWritable() &amp;&amp;404            (!isPendingModified() || hasPostDowngrade() || isForward))))) {405         // need to put on deferred list406         if (inService &amp;&amp; hasPostInvalidate())407             replaceUpgrade(pkt);408         deferredTargets.add(pkt, whenReady, _order, Target::FromCPU, true,409                             alloc_on_fill);410     } else {411         // No request outstanding, or still OK to append to412         // outstanding request: append to regular target list.  Only413         // mark pending if current request hasn't been issued yet414         // (isn't in service).415         targets.add(pkt, whenReady, _order, Target::FromCPU, !inService,416                     alloc_on_fill);417     }418 419     DPRINTF(MSHR, \"After target allocation: %s\", print());420 }The basic functionality of the allocateTarget is adding the missed memory request to one particular MSHR entries’ target list. Because MSHR collects every memory accesses targeting specific address and maintains them as its targets, this function must associates the missed packet to proper MSHR entry. Also, based on the current condition of the MSHR and pending requests associated with that MSHR entry,the new packet can be added to either deferredTargets and targets.Because they are all TargetList objects, let’s take a look at it first.Target and TargetListThe TargetList is the expanded vector class with Target type. Because one MSHR should record all the memory request associated with that entry, the TargetList vector stores all the missed request and associated information togetherrepresented as a Target type.129     class Target : public QueueEntry::Target130     {   131       public:132         133         enum Source134         {135             FromCPU,136             FromSnoop,137             FromPrefetcher138         };139 140         const Source source;  //!&lt; Request from cpu, memory, or prefetcher?141 142         /**143          * We use this flag to track whether we have cleared the144          * downstreamPending flag for the MSHR of the cache above145          * where this packet originates from and guard noninitial146          * attempts to clear it.147          *148          * The flag markedPending needs to be updated when the149          * TargetList is in service which can be:150          * 1) during the Target instantiation if the MSHR is in151          * service and the target is not deferred,152          * 2) when the MSHR becomes in service if the target is not153          * deferred,154          * 3) or when the TargetList is promoted (deferredTargets -&gt;155          * targets).156          */157         bool markedPending;158 159         const bool allocOnFill;   //!&lt; Should the response servicing this160                                   //!&lt; target list allocate in the cache?161 162         Target(PacketPtr _pkt, Tick _readyTime, Counter _order,163                Source _source, bool _markedPending, bool alloc_on_fill)164             : QueueEntry::Target(_pkt, _readyTime, _order), source(_source),165               markedPending(_markedPending), allocOnFill(alloc_on_fill)166         {}167     };168 169     class TargetList : public std::list&lt;Target&gt;, public Named170     {When no MSHR is present 308     } else { 309         // no MSHR 310         assert(pkt-&gt;req-&gt;requestorId() &lt; system-&gt;maxRequestors()); 311         stats.cmdStats(pkt).mshrMisses[pkt-&gt;req-&gt;requestorId()]++; 312         if (prefetcher &amp;&amp; pkt-&gt;isDemand()) 313             prefetcher-&gt;incrDemandMhsrMisses(); 314  315         if (pkt-&gt;isEviction() || pkt-&gt;cmd == MemCmd::WriteClean) { 316             // We use forward_time here because there is an 317             // writeback or writeclean, forwarded to WriteBuffer. 318             allocateWriteBuffer(pkt, forward_time); 319         } else { 320             if (blk &amp;&amp; blk-&gt;isValid()) { 321                 // If we have a write miss to a valid block, we 322                 // need to mark the block non-readable.  Otherwise 323                 // if we allow reads while there's an outstanding 324                 // write miss, the read could return stale data 325                 // out of the cache block... a more aggressive 326                 // system could detect the overlap (if any) and 327                 // forward data out of the MSHRs, but we don't do 328                 // that yet.  Note that we do need to leave the 329                 // block valid so that it stays in the cache, in 330                 // case we get an upgrade response (and hence no 331                 // new data) when the write miss completes. 332                 // As long as CPUs do proper store/load forwarding 333                 // internally, and have a sufficiently weak memory 334                 // model, this is probably unnecessary, but at some 335                 // point it must have seemed like we needed it... 336                 assert((pkt-&gt;needsWritable() &amp;&amp; 337                     !blk-&gt;isSet(CacheBlk::WritableBit)) || 338                     pkt-&gt;req-&gt;isCacheMaintenance()); 339                 blk-&gt;clearCoherenceBits(CacheBlk::ReadableBit); 340             } 341             // Here we are using forward_time, modelling the latency of 342             // a miss (outbound) just as forwardLatency, neglecting the 343             // lookupLatency component. 344             allocateMissBuffer(pkt, forward_time); 345         } 346     } 347 }It first checks whether the current memory request is Eviction request. Note that cache miss can happen either because of the read and write operation.When it already has a valid block, but the cache access returns miss,it means that the block exists but not writable. In that case, it first set the selected block as non-readable (line 339)because the data should not be read untilthe write miss is resolved through the XBar.To handle the write miss request, it invokes allocateMissBuffer function.allocateMissBuffer: allocate MSHR entry for the write miss event1164     MSHR *allocateMissBuffer(PacketPtr pkt, Tick time, bool sched_send = true)1165     {1166         MSHR *mshr = mshrQueue.allocate(pkt-&gt;getBlockAddr(blkSize), blkSize,1167                                         pkt, time, order++,1168                                         allocOnFill(pkt-&gt;cmd));1169 1170         if (mshrQueue.isFull()) {1171             setBlocked((BlockedCause)MSHRQueue_MSHRs);1172         }1173 1174         if (sched_send) {1175             // schedule the send1176             schedMemSideSendEvent(time);1177         }1178 1179         return mshr;1180     }When there is no MSHR entry associated with current request, the first priority job is allocating new MSHR entry for this memory request and further memory requests.mshrQueue maintains all MSHR entries and provide allocate interfacethat adds new MSHR entry to the queue. After that, because the allocateMissBuffer by default set sched_send parameter,it invokes schedMemSideSendEvent to let the lower level cache or memoryto fetch data. Let’s take a look at how the MSHR entry is allocated and processed by the schedMemSideSendEvent later. 62 MSHR * 63 MSHRQueue::allocate(Addr blk_addr, unsigned blk_size, PacketPtr pkt, 64                     Tick when_ready, Counter order, bool alloc_on_fill) 65 { 66     assert(!freeList.empty()); 67     MSHR *mshr = freeList.front(); 68     assert(mshr-&gt;getNumTargets() == 0); 69     freeList.pop_front(); 70  71     DPRINTF(MSHR, \"Allocating new MSHR. Number in use will be %lu/%lu\\n\", 72             allocatedList.size() + 1, numEntries); 73  74     mshr-&gt;allocate(blk_addr, blk_size, pkt, when_ready, order, alloc_on_fill); 75     mshr-&gt;allocIter = allocatedList.insert(allocatedList.end(), mshr); 76     mshr-&gt;readyIter = addToReadyList(mshr); 77  78     allocated += 1; 79     return mshr; 80 }The MSHRQueue manages entire MSHR entries in the system.Also, the MSHRQueue is the child class of the Queue class.Therefore, to understand how each MSHR entry is allocated,we should take a look at the methods and fields implemented in the Queue class. Note that the Queue is template class so that it can manage any type of queue entries. Each Queue has a list called freeList which have free queue entries typed passed at template initialization.302 void303 MSHR::allocate(Addr blk_addr, unsigned blk_size, PacketPtr target,304                Tick when_ready, Counter _order, bool alloc_on_fill)305 {306     blkAddr = blk_addr;307     blkSize = blk_size;308     isSecure = target-&gt;isSecure();309     readyTime = when_ready;310     order = _order;311     assert(target);312     isForward = false;313     wasWholeLineWrite = false;314     _isUncacheable = target-&gt;req-&gt;isUncacheable();315     inService = false;316     downstreamPending = false;317 318     targets.init(blkAddr, blkSize);319     deferredTargets.init(blkAddr, blkSize);320 321     // Don't know of a case where we would allocate a new MSHR for a322     // snoop (mem-side request), so set source according to request here323     Target::Source source = (target-&gt;cmd == MemCmd::HardPFReq) ?324         Target::FromPrefetcher : Target::FromCPU;325     targets.add(target, when_ready, _order, source, true, alloc_on_fill);326 327     // All targets must refer to the same block328     assert(target-&gt;matchBlockAddr(targets.front().pkt, blkSize));329 }First of all, the retrieved MSHR entry should be initialized. The allocation function of the MSHR objectfirst initialize the targets list. Remember that one MSHR entry can have multiple targets.Also, those targets are maintained by targets and deferredTargets TargetList. Therefore, the two TargetLists should be initialized first.After the initialization, it adds the current request to the targets list.104     typename Entry::Iterator addToReadyList(Entry* entry)105     {106         if (readyList.empty() ||107             readyList.back()-&gt;readyTime &lt;= entry-&gt;readyTime) {108             return readyList.insert(readyList.end(), entry);109         }110 111         for (auto i = readyList.begin(); i != readyList.end(); ++i) {112             if ((*i)-&gt;readyTime &gt; entry-&gt;readyTime) {113                 return readyList.insert(i, entry);114             }115         }116         panic(\"Failed to add to ready list.\");117     } After the MSHR entry is initialized,the packet should also be registered to the readyListof the MSHRQueue. The readyList manages all MSHR entries in ascending order of the readyTime of the initial packet that populated the MSHR entry. Because the MSHR entries should be processed in the readyTime order, when the time specified by the readyTime reaches,the waiting MSHR will be processed. You can think of the readyList is kind of a queue determines the order which entry should be processed first among all MSHR entries.schedMemSideSendEvent: schedule sending deferred packetAfter allocating the MSHR entry for the missed packet, the missed request should be forwarded to the next cache level or the memory based on where the current cache is located on.However, the real hardware cannot process cache miss and forwarding at the same clock cycle.Therefore, it schedules the sending missed cache request packetafter a few clock cycles elapsed. For that purpose, the schedMemSideSendEvent function is invoked.1257     /**1258      * Schedule a send event for the memory-side port. If already1259      * scheduled, this may reschedule the event at an earlier1260      * time. When the specified time is reached, the port is free to1261      * send either a response, a request, or a prefetch request.1262      *      1263      * @param time The time when to attempt sending a packet.1264      */ 1265     void schedMemSideSendEvent(Tick time) 1266     { 1267         memSidePort.schedSendEvent(time);1268     }  We took a look at the schedSendEvent function provided by the PacketQueue. The major job of the function was registering event to process deferred packet and send response to the CpuSidePort.However, note that we are currently looking at the memSidePort’s schedSendEvent. 234     /** 235      * The memory-side port extends the base cache request port with 236      * access functions for functional, atomic and timing snoops. 237      */ 238     class MemSidePort : public CacheRequestPort 239     { 240       private: 241  242         /** The cache-specific queue. */ 243         CacheReqPacketQueue _reqQueue; 244  245         SnoopRespPacketQueue _snoopRespQueue; 246  247         // a pointer to our specific cache implementation 248         BaseCache *cache; 249  250       protected: 251  252         virtual void recvTimingSnoopReq(PacketPtr pkt); 253  254         virtual bool recvTimingResp(PacketPtr pkt); 255  256         virtual Tick recvAtomicSnoop(PacketPtr pkt); 257  258         virtual void recvFunctionalSnoop(PacketPtr pkt); 259  260       public: 261  262         MemSidePort(const std::string &amp;_name, BaseCache *_cache, 263                     const std::string &amp;_label); 264     };Because it doesn’t provide the function schedSendEvent,we should go deeper to its parent class, CacheRequestPort. 143     /** 144      * A cache request port is used for the memory-side port of the 145      * cache, and in addition to the basic timing port that only sends 146      * response packets through a transmit list, it also offers the 147      * ability to schedule and send request packets (requests &amp; 148      * writebacks). The send event is scheduled through schedSendEvent, 149      * and the sendDeferredPacket of the timing port is modified to 150      * consider both the transmit list and the requests from the MSHR. 151      */ 152     class CacheRequestPort : public QueuedRequestPort 153     { 154  155       public: 156  157         /** 158          * Schedule a send of a request packet (from the MSHR). Note 159          * that we could already have a retry outstanding. 160          */ 161         void schedSendEvent(Tick time) 162         { 163             DPRINTF(CachePort, \"Scheduling send event at %llu\\n\", time); 164             reqQueue.schedSendEvent(time); 165         } 166  167       protected: 168  169         CacheRequestPort(const std::string &amp;_name, BaseCache *_cache, 170                         ReqPacketQueue &amp;_reqQueue, 171                         SnoopRespPacketQueue &amp;_snoopRespQueue) : 172             QueuedRequestPort(_name, _cache, _reqQueue, _snoopRespQueue) 173         { } 174  175         /** 176          * Memory-side port always snoops. 177          * 178          * @return always true 179          */ 180         virtual bool isSnooping() const { return true; } 181     };Yeah this has very similar interfaces with the CpuSidePort. However, the schedSendEvent function invokes schedSendEvent function of the reqQueue instead of the respQueue.154 void155 PacketQueue::schedSendEvent(Tick when)156 {157     // if we are waiting on a retry just hold off158     if (waitingOnRetry) {159         DPRINTF(PacketQueue, \"Not scheduling send as waiting for retry\\n\");160         assert(!sendEvent.scheduled());161         return;162     }163 164     if (when != MaxTick) {165         // we cannot go back in time, and to be consistent we stick to166         // one tick in the future167         when = std::max(when, curTick() + 1);168         // @todo Revisit the +1169 170         if (!sendEvent.scheduled()) {171             em.schedule(&amp;sendEvent, when);172         } else if (when &lt; sendEvent.when()) {173             // if the new time is earlier than when the event174             // currently is scheduled, move it forward175             em.reschedule(&amp;sendEvent, when);176         }177     } else {178         // we get a MaxTick when there is no more to send, so if we're179         // draining, we may be done at this point180         if (drainState() == DrainState::Draining &amp;&amp;181             transmitList.empty() &amp;&amp; !sendEvent.scheduled()) {182 183             DPRINTF(Drain, \"PacketQueue done draining,\"184                     \"processing drain event\\n\");185             signalDrainDone();186         }187     }188 }Although the reqQueue type is different from respQueue,note that the same methods are invoked because they both inherit the PacketQueue class. 50 PacketQueue::PacketQueue(EventManager&amp; _em, const std::string&amp; _label, 51                          const std::string&amp; _sendEventName, 52                          bool force_order, 53                          bool disable_sanity_check) 54     : em(_em), sendEvent([this]{ processSendEvent(); }, _sendEventName), 55       _disableSanityCheck(disable_sanity_check), 56       forceOrder(force_order), 57       label(_label), waitingOnRetry(false) 58 { 59 }......220 void 221 PacketQueue::processSendEvent()222 {223     assert(!waitingOnRetry);224     sendDeferredPacket();225 }It schedules sendEvent and involves processSendEvent when the event fires. However, when the sendEvent raises, processSendEvent function invokes different sendDeferredPacket function.Note that respQueue is CacheReqPacketQueue inheriting ReqPacketQueue. Also, the CacheReqPacketQueue overrides sendDeferredPacket implemented in the PacketQueue class. Although the CacheReqPacketQueue inherits the PacketQueue class,the overidden implementation of sendDeferredPacket will be invoked instead.2549 void2550 BaseCache::CacheReqPacketQueue::sendDeferredPacket()2551 {2552     // sanity check2553     assert(!waitingOnRetry);2554 2555     // there should never be any deferred request packets in the2556     // queue, instead we rely on the cache to provide the packets2557     // from the MSHR queue or write queue2558     assert(deferredPacketReadyTime() == MaxTick);2559 2560     // check for request packets (requests &amp; writebacks)2561     QueueEntry* entry = cache.getNextQueueEntry();2562 2563     if (!entry) {2564         // can happen if e.g. we attempt a writeback and fail, but2565         // before the retry, the writeback is eliminated because2566         // we snoop another cache's ReadEx.2567     } else {2568         // let our snoop responses go first if there are responses to2569         // the same addresses2570         if (checkConflictingSnoop(entry-&gt;getTarget()-&gt;pkt)) {2571             return;2572         }2573         waitingOnRetry = entry-&gt;sendPacket(cache);2574     }2575 2576     // if we succeeded and are not waiting for a retry, schedule the2577     // next send considering when the next queue is ready, note that2578     // snoop responses have their own packet queue and thus schedule2579     // their own events2580     if (!waitingOnRetry) {2581         schedSendEvent(cache.nextQueueReadyTime());2582     }2583 }You might remember that the sendDeferredPacket of the PacketQueue utilizes the transmitList to dequeue the packets and send it to the CPU in our previous cache hit cases (sending response to the CPU). However, when the cache miss happens, it needs help from complicated cache units MSHR and writeBuffer. Also, you might have noticed that the packet had not been pushed to the transmitList but MSHR or writeBuffer. Instead of searching the transmitList, it invokes getNextQueueEntry function to find the next entry to process.getNextQueueEntry: select entry to send to the memory either from MSHR or writeBuffer 773 QueueEntry* 774 BaseCache::getNextQueueEntry() 775 { 776     // Check both MSHR queue and write buffer for potential requests, 777     // note that null does not mean there is no request, it could 778     // simply be that it is not ready 779     MSHR *miss_mshr  = mshrQueue.getNext(); 780     WriteQueueEntry *wq_entry = writeBuffer.getNext();When the cache miss happens, the missed request packet could be stored ineither MSHR or WriteBuffer. This is because the sending memory request operations can be issued from two different units depending on the type of the memory request.However, the sending response to the upper cache or processorcan be handled in unified way regardless of the request type.getNext functions return entry which becomes ready to be processedWhen one entry is retrieved with the getNext method in the getNextQueueEntry function, it returns the MSHR entry or writeBack entrythat waits the longest time among them. Note that getNext function is defined in the Queue class, andthe WriteBuffer and MSHRQueue inherits the Queue class.217     /**218      * Returns the WriteQueueEntry at the head of the readyList.219      * @return The next request to service.220      */221     Entry* getNext() const222     {223         if (readyList.empty() || readyList.front()-&gt;readyTime &gt; curTick()) {224             return nullptr;225         }226         return readyList.front();227     }The getNext function returns the first entrystored in the readyList.Note that the front entry of the readyList is the entry that has highest priority based on the readyTime. Therefore, it can process the entry that needs to be handled as soon as possible. 782     // If we got a write buffer request ready, first priority is a 783     // full write buffer, otherwise we favour the miss requests 784     if (wq_entry &amp;&amp; (writeBuffer.isFull() || !miss_mshr)) { 785         // need to search MSHR queue for conflicting earlier miss. 786         MSHR *conflict_mshr = mshrQueue.findPending(wq_entry); 787  788         if (conflict_mshr &amp;&amp; conflict_mshr-&gt;order &lt; wq_entry-&gt;order) { 789             // Service misses in order until conflict is cleared. 790             return conflict_mshr; 791  792             // @todo Note that we ignore the ready time of the conflict here 793         } 794  795         // No conflicts; issue write 796         return wq_entry; 797     } else if (miss_mshr) { 798         // need to check for conflicting earlier writeback 799         WriteQueueEntry *conflict_mshr = writeBuffer.findPending(miss_mshr); 800         if (conflict_mshr) { 801             // not sure why we don't check order here... it was in the 802             // original code but commented out. 803  804             // The only way this happens is if we are 805             // doing a write and we didn't have permissions 806             // then subsequently saw a writeback (owned got evicted) 807             // We need to make sure to perform the writeback first 808             // To preserve the dirty data, then we can issue the write 809  810             // should we return wq_entry here instead?  I.e. do we 811             // have to flush writes in order?  I don't think so... not 812             // for Alpha anyway.  Maybe for x86? 813             return conflict_mshr; 814  815             // @todo Note that we ignore the ready time of the conflict here 816         } 817  818         // No conflicts; issue read 819         return miss_mshr; 820     }After the two entries from the MSHR and writeBack queue are retrieved, it should check condition of two entries to determine which entry should be processed first. It is important to note that the port from the cache unit to the memory is limited resource. However, because we have two input sources to choosewe need to determine which packet retrieved from where should be sent to the memory.Here, the logic put more priority in consuming full writeBuffer.When the writeBuffer is not full, then MSHRqueue will be consumed.Also, even when the writeBuffer is full, if there is conflicting and earlier entry in the MSHR, then the selected entry should be replaced with the conflicting MSHR entry. Otherwise, the selected entry from the writeBuffer will be returned. Based on the comment in the left part of the getNextQueueEntry function,it seems that the selecting order is somewhat controversial, so I will skip them.Generate prefetching request when there is no entries to process 822     // fall through... no pending requests.  Try a prefetch. 823     assert(!miss_mshr &amp;&amp; !wq_entry); 824     if (prefetcher &amp;&amp; mshrQueue.canPrefetch() &amp;&amp; !isBlocked()) { 825         // If we have a miss queue slot, we can try a prefetch 826         PacketPtr pkt = prefetcher-&gt;getPacket(); 827         if (pkt) { 828             Addr pf_addr = pkt-&gt;getBlockAddr(blkSize); 829             if (tags-&gt;findBlock(pf_addr, pkt-&gt;isSecure())) { 830                 DPRINTF(HWPrefetch, \"Prefetch %#x has hit in cache, \" 831                         \"dropped.\\n\", pf_addr); 832                 prefetcher-&gt;pfHitInCache(); 833                 // free the request and packet 834                 delete pkt; 835             } else if (mshrQueue.findMatch(pf_addr, pkt-&gt;isSecure())) { 836                 DPRINTF(HWPrefetch, \"Prefetch %#x has hit in a MSHR, \" 837                         \"dropped.\\n\", pf_addr); 838                 prefetcher-&gt;pfHitInMSHR(); 839                 // free the request and packet 840                 delete pkt; 841             } else if (writeBuffer.findMatch(pf_addr, pkt-&gt;isSecure())) { 842                 DPRINTF(HWPrefetch, \"Prefetch %#x has hit in the \" 843                         \"Write Buffer, dropped.\\n\", pf_addr); 844                 prefetcher-&gt;pfHitInWB(); 845                 // free the request and packet 846                 delete pkt; 847             } else { 848                 // Update statistic on number of prefetches issued 849                 // (hwpf_mshr_misses) 850                 assert(pkt-&gt;req-&gt;requestorId() &lt; system-&gt;maxRequestors()); 851                 stats.cmdStats(pkt).mshrMisses[pkt-&gt;req-&gt;requestorId()]++; 852  853                 // allocate an MSHR and return it, note 854                 // that we send the packet straight away, so do not 855                 // schedule the send 856                 return allocateMissBuffer(pkt, curTick(), false); 857             } 858         } 859     } 860  861     return nullptr; 862 }The fall through pass can only be reachable when there are no suitable request waiting in the writeBuffer and mshrQueue. In that case, it tries to prefetch entries.Note that this prefetching is not software thing, but a hardware prefetcher generated addresses are accessed.Because hardware prefetcher doesn’t know whether the cache or other waiting queues already have entry for that prefetched cache line,it checks them to confirm this is the fresh prefetch request. If it is the fresh request, then add the request to the MSHR.Because the added request will be handled later when the next events happen,so it returns nullptr to report that there is no packet to be sent to the memoryat this cycle.checkConflictingSnoop2563     if (!entry) {2564         // can happen if e.g. we attempt a writeback and fail, but2565         // before the retry, the writeback is eliminated because2566         // we snoop another cache's ReadEx.2567     } else {2568         // let our snoop responses go first if there are responses to2569         // the same addresses2570         if (checkConflictingSnoop(entry-&gt;getTarget()-&gt;pkt)) {2571             return;2572         }2573         waitingOnRetry = entry-&gt;sendPacket(cache);2574     }After the entry is found it should check that whether the found entry has conflicting snoop response. 212         /** 213          * Check if there is a conflicting snoop response about to be 214          * send out, and if so simply stall any requests, and schedule 215          * a send event at the same time as the next snoop response is 216          * being sent out. 217          * 218          * @param pkt The packet to check for conflicts against. 219          */ 220         bool checkConflictingSnoop(const PacketPtr pkt) 221         {    222             if (snoopRespQueue.checkConflict(pkt, cache.blkSize)) { 223                 DPRINTF(CachePort, \"Waiting for snoop response to be \" 224                         \"sent\\n\"); 225                 Tick when = snoopRespQueue.deferredPacketReadyTime(); 226                 schedSendEvent(when); 227                 return true; 228             } 229             return false; 230         }In other words, if there are the waiting snoop responsefor the same address,currently selected entry should be deferred until the snooping response is handled. The deferredPacketReadyTime function calculates the required time to send the snoop response, so thatthe cache miss handling is done after the elapsed time passes (by schedSendEvent). 74 bool              75 PacketQueue::checkConflict(const PacketPtr pkt, const int blk_size) const 76 { 77     // caller is responsible for ensuring that all packets have the 78     // same alignment 79     for (const auto&amp; p : transmitList) { 80         if (p.pkt-&gt;matchBlockAddr(pkt, blk_size)) 81             return true; 82     } 83     return false; 84 }Because the SnoopRespPacketQueue is the child of PacketQueue,it invokes the above checkConflict functionto figure out if there is waiting snoopResponse packet for the same address of the selected entry.finally sendPacketWhen there is no conflict between the selected entry and the snoop response,it will send the request stored in the selected entry.2549 void2550 BaseCache::CacheReqPacketQueue::sendDeferredPacket()......2561     QueueEntry* entry = cache.getNextQueueEntry();25622563     if (!entry) {2564         // can happen if e.g. we attempt a writeback and fail, but2565         // before the retry, the writeback is eliminated because2566         // we snoop another cache's ReadEx.2567     } else {2568         // let our snoop responses go first if there are responses to2569         // the same addresses2570         if (checkConflictingSnoop(entry-&gt;getTarget()-&gt;pkt)) {2571             return;2572         }2573         waitingOnRetry = entry-&gt;sendPacket(cache);2574     }25752576     // if we succeeded and are not waiting for a retry, schedule the2577     // next send considering when the next queue is ready, note that2578     // snoop responses have their own packet queue and thus schedule2579     // their own events2580     if (!waitingOnRetry) {2581         schedSendEvent(cache.nextQueueReadyTime());2582     }2583 }The sendPacket function is defined as a virtual function in the QueueEntry class. Therefore, the corresponding implementation of the sendPacket function should be implemented in the MSHR class and WriteQueueEntry class.Therefore, based on which type of packet is selected,one of below sendPacket implementation will be invoked. Also note that the CacheReqPacketQueue has member field cache which is the reference of the BaseCache. And this cache field is initialized as the cache object itself who owns this CacheReqPacketQueue. In our case it will be the Cache object.705 bool706 MSHR::sendPacket(BaseCache &amp;cache)707 {708     return cache.sendMSHRQueuePacket(this);709 }140 bool141 WriteQueueEntry::sendPacket(BaseCache &amp;cache)142 {143     return cache.sendWriteQueuePacket(this);144 }Processing selected MSHR entryCache::sendMSHRQueuePacket1358 bool1359 Cache::sendMSHRQueuePacket(MSHR* mshr)1360 {1361     assert(mshr);1362 1363     // use request from 1st target1364     PacketPtr tgt_pkt = mshr-&gt;getTarget()-&gt;pkt;1365 1366     if (tgt_pkt-&gt;cmd == MemCmd::HardPFReq &amp;&amp; forwardSnoops) {1367         DPRINTF(Cache, \"%s: MSHR %s\\n\", __func__, tgt_pkt-&gt;print());1368 1369         // we should never have hardware prefetches to allocated1370         // blocks1371         assert(!tags-&gt;findBlock(mshr-&gt;blkAddr, mshr-&gt;isSecure));1372 1373         // We need to check the caches above us to verify that1374         // they don't have a copy of this block in the dirty state1375         // at the moment. Without this check we could get a stale1376         // copy from memory that might get used in place of the1377         // dirty one.1378         Packet snoop_pkt(tgt_pkt, true, false);1379         snoop_pkt.setExpressSnoop();1380         // We are sending this packet upwards, but if it hits we will1381         // get a snoop response that we end up treating just like a1382         // normal response, hence it needs the MSHR as its sender1383         // state1384         snoop_pkt.senderState = mshr;1385         cpuSidePort.sendTimingSnoopReq(&amp;snoop_pkt);1386 1387         // Check to see if the prefetch was squashed by an upper cache (to1388         // prevent us from grabbing the line) or if a Check to see if a1389         // writeback arrived between the time the prefetch was placed in1390         // the MSHRs and when it was selected to be sent or if the1391         // prefetch was squashed by an upper cache.1392 1393         // It is important to check cacheResponding before1394         // prefetchSquashed. If another cache has committed to1395         // responding, it will be sending a dirty response which will1396         // arrive at the MSHR allocated for this request. Checking the1397         // prefetchSquash first may result in the MSHR being1398         // prematurely deallocated.1399         if (snoop_pkt.cacheResponding()) {1400             GEM5_VAR_USED auto r = outstandingSnoop.insert(snoop_pkt.req);1401             assert(r.second);1402 1403             // if we are getting a snoop response with no sharers it1404             // will be allocated as Modified1405             bool pending_modified_resp = !snoop_pkt.hasSharers();1406             markInService(mshr, pending_modified_resp);1407 1408             DPRINTF(Cache, \"Upward snoop of prefetch for addr\"1409                     \" %#x (%s) hit\\n\",1410                     tgt_pkt-&gt;getAddr(), tgt_pkt-&gt;isSecure()? \"s\": \"ns\");1411             return false;1412         }1413 1414         if (snoop_pkt.isBlockCached()) {1415             DPRINTF(Cache, \"Block present, prefetch squashed by cache.  \"1416                     \"Deallocating mshr target %#x.\\n\",1417                     mshr-&gt;blkAddr);1418 1419             // Deallocate the mshr target1420             if (mshrQueue.forceDeallocateTarget(mshr)) {1421                 // Clear block if this deallocation resulted freed an1422                 // mshr when all had previously been utilized1423                 clearBlocked(Blocked_NoMSHRs);1424             }1425 1426             // given that no response is expected, delete Request and Packet1427             delete tgt_pkt;1428 1429             return false;1430         }1431     }1432 1433     return BaseCache::sendMSHRQueuePacket(mshr);1434 }Because we are currently dealing with Cache not the BaseCache,it should first invokes sendMSHRQueuePacket of the Cache class.Although it has pretty complicated code,most of the code are not relevant to general MSHR packet handling. At the end of the function it invokes sendMSHRQueuePacket function of the BaseCache to handle the packets in common scenario.BaseCache::sendMSHRQueuePacket1789 bool1790 BaseCache::sendMSHRQueuePacket(MSHR* mshr)1791 {1792     assert(mshr);1793 1794     // use request from 1st target1795     PacketPtr tgt_pkt = mshr-&gt;getTarget()-&gt;pkt;1796 1797     DPRINTF(Cache, \"%s: MSHR %s\\n\", __func__, tgt_pkt-&gt;print());1798 1799     // if the cache is in write coalescing mode or (additionally) in1800     // no allocation mode, and we have a write packet with an MSHR1801     // that is not a whole-line write (due to incompatible flags etc),1802     // then reset the write mode1803     if (writeAllocator &amp;&amp; writeAllocator-&gt;coalesce() &amp;&amp; tgt_pkt-&gt;isWrite()) {1804         if (!mshr-&gt;isWholeLineWrite()) {1805             // if we are currently write coalescing, hold on the1806             // MSHR as many cycles extra as we need to completely1807             // write a cache line1808             if (writeAllocator-&gt;delay(mshr-&gt;blkAddr)) {1809                 Tick delay = blkSize / tgt_pkt-&gt;getSize() * clockPeriod();1810                 DPRINTF(CacheVerbose, \"Delaying pkt %s %llu ticks to allow \"1811                         \"for write coalescing\\n\", tgt_pkt-&gt;print(), delay);1812                 mshrQueue.delay(mshr, delay);1813                 return false;1814             } else {1815                 writeAllocator-&gt;reset();1816             }1817         } else {1818             writeAllocator-&gt;resetDelay(mshr-&gt;blkAddr);1819         }1820     }1821 1822     CacheBlk *blk = tags-&gt;findBlock(mshr-&gt;blkAddr, mshr-&gt;isSecure);1823 1824     // either a prefetch that is not present upstream, or a normal1825     // MSHR request, proceed to get the packet to send downstream1826     PacketPtr pkt = createMissPacket(tgt_pkt, blk, mshr-&gt;needsWritable(),1827                                      mshr-&gt;isWholeLineWrite());Note that we are currently have information about the MSHR entry selected based on the priority and timing. Therefore, the first job is find the associated cache block if existand generate MissPacket to send it to next level cache or memory.createMissPacketRemind that we are here because of the cache miss event.However, based on the event,the cache miss request might be already associated with specific cache block.For example, when the cache block is allocated and set as non-writable state,the cache miss event happens and make the allocated block as exclusively writable.For that purpose,it should generate proper packet and send it through the XBar to the other components that might share the cache block.Let’s take a look at more details. 476 PacketPtr 477 Cache::createMissPacket(PacketPtr cpu_pkt, CacheBlk *blk, 478                         bool needsWritable, 479                         bool is_whole_line_write) const 480 { 481     // should never see evictions here 482     assert(!cpu_pkt-&gt;isEviction()); 483  484     bool blkValid = blk &amp;&amp; blk-&gt;isValid(); 485  486     if (cpu_pkt-&gt;req-&gt;isUncacheable() || 487         (!blkValid &amp;&amp; cpu_pkt-&gt;isUpgrade()) || 488         cpu_pkt-&gt;cmd == MemCmd::InvalidateReq || cpu_pkt-&gt;isClean()) { 489         // uncacheable requests and upgrades from upper-level caches 490         // that missed completely just go through as is 491         return nullptr; 492     } 493  494     assert(cpu_pkt-&gt;needsResponse()); 495  496     MemCmd cmd; 497     // @TODO make useUpgrades a parameter. 498     // Note that ownership protocols require upgrade, otherwise a 499     // write miss on a shared owned block will generate a ReadExcl, 500     // which will clobber the owned copy. 501     const bool useUpgrades = true; 502     assert(cpu_pkt-&gt;cmd != MemCmd::WriteLineReq || is_whole_line_write); 503     if (is_whole_line_write) { 504         assert(!blkValid || !blk-&gt;isSet(CacheBlk::WritableBit)); 505         // forward as invalidate to all other caches, this gives us 506         // the line in Exclusive state, and invalidates all other 507         // copies 508         cmd = MemCmd::InvalidateReq; 509     } else if (blkValid &amp;&amp; useUpgrades) { 510         // only reason to be here is that blk is read only and we need 511         // it to be writable 512         assert(needsWritable); 513         assert(!blk-&gt;isSet(CacheBlk::WritableBit)); 514         cmd = cpu_pkt-&gt;isLLSC() ? MemCmd::SCUpgradeReq : MemCmd::UpgradeReq; 515     } else if (cpu_pkt-&gt;cmd == MemCmd::SCUpgradeFailReq || 516                cpu_pkt-&gt;cmd == MemCmd::StoreCondFailReq) { 517         // Even though this SC will fail, we still need to send out the 518         // request and get the data to supply it to other snoopers in the case 519         // where the determination the StoreCond fails is delayed due to 520         // all caches not being on the same local bus. 521         cmd = MemCmd::SCUpgradeFailReq; 522     } else { 523         // block is invalid 524  525         // If the request does not need a writable there are two cases 526         // where we need to ensure the response will not fetch the 527         // block in dirty state: 528         // * this cache is read only and it does not perform 529         //   writebacks, 530         // * this cache is mostly exclusive and will not fill (since 531         //   it does not fill it will have to writeback the dirty data 532         //   immediately which generates uneccesary writebacks). 533         bool force_clean_rsp = isReadOnly || clusivity == enums::mostly_excl; 534         cmd = needsWritable ? MemCmd::ReadExReq : 535             (force_clean_rsp ? MemCmd::ReadCleanReq : MemCmd::ReadSharedReq); 536     } 537     PacketPtr pkt = new Packet(cpu_pkt-&gt;req, cmd, blkSize); 538  539     // if there are upstream caches that have already marked the 540     // packet as having sharers (not passing writable), pass that info 541     // downstream 542     if (cpu_pkt-&gt;hasSharers() &amp;&amp; !needsWritable) { 543         // note that cpu_pkt may have spent a considerable time in the 544         // MSHR queue and that the information could possibly be out 545         // of date, however, there is no harm in conservatively 546         // assuming the block has sharers 547         pkt-&gt;setHasSharers(); 548         DPRINTF(Cache, \"%s: passing hasSharers from %s to %s\\n\", 549                 __func__, cpu_pkt-&gt;print(), pkt-&gt;print()); 550     } 551  552     // the packet should be block aligned 553     assert(pkt-&gt;getAddr() == pkt-&gt;getBlockAddr(blkSize)); 554  555     pkt-&gt;allocate(); 556     DPRINTF(Cache, \"%s: created %s from %s\\n\", __func__, pkt-&gt;print(), 557             cpu_pkt-&gt;print()); 558     return pkt; 559 }Most of the time the else condition will be excuted and the ReadExReq packet will be generated for the cache miss event caused by read operation.Sending miss packet !1789 bool1790 BaseCache::sendMSHRQueuePacket(MSHR* mshr)1791 {......1829     mshr-&gt;isForward = (pkt == nullptr);1830 1831     if (mshr-&gt;isForward) {1832         // not a cache block request, but a response is expected1833         // make copy of current packet to forward, keep current1834         // copy for response handling1835         pkt = new Packet(tgt_pkt, false, true);1836         assert(!pkt-&gt;isWrite());1837     }1838 1839     // play it safe and append (rather than set) the sender state,1840     // as forwarded packets may already have existing state1841     pkt-&gt;pushSenderState(mshr);1842 1843     if (pkt-&gt;isClean() &amp;&amp; blk &amp;&amp; blk-&gt;isSet(CacheBlk::DirtyBit)) {1844         // A cache clean opearation is looking for a dirty block. Mark1845         // the packet so that the destination xbar can determine that1846         // there will be a follow-up write packet as well.1847         pkt-&gt;setSatisfied();1848     }1849 1850     if (!memSidePort.sendTimingReq(pkt)) {1851         // we are awaiting a retry, but we1852         // delete the packet and will be creating a new packet1853         // when we get the opportunity1854         delete pkt;1855 1856         // note that we have now masked any requestBus and1857         // schedSendEvent (we will wait for a retry before1858         // doing anything), and this is so even if we do not1859         // care about this packet and might override it before1860         // it gets retried1861         return true;1862     } else {1863         // As part of the call to sendTimingReq the packet is1864         // forwarded to all neighbouring caches (and any caches1865         // above them) as a snoop. Thus at this point we know if1866         // any of the neighbouring caches are responding, and if1867         // so, we know it is dirty, and we can determine if it is1868         // being passed as Modified, making our MSHR the ordering1869         // point1870         bool pending_modified_resp = !pkt-&gt;hasSharers() &amp;&amp;1871             pkt-&gt;cacheResponding();1872         markInService(mshr, pending_modified_resp);1873 1874         if (pkt-&gt;isClean() &amp;&amp; blk &amp;&amp; blk-&gt;isSet(CacheBlk::DirtyBit)) {1875             // A cache clean opearation is looking for a dirty1876             // block. If a dirty block is encountered a WriteClean1877             // will update any copies to the path to the memory1878             // until the point of reference.1879             DPRINTF(CacheVerbose, \"%s: packet %s found block: %s\\n\",1880                     __func__, pkt-&gt;print(), blk-&gt;print());1881             PacketPtr wb_pkt = writecleanBlk(blk, pkt-&gt;req-&gt;getDest(),1882                                              pkt-&gt;id);1883             PacketList writebacks;1884             writebacks.push_back(wb_pkt);1885             doWritebacks(writebacks, 0);1886         }1887 1888         return false;1889     }1890 }end of the recvTimingReq of the cache.Two ports in the cache  92 /**  93  * A basic cache interface. Implements some common functions for speed.  94  */  95 class BaseCache : public ClockedObject  96 {...... 338     CpuSidePort cpuSidePort; 339     MemSidePort memSidePort;CpuSidePort: receive request from the processor and send response 307     /** 308      * The CPU-side port extends the base cache response port with access 309      * functions for functional, atomic and timing requests. 310      */ 311     class CpuSidePort : public CacheResponsePort 312     { 313       private: 314  315         // a pointer to our specific cache implementation 316         BaseCache *cache; 317  318       protected: 319         virtual bool recvTimingSnoopResp(PacketPtr pkt) override; 320  321         virtual bool tryTiming(PacketPtr pkt) override; 322  323         virtual bool recvTimingReq(PacketPtr pkt) override; 324  325         virtual Tick recvAtomic(PacketPtr pkt) override; 326  327         virtual void recvFunctional(PacketPtr pkt) override; 328  329         virtual AddrRangeList getAddrRanges() const override; 330  331       public: 332  333         CpuSidePort(const std::string &amp;_name, BaseCache *_cache, 334                     const std::string &amp;_label); 335  336     }; 337   79 BaseCache::BaseCache(const BaseCacheParams &amp;p, unsigned blk_size)  80     : ClockedObject(p),  81       cpuSidePort (p.name + \".cpu_side_port\", this, \"CpuSidePort\"),  82       memSidePort(p.name + \".mem_side_port\", this, \"MemSidePort\"),  83       mshrQueue(\"MSHRs\", p.mshrs, 0, p.demand_mshr_reserve, p.name),  84       writeBuffer(\"write buffer\", p.write_buffers, p.mshrs, p.name),cpuSidePort is a member field of the BaseCache, but it has cache member fieldwhich is a pointer to the BaseCache.Note that this field is initialized as pointing to the BaseCache itself that embeds the cpuSidePort.Also, it has recvTimingReq function that will be invoked when the processor tries to send request to the cache.CacheResponsePort 266     /** 267      * A cache response port is used for the CPU-side port of the cache, 268      * and it is basically a simple timing port that uses a transmit 269      * list for responses to the CPU (or connected requestor). In 270      * addition, it has the functionality to block the port for 271      * incoming requests. If blocked, the port will issue a retry once 272      * unblocked. 273      */ 274     class CacheResponsePort : public QueuedResponsePort 275     { 276  277       public: 278  279         /** Do not accept any new requests. */ 280         void setBlocked(); 281  282         /** Return to normal operation and accept new requests. */ 283         void clearBlocked(); 284  285         bool isBlocked() const { return blocked; } 286  287       protected: 288  289         CacheResponsePort(const std::string &amp;_name, BaseCache *_cache, 290                        const std::string &amp;_label); 291  292         /** A normal packet queue used to store responses. */ 293         RespPacketQueue queue; 294  295         bool blocked; 296  297         bool mustSendRetry; 298  299       private: 300  301         void processSendRetry(); 302  303         EventFunctionWrapper sendRetryEvent; 304  305     };  69 BaseCache::CacheResponsePort::CacheResponsePort(const std::string &amp;_name,  70                                           BaseCache *_cache,  71                                           const std::string &amp;_label)  72     : QueuedResponsePort(_name, _cache, queue),  73       queue(*_cache, *this, true, _label),  74       blocked(false), mustSendRetry(false),  75       sendRetryEvent([this]{ processSendRetry(); }, _name)  76 {  77 }The CpuSidePort class inherits the CacheResponsePort. The main functionality of the CacheResponsePort is allowing the port to be blocked while it is busy to process previous packets.QueuedResponsePort 53 /** 54  * A queued port is a port that has an infinite queue for outgoing 55  * packets and thus decouples the module that wants to send 56  * request/responses from the flow control (retry mechanism) of the 57  * port. A queued port can be used by both a requestor and a responder. The 58  * queue is a parameter to allow tailoring of the queue implementation 59  * (used in the cache). 60  */       61 class QueuedResponsePort : public ResponsePort 62 {       63  64   protected: 65  66     /** Packet queue used to store outgoing responses. */ 67     RespPacketQueue &amp;respQueue; 68  69     void recvRespRetry() { respQueue.retry(); } 70  71   public: 72  73     /** 74      * Create a QueuedPort with a given name, owner, and a supplied 75      * implementation of a packet queue. The external definition of 76      * the queue enables e.g. the cache to implement a specific queue 77      * behaviuor in a subclass, and provide the latter to the 78      * QueuePort constructor.  79      */ 80     QueuedResponsePort(const std::string&amp; name, SimObject* owner, 81                     RespPacketQueue &amp;resp_queue, PortID id = InvalidPortID) : 82         ResponsePort(name, owner, id), respQueue(resp_queue) 83     { } 84  85     virtual ~QueuedResponsePort() { } 86  87     /** 88      * Schedule the sending of a timing response. 89      * 90      * @param pkt Packet to send 91      * @param when Absolute time (in ticks) to send packet 92      */ 93     void schedTimingResp(PacketPtr pkt, Tick when) 94     { respQueue.schedSendTiming(pkt, when); } 95  96     /** Check the list of buffered packets against the supplied 97      * functional request. */ 98     bool trySatisfyFunctional(PacketPtr pkt) 99     { return respQueue.trySatisfyFunctional(pkt); }100 };ResponsePort259 /**260  * A ResponsePort is a specialization of a port. In addition to the261  * basic functionality of sending packets to its requestor peer, it also262  * has functions specific to a responder, e.g. to send range changes263  * and get the address ranges that the port responds to.264  *265  * The three protocols are atomic, timing, and functional, each with its own266  * header file.267  */268 class ResponsePort : public Port, public AtomicResponseProtocol,269     public TimingResponseProtocol, public FunctionalResponseProtocol270 {271     friend class RequestPort;272 273   private:274     RequestPort* _requestPort;275 276     bool defaultBackdoorWarned;277 278   protected:279     SimObject&amp; owner;280 281   public:282     ResponsePort(const std::string&amp; name, SimObject* _owner,283               PortID id=InvalidPortID);284     virtual ~ResponsePort();285 286     /**287      * Find out if the peer request port is snooping or not.288      *289      * @return true if the peer request port is snooping290      */291     bool isSnooping() const { return _requestPort-&gt;isSnooping(); }292 293     /**294      * Called by the owner to send a range change295      */296     void sendRangeChange() const { _requestPort-&gt;recvRangeChange(); }297 298     /**299      * Get a list of the non-overlapping address ranges the owner is300      * responsible for. All response ports must override this function301      * and return a populated list with at least one item.302      *303      * @return a list of ranges responded to304      */305     virtual AddrRangeList getAddrRanges() const = 0;306 307     /**308      * We let the request port do the work, so these don't do anything.309      */310     void unbind() override {}311     void bind(Port &amp;peer) override {}312 313   public:314     /* The atomic protocol. */315 316     /**317      * Send an atomic snoop request packet, where the data is moved318      * and the state is updated in zero time, without interleaving319      * with other memory accesses.320      *321      * @param pkt Snoop packet to send.322      *323      * @return Estimated latency of access.324      */325     Tick326     sendAtomicSnoop(PacketPtr pkt)327     {328         try {329             return AtomicResponseProtocol::sendSnoop(_requestPort, pkt);330         } catch (UnboundPortException) {331             reportUnbound();332         }333     }334 335   public:336     /* The functional protocol. */337 338     /**339      * Send a functional snoop request packet, where the data is340      * instantly updated everywhere in the memory system, without341      * affecting the current state of any block or moving the block.342      *343      * @param pkt Snoop packet to send.344      */345     void346     sendFunctionalSnoop(PacketPtr pkt) const347     {348         try {349             FunctionalResponseProtocol::sendSnoop(_requestPort, pkt);350         } catch (UnboundPortException) {351             reportUnbound();352         }353     }354 355   public:356     /* The timing protocol. */357 358     /**359      * Attempt to send a timing response to the request port by calling360      * its corresponding receive function. If the send does not361      * succeed, as indicated by the return value, then the sender must362      * wait for a recvRespRetry at which point it can re-issue a363      * sendTimingResp.364      *365      * @param pkt Packet to send.366      *367      * @return If the send was successful or not.368     */369     bool370     sendTimingResp(PacketPtr pkt)371     {372         try {373             return TimingResponseProtocol::sendResp(_requestPort, pkt);374         } catch (UnboundPortException) {375             reportUnbound();376         }377     }378 379     /**380      * Attempt to send a timing snoop request packet to the request port381      * by calling its corresponding receive function. Snoop requests382      * always succeed and hence no return value is needed.383      *384      * @param pkt Packet to send.385      */386     void387     sendTimingSnoopReq(PacketPtr pkt)388     {389         try {390             TimingResponseProtocol::sendSnoopReq(_requestPort, pkt);391         } catch (UnboundPortException) {392             reportUnbound();393         }394     }395 396     /**397      * Send a retry to the request port that previously attempted a398      * sendTimingReq to this response port and failed.399      */400     void401     sendRetryReq()402     {403         try {404             TimingResponseProtocol::sendRetryReq(_requestPort);405         } catch (UnboundPortException) {406             reportUnbound();407         }408     }409 410     /**411      * Send a retry to the request port that previously attempted a412      * sendTimingSnoopResp to this response port and failed.413      */414     void415     sendRetrySnoopResp()416     {417         try {418             TimingResponseProtocol::sendRetrySnoopResp(_requestPort);419         } catch (UnboundPortException) {420             reportUnbound();421         }422     }423 424   protected:425     /**426      * Called by the request port to unbind. Should never be called427      * directly.428      */429     void responderUnbind();430 431     /**432      * Called by the request port to bind. Should never be called433      * directly.434      */435     void responderBind(RequestPort&amp; request_port);436 437     /**438      * Default implementations.439      */440     Tick recvAtomicBackdoor(PacketPtr pkt, MemBackdoorPtr &amp;backdoor) override;441 442     bool443     tryTiming(PacketPtr pkt) override444     {445         panic(\"%s was not expecting a %s\\n\", name(), __func__);446     }447 448     bool449     recvTimingSnoopResp(PacketPtr pkt) override450     {451         panic(\"%s was not expecting a timing snoop response\\n\", name());452     }453 };This is the basic class that provides most of the interfaces required for handling receive operations.Although some operations are not provided by the ResponsePort,but they are provided by the TimingResponseProtocol inherited by the ResponsePort.169 /**170  * Response port171  */172 ResponsePort::ResponsePort(const std::string&amp; name, SimObject* _owner,173     PortID id) : Port(name, id), _requestPort(&amp;defaultRequestPort),174     defaultBackdoorWarned(false), owner(*_owner)175 {176 }177 178 ResponsePort::~ResponsePort()179 {180 }181 182 void183 ResponsePort::responderUnbind()184 {185     _requestPort = &amp;defaultRequestPort;186     Port::unbind();187 }188 189 void190 ResponsePort::responderBind(RequestPort&amp; request_port)191 {192     _requestPort = &amp;request_port;193     Port::bind(request_port);194 }ResponsePort is initialized with defaultRequestPort by default.Because ResponsePort needs to understand who sent the request (_requestPort),the RequestPort object reference should be passed to the ResponsePort at the time of construction.Or dynamically, it can bind to another RequestPort through the responderBind method. When proper RequestPort is not set for the ResponsePort, it will generate error messages during execution of the GEM5.RespPacketQueueOne thing that should be maintained by the QueuedResponsePort is the response packets.When the all cache accesses finished, it should pass the response packet to the processor.However, when the processor is busy not to get the response from the cache,then it should retry later.For that purpose, the QueuedResponsePort contains RespPacketQueue which maintains all the unhandled response packets.300 class RespPacketQueue : public PacketQueue301 {302 303   protected:304 305     ResponsePort&amp; cpuSidePort;306 307     // Static definition so it can be called when constructing the parent308     // without us being completely initialized.309     static const std::string name(const ResponsePort&amp; cpuSidePort,310                                   const std::string&amp; label)311     { return cpuSidePort.name() + \"-\" + label; }312 313   public:314 315     /**316      * Create a response packet queue, linked to an event manager, a317      * CPU-side port, and a label that will be used for functional print318      * request packets.319      *320      * @param _em Event manager used for scheduling this queue321      * @param _cpu_side_port Cpu_side port used to send the packets322      * @param force_order Force insertion order for packets with same address323      * @param _label Label to push on the label stack for print request packets324      */325     RespPacketQueue(EventManager&amp; _em, ResponsePort&amp; _cpu_side_port,326                     bool force_order = false,327                     const std::string _label = \"RespPacketQueue\");328 329     virtual ~RespPacketQueue() { }330 331     const std::string name() const332     { return name(cpuSidePort, label); }333 334     bool sendTiming(PacketPtr pkt);335 336 };266 RespPacketQueue::RespPacketQueue(EventManager&amp; _em,267                                  ResponsePort&amp; _cpu_side_port,268                                  bool force_order,269                                  const std::string _label)270     : PacketQueue(_em, _label, name(_cpu_side_port, _label), force_order),271       cpuSidePort(_cpu_side_port)272 {273 }274 275 bool276 RespPacketQueue::sendTiming(PacketPtr pkt)277 {278     return cpuSidePort.sendTimingResp(pkt);279 }RespPacketQueue has cpuSidePort as its member and initialized by its constructor. When the sendTiming function of the RespPacketQueue is invoked,it sends the packet through the cpuSidePort using the sendTimingResp. Also, note that the RespPacketQueue is initialized with the EventManager object’s reference.However, when you take a look at its initialization in the BaseCache::CacheResponsePort::CacheResponsePort,the queue which is the RespPacketQueue object is initialized with _cache as its first operand. Yeah it is not the EventManager but the BaseCache!Because the BaseCache is SimObject, it must inherit from EventManager class.Therefore, the cache object itself can be handled as the EventManager object. Let’s take a look at the PacketQueue which is the parent class of RespPacketQueue.Also, note that RespPacketQueue itself is not capable of scheduling eventbecause it doesn’t have any member function or field to utilize the passed EventManager, BaseCache.PacketQueueInstead of the RespPacketQueue, its parent class, PacketQueue utilizes the EventManagerand organize events using the schedule method and EventFunctionWrapper. 61 /** 62  * A packet queue is a class that holds deferred packets and later 63  * sends them using the associated CPU-side port or memory-side port. 64  */ 65 class PacketQueue : public Drainable 66 { 67   private: 68     /** A deferred packet, buffered to transmit later. */ 69     class DeferredPacket 70     { 71       public: 72         Tick tick;      ///&lt; The tick when the packet is ready to transmit 73         PacketPtr pkt;  ///&lt; Pointer to the packet to transmit 74         DeferredPacket(Tick t, PacketPtr p) 75             : tick(t), pkt(p) 76         {} 77     }; 78  79     typedef std::list&lt;DeferredPacket&gt; DeferredPacketList; 80  81     /** A list of outgoing packets. */ 82     DeferredPacketList transmitList; 83  84     /** The manager which is used for the event queue */ 85     EventManager&amp; em; 86  87     /** Used to schedule sending of deferred packets. */ 88     void processSendEvent(); 89  90     /** Event used to call processSendEvent. */ 91     EventFunctionWrapper sendEvent; 92  93      /* 94       * Optionally disable the sanity check 95       * on the size of the transmitList. The 96       * sanity check will be enabled by default. 97       */ 98     bool _disableSanityCheck; 99 100     /**101      * if true, inserted packets have to be unconditionally scheduled102      * after the last packet in the queue that references the same103      * address104      */105     bool forceOrder;106 107   protected:108 109     /** Label to use for print request packets label stack. */110     const std::string label;111 112     /** Remember whether we're awaiting a retry. */113     bool waitingOnRetry;114 115     /** Check whether we have a packet ready to go on the transmit list. */116     bool deferredPacketReady() const117     { return !transmitList.empty() &amp;&amp; transmitList.front().tick &lt;= curTick(); }118 119     /**120      * Attempt to send a packet. Note that a subclass of the121      * PacketQueue can override this method and thus change the122      * behaviour (as done by the cache for the request queue). The123      * default implementation sends the head of the transmit list. The124      * caller must guarantee that the list is non-empty and that the125      * head packet is scheduled for curTick() (or earlier).126      */127     virtual void sendDeferredPacket();128 129     /**130      * Send a packet using the appropriate method for the specific131      * subclass (request, response or snoop response).132      */133     virtual bool sendTiming(PacketPtr pkt) = 0;134 135     /**136      * Create a packet queue, linked to an event manager, and a label137      * that will be used for functional print request packets.138      *139      * @param _em Event manager used for scheduling this queue140      * @param _label Label to push on the label stack for print request packets141      * @param force_order Force insertion order for packets with same address142      * @param disable_sanity_check Flag used to disable the sanity check143      *        on the size of the transmitList. The check is enabled by default.144      */145     PacketQueue(EventManager&amp; _em, const std::string&amp; _label,146                 const std::string&amp; _sendEventName,147                 bool force_order = false,148                 bool disable_sanity_check = false);149 150     /**151      * Virtual desctructor since the class may be used as a base class.152      */153     virtual ~PacketQueue();154 155   public:156 157     /**158      * Provide a name to simplify debugging.159      *160      * @return A complete name, appended to module and port161      */162     virtual const std::string name() const = 0;163 164     /**165      * Get the size of the queue.166      */167     size_t size() const { return transmitList.size(); }168 169     /**170      * Get the next packet ready time.171      */172     Tick deferredPacketReadyTime() const173     { return transmitList.empty() ? MaxTick : transmitList.front().tick; }174 175     /**176      * Check if a packet corresponding to the same address exists in the177      * queue.178      *179      * @param pkt The packet to compare against.180      * @param blk_size Block size in bytes.181      * @return Whether a corresponding packet is found.182      */183     bool checkConflict(const PacketPtr pkt, const int blk_size) const;184 185     /** Check the list of buffered packets against the supplied186      * functional request. */187     bool trySatisfyFunctional(PacketPtr pkt);188 189     /**190      * Schedule a send event if we are not already waiting for a191      * retry. If the requested time is before an already scheduled192      * send event, the event will be rescheduled. If MaxTick is193      * passed, no event is scheduled. Instead, if we are idle and194      * asked to drain then check and signal drained.195      *196      * @param when time to schedule an event197      */198     void schedSendEvent(Tick when);199 200     /**201      * Add a packet to the transmit list, and schedule a send event.202      *203      * @param pkt Packet to send204      * @param when Absolute time (in ticks) to send packet205      */206     void schedSendTiming(PacketPtr pkt, Tick when);207 208     /**209      * Retry sending a packet from the queue. Note that this is not210      * necessarily the same packet if something has been added with an211      * earlier time stamp.212      */213     void retry();214 215     /**216       * This allows a user to explicitly disable the sanity check217       * on the size of the transmitList, which is enabled by default.218       * Users must use this function to explicitly disable the sanity219       * check.220       */221     void disableSanityCheck() { _disableSanityCheck = true; }222 223     DrainState drain() override;224 };Port binding 73 class BaseCache(ClockedObject): 74     type = 'BaseCache'......121     cpu_side = ResponsePort(\"Upstream port closer to the CPU and/or device\")122     mem_side = RequestPort(\"Downstream port closer to memory\")gem5/src/python/m5/params.py2123 # Port description object.  Like a ParamDesc object, this represents a2124 # logical port in the SimObject class, not a particular port on a2125 # SimObject instance.  The latter are represented by PortRef objects.2126 class Port(object):2127     # Port(\"role\", \"description\")2128 2129     _compat_dict = { }2130 2131     @classmethod2132     def compat(cls, role, peer):2133         cls._compat_dict.setdefault(role, set()).add(peer)2134         cls._compat_dict.setdefault(peer, set()).add(role)2135 2136     @classmethod2137     def is_compat(cls, one, two):2138         for port in one, two:2139             if not port.role in Port._compat_dict:2140                 fatal(\"Unrecognized role '%s' for port %s\\n\", port.role, port)2141         return one.role in Port._compat_dict[two.role]2142 2143     def __init__(self, role, desc, is_source=False):2144         self.desc = desc2145         self.role = role2146         self.is_source = is_source2147 2148     # Generate a PortRef for this port on the given SimObject with the2149     # given name2150     def makeRef(self, simobj):2151         return PortRef(simobj, self.name, self.role, self.is_source)2152 2153     # Connect an instance of this port (on the given SimObject with2154     # the given name) with the port described by the supplied PortRef2155     def connect(self, simobj, ref):2156         self.makeRef(simobj).connect(ref)2157 2158     # No need for any pre-declarations at the moment as we merely rely2159     # on an unsigned int.2160     def cxx_predecls(self, code):2161         pass2162 2163     def pybind_predecls(self, code):2164         cls.cxx_predecls(self, code)2165 2166     # Declare an unsigned int with the same name as the port, that2167     # will eventually hold the number of connected ports (and thus the2168     # number of elements for a VectorPort).2169     def cxx_decl(self, code):2170         code('unsigned int port_$_connection_count;')2171 2172 Port.compat('GEM5 REQUESTOR', 'GEM5 RESPONDER')2173 2174 class RequestPort(Port):2175     # RequestPort(\"description\")2176     def __init__(self, desc):2177         super(RequestPort, self).__init__(2178                 'GEM5 REQUESTOR', desc, is_source=True)2179 2180 class ResponsePort(Port):2181     # ResponsePort(\"description\")2182     def __init__(self, desc):2183         super(ResponsePort, self).__init__('GEM5 RESPONDER', desc)2184 1896 #####################################################################1897 #1898 # Port objects1899 #1900 # Ports are used to interconnect objects in the memory system.1901 #1902 #####################################################################1903 1904 # Port reference: encapsulates a reference to a particular port on a1905 # particular SimObject.1906 class PortRef(object):......1941     # Full connection is symmetric (both ways).  Called via1942     # SimObject.__setattr__ as a result of a port assignment, e.g.,1943     # \"obj1.portA = obj2.portB\", or via VectorPortElementRef.__setitem__,1944     # e.g., \"obj1.portA[3] = obj2.portB\".1945     def connect(self, other):1946         if isinstance(other, VectorPortRef):1947             # reference to plain VectorPort is implicit append1948             other = other._get_next()1949         if self.peer and not proxy.isproxy(self.peer):1950             fatal(\"Port %s is already connected to %s, cannot connect %s\\n\",1951                   self, self.peer, other);1952         self.peer = other1953 1954         if proxy.isproxy(other):1955             other.set_param_desc(PortParamDesc())1956             return1957         elif not isinstance(other, PortRef):1958             raise TypeError(\"assigning non-port reference '%s' to port '%s'\" \\1959                   % (other, self))1960 1961         if not Port.is_compat(self, other):1962             fatal(\"Ports %s and %s with roles '%s' and '%s' \"1963                     \"are not compatible\", self, other, self.role, other.role)1964 1965         if other.peer is not self:1966             other.connect(self)......2023     # Call C++ to create corresponding port connection between C++ objects2024     def ccConnect(self):2025         if self.ccConnected: # already done this2026             return2027 2028         peer = self.peer2029         if not self.peer: # nothing to connect to2030             return2031 2032         port = self.simobj.getPort(self.name, self.index)2033         peer_port = peer.simobj.getPort(peer.name, peer.index)2034         port.bind(peer_port)2035 2036         self.ccConnected = True127 void128 RequestPort::bind(Port &amp;peer)129 {130     auto *response_port = dynamic_cast&lt;ResponsePort *&gt;(&amp;peer);131     fatal_if(!response_port, \"Can't bind port %s to non-response port %s.\",132              name(), peer.name());133     // request port keeps track of the response port134     _responsePort = response_port;135     Port::bind(peer);136     // response port also keeps track of request port137     _responsePort-&gt;responderBind(*this);138 }189 void190 ResponsePort::responderBind(RequestPort&amp; request_port)191 {192     _requestPort = &amp;request_port;193     Port::bind(request_port);194 } 58 /** 59  * Ports are used to interface objects to each other. 60  */ 61 class Port 62 {116     /** Attach to a peer port. */117     virtual void118     bind(Port &amp;peer)119     {120         _peer = &amp;peer;121         _connected = true;122     } 200 Port &amp; 201 BaseCache::getPort(const std::string &amp;if_name, PortID idx) 202 { 203     if (if_name == \"mem_side\") { 204         return memSidePort; 205     } else if (if_name == \"cpu_side\") { 206         return cpuSidePort; 207     }  else { 208         return ClockedObject::getPort(if_name, idx); 209     } 210 }#######################allocateBlock1529 CacheBlk*1530 BaseCache::allocateBlock(const PacketPtr pkt, PacketList &amp;writebacks)1531 {  1532     // Get address1533     const Addr addr = pkt-&gt;getAddr();1534 1535     // Get secure bit1536     const bool is_secure = pkt-&gt;isSecure();1537 1538     // Block size and compression related access latency. Only relevant if1539     // using a compressor, otherwise there is no extra delay, and the block1540     // is fully sized1541     std::size_t blk_size_bits = blkSize*8;1542     Cycles compression_lat = Cycles(0);1543     Cycles decompression_lat = Cycles(0);1544 1545     // If a compressor is being used, it is called to compress data before1546     // insertion. Although in Gem5 the data is stored uncompressed, even if a1547     // compressor is used, the compression/decompression methods are called to1548     // calculate the amount of extra cycles needed to read or write compressed1549     // blocks.1550     if (compressor &amp;&amp; pkt-&gt;hasData()) {1551         const auto comp_data = compressor-&gt;compress(1552             pkt-&gt;getConstPtr&lt;uint64_t&gt;(), compression_lat, decompression_lat);1553         blk_size_bits = comp_data-&gt;getSizeBits();1554     }1555 1556     // Find replacement victim1557     std::vector&lt;CacheBlk*&gt; evict_blks;1558     CacheBlk *victim = tags-&gt;findVictim(addr, is_secure, blk_size_bits,1559                                         evict_blks);1560    1561     // It is valid to return nullptr if there is no victim1562     if (!victim)1563         return nullptr;1564 1565     // Print victim block's information1566     DPRINTF(CacheRepl, \"Replacement victim: %s\\n\", victim-&gt;print());1567 1568     // Try to evict blocks; if it fails, give up on allocation1569     if (!handleEvictions(evict_blks, writebacks)) {1570         return nullptr;1571     }1572 1573     // Insert new block at victimized entry1574     tags-&gt;insertBlock(pkt, victim);1575 1576     // If using a compressor, set compression data. This must be done after1577     // insertion, as the compression bit may be set.1578     if (compressor) {1579         compressor-&gt;setSizeBits(victim, blk_size_bits);1580         compressor-&gt;setDecompressionLatency(victim, decompression_lat);1581     }1582 1583     return victim;1584 }158     /**159      * Find replacement victim based on address. The list of evicted blocks160      * only contains the victim.161      *162      * @param addr Address to find a victim for.163      * @param is_secure True if the target memory space is secure.164      * @param size Size, in bits, of new block to allocate.165      * @param evict_blks Cache blocks to be evicted.166      * @return Cache block to be replaced.167      */168     CacheBlk* findVictim(Addr addr, const bool is_secure,169                          const std::size_t size,170                          std::vector&lt;CacheBlk*&gt;&amp; evict_blks) override171     {172         // Get possible entries to be victimized173         const std::vector&lt;ReplaceableEntry*&gt; entries =174             indexingPolicy-&gt;getPossibleEntries(addr);175 176         // Choose replacement victim from replacement candidates177         CacheBlk* victim = static_cast&lt;CacheBlk*&gt;(replacementPolicy-&gt;getVictim(178                                 entries));179 180         // There is only one eviction for this replacement181         evict_blks.push_back(victim);182 183         return victim;184     }getPossibleEntries select entries of one set associated with the address passed to the findVictim function.Because it returns N-ways of entries mapped to one set, the getVictim function should search proper entry to evict.As a result, one entry will be selected and pushed into the eviction list.For further memory allocation, the invalidated block is returned. 864 bool 865 BaseCache::handleEvictions(std::vector&lt;CacheBlk*&gt; &amp;evict_blks, 866     PacketList &amp;writebacks) 867 { 868     bool replacement = false; 869     for (const auto&amp; blk : evict_blks) { 870         if (blk-&gt;isValid()) { 871             replacement = true; 872  873             const MSHR* mshr = 874                 mshrQueue.findMatch(regenerateBlkAddr(blk), blk-&gt;isSecure()); 875             if (mshr) { 876                 // Must be an outstanding upgrade or clean request on a block 877                 // we're about to replace 878                 assert((!blk-&gt;isSet(CacheBlk::WritableBit) &amp;&amp; 879                     mshr-&gt;needsWritable()) || mshr-&gt;isCleaning()); 880                 return false; 881             } 882         } 883     } 884  885     // The victim will be replaced by a new entry, so increase the replacement 886     // counter if a valid block is being replaced 887     if (replacement) { 888         stats.replacements++; 889  890         // Evict valid blocks associated to this victim block 891         for (auto&amp; blk : evict_blks) { 892             if (blk-&gt;isValid()) { 893                 evictBlock(blk, writebacks); 894             } 895         } 896     } 897  898     return true; 899 }1606 void1607 BaseCache::evictBlock(CacheBlk *blk, PacketList &amp;writebacks)1608 {1609     PacketPtr pkt = evictBlock(blk);1610     if (pkt) {1611         writebacks.push_back(pkt);1612     }1613 } 899 PacketPtr 900 Cache::evictBlock(CacheBlk *blk) 901 { 902     PacketPtr pkt = (blk-&gt;isSet(CacheBlk::DirtyBit) || writebackClean) ? 903         writebackBlk(blk) : cleanEvictBlk(blk); 904  905     invalidateBlock(blk); 906  907     return pkt; 908 }1586 void1587 BaseCache::invalidateBlock(CacheBlk *blk)1588 {1589     // If block is still marked as prefetched, then it hasn't been used1590     if (blk-&gt;wasPrefetched()) {1591         prefetcher-&gt;prefetchUnused();1592     }1593 1594     // Notify that the data contents for this address are no longer present1595     updateBlockData(blk, nullptr, blk-&gt;isValid());1596 1597     // If handling a block present in the Tags, let it do its invalidation1598     // process, which will update stats and invalidate the block itself1599     if (blk != tempBlock) {1600         tags-&gt;invalidate(blk);1601     } else {1602         tempBlock-&gt;invalidate();1603     }1604 }   gem5/src/mem/cache/tags/base_set_assoc.cc 88 void 89 BaseSetAssoc::invalidate(CacheBlk *blk) 90 { 91     BaseTags::invalidate(blk); 92  93     // Decrease the number of tags in use 94     stats.tagsInUse--; 95  96     // Invalidate replacement data 97     replacementPolicy-&gt;invalidate(blk-&gt;replacementData); 98 }Because the invalidate function of the BaseTag class is virtual function,it should be implemented by its children class.I utilize the base_set_assoc tags for generating cache in my system, so I will follow the implementation of the BaseSetAssoc class. Note that it invokes the invalidate function of the block firstand then invalidate replacement data.gem5/src/mem/cache_blk.hh 70 class CacheBlk : public TaggedEntry 71 { 72   public:......197     /**198      * Invalidate the block and clear all state.199      */200     virtual void invalidate() override201     {202         TaggedEntry::invalidate();203 204         clearPrefetched();205         clearCoherenceBits(AllBits);206 207         setTaskId(context_switch_task_id::Unknown);208         setWhenReady(MaxTick);209         setRefCount(0);210         setSrcRequestorId(Request::invldRequestorId);211         lockList.clear();212     }Although the invalidate function of the CacheBlk is defined as virtual function,the system utilize the CahceBlk class as it is instead of adopting another class inheriting CacheBlk.Therefore, the invalidate function of the CacheBlk is called.Most importantly it inovkes the invalidate function of its parent class TaggedEntry. Also, it clears all the coherence bits and prefetched bitif they are set.gem5/src/mem/tags/tagged_entry 46 class TaggedEntry : public ReplaceableEntry 47 {......102     /** Invalidate the block. Its contents are no longer valid. */103     virtual void invalidate()104     {105         _valid = false;106         setTag(MaxAddr);107         clearSecure();108     }Finally, it sets the _valid member field of the CacheBlk as false and clear secure flag."
  },
  
  {
    "title": "O3 Cpu Commit",
    "url": "/posts/O3-CPU-commit/",
    "categories": "",
    "tags": "",
    "date": "2021-06-02 00:00:00 -0400",
    





    
    "snippet": "Commit (Commit::commitInsts()). Once the instruction reaches the head of ROB, it will be committed and released from ROB.",
    "content": "Commit (Commit::commitInsts()). Once the instruction reaches the head of ROB, it will be committed and released from ROB."
  },
  
  {
    "title": "O3 Cpu Iew",
    "url": "/posts/O3-CPU-iew/",
    "categories": "GEM5, Pipeline, O3",
    "tags": "",
    "date": "2021-06-01 00:00:00 -0400",
    





    
    "snippet": "IEW: Issue/Execute/Writeback  GEM5 handles both execute and writeback when the execute() function is called on an instruction. Therefore, GEM5 combines Issue, Execute, and Writeback stage into one ...",
    "content": "IEW: Issue/Execute/Writeback  GEM5 handles both execute and writeback when the execute() function is called on an instruction. Therefore, GEM5 combines Issue, Execute, and Writeback stage into one stage called IEW. This stage (IEW) handles dispatching instructions to the instruction queue, telling the instruction queue to issue instruction, and executing and writing back instructions.Nice description about the IEW stage provided by the GEM5 Documentation.Also, this documentation provide which functions are mainly designed to achieve those three operations.Rename::tick()-&gt;Rename::RenameInsts()IEW::tick()-&gt;IEW::dispatchInsts()IEW::tick()-&gt;InstructionQueue::scheduleReadyInsts()IEW::tick()-&gt;IEW::executeInsts()IEW::tick()-&gt;IEW::writebackInsts()In this posting, I will explain dispatch, schedule, execute, and write back in details.The commit stage will be studied in the other posting. The tick function of the iew stage is the main body of execution as other stages. Therefore, I will explain each part of the iew stage following the tick implementation. The dispatch function tries to dispatch renamed instructions to the LSQ/IQ (Note that already the rename stage checked availability of the LSQ and IQ)and actually issues instructions every cycle. The execute latency is actually tied to the issue latency to allow the IQ to be able to do back-to-back scheduling without having to speculatively schedule instructions. The IEW separates memory instructions from non-memory instructions.(issuing the instruction to different queues, LSQ or IQ) The writeback portion of IEW completes the instructions,wakes up any dependents, and marks the register as ready on the scoreboard.With those information,IQ can tell which instructions can be woke up and to be issued.Dispatch1502 template&lt;class Impl&gt;1503 void1504 DefaultIEW&lt;Impl&gt;::tick()1505 {1506     wbNumInst = 0;1507     wbCycle = 0;1508 1509     wroteToTimeBuffer = false;1510     updatedQueues = false;1511 1512     ldstQueue.tick();1513 1514     sortInsts();1515 1516     // Free function units marked as being freed this cycle.1517     fuPool-&gt;processFreeUnits();1518 1519     list&lt;ThreadID&gt;::iterator threads = activeThreads-&gt;begin();1520     list&lt;ThreadID&gt;::iterator end = activeThreads-&gt;end();1521 1522     // Check stall and squash signals, dispatch any instructions.1523     while (threads != end) {1524         ThreadID tid = *threads++;1525 1526         DPRINTF(IEW,\"Issue: Processing [tid:%i]\\n\",tid);1527 1528         checkSignalsAndUpdate(tid);1529         dispatch(tid);1530     }As shown in the tick function, after checking signal such as block and squash, the first job done by the IEW is dispatching the renamed instructions. The main goal of the dispatch is inserting the renamed instruction into the IQ and LSQbased on the instruction’s type.Dispatch implementation 911 template&lt;class Impl&gt; 912 void 913 DefaultIEW&lt;Impl&gt;::dispatch(ThreadID tid) 914 { 915     // If status is Running or idle, 916     //     call dispatchInsts() 917     // If status is Unblocking, 918     //     buffer any instructions coming from rename 919     //     continue trying to empty skid buffer 920     //     check if stall conditions have passed 921  922     if (dispatchStatus[tid] == Blocked) { 923         ++iewBlockCycles; 924  925     } else if (dispatchStatus[tid] == Squashing) { 926         ++iewSquashCycles; 927     } 928  929     // Dispatch should try to dispatch as many instructions as its bandwidth 930     // will allow, as long as it is not currently blocked. 931     if (dispatchStatus[tid] == Running || 932         dispatchStatus[tid] == Idle) { 933         DPRINTF(IEW, \"[tid:%i] Not blocked, so attempting to run \" 934                 \"dispatch.\\n\", tid); 935  936         dispatchInsts(tid); 937     } else if (dispatchStatus[tid] == Unblocking) { 938         // Make sure that the skid buffer has something in it if the 939         // status is unblocking. 940         assert(!skidsEmpty()); 941  942         // If the status was unblocking, then instructions from the skid 943         // buffer were used.  Remove those instructions and handle 944         // the rest of unblocking. 945         dispatchInsts(tid); 946  947         ++iewUnblockCycles; 948  949         if (validInstsFromRename()) { 950             // Add the current inputs to the skid buffer so they can be 951             // reprocessed when this stage unblocks. 952             skidInsert(tid); 953         } 954  955         unblock(tid); 956     } 957 }The dispatch function is just a wrapper function of the dispatchInsts. Based on the current status of the dispatch stage, associated operations should be executed in addition to the main dispatch function, dispatchInsts.Because the dispatchInsts is fairly complex, I will explain one by one.Checking availability of resources to dispatch instruction959 template &lt;class Impl&gt; 960 void 961 DefaultIEW&lt;Impl&gt;::dispatchInsts(ThreadID tid) 962 { 963     // Obtain instructions from skid buffer if unblocking, or queue from rename 964     // otherwise. 965     std::queue&lt;DynInstPtr&gt; &amp;insts_to_dispatch = 966         dispatchStatus[tid] == Unblocking ? 967         skidBuffer[tid] : insts[tid]; 968  969     int insts_to_add = insts_to_dispatch.size(); 970  971     DynInstPtr inst; 972     bool add_to_iq = false; 973     int dis_num_inst = 0; 974  975     // Loop through the instructions, putting them in the instruction 976     // queue. 977     for ( ; dis_num_inst &lt; insts_to_add &amp;&amp; 978               dis_num_inst &lt; dispatchWidth; 979           ++dis_num_inst) 980     { 981         inst = insts_to_dispatch.front(); 982  983         if (dispatchStatus[tid] == Unblocking) { 984             DPRINTF(IEW, \"[tid:%i] Issue: Examining instruction from skid \" 985                     \"buffer\\n\", tid); 986         } 987  988         // Make sure there's a valid instruction there. 989         assert(inst); 990  991         DPRINTF(IEW, \"[tid:%i] Issue: Adding PC %s [sn:%lli] [tid:%i] to \" 992                 \"IQ.\\n\", 993                 tid, inst-&gt;pcState(), inst-&gt;seqNum, inst-&gt;threadNumber); 994  995         // Be sure to mark these instructions as ready so that the 996         // commit stage can go ahead and execute them, and mark 997         // them as issued so the IQ doesn't reprocess them. 998  999         // Check for squashed instructions.1000         if (inst-&gt;isSquashed()) {1001             DPRINTF(IEW, \"[tid:%i] Issue: Squashed instruction encountered, \"1002                     \"not adding to IQ.\\n\", tid);1003 1004             ++iewDispSquashedInsts;1005 1006             insts_to_dispatch.pop();1007 1008             //Tell Rename That An Instruction has been processed1009             if (inst-&gt;isLoad()) {1010                 toRename-&gt;iewInfo[tid].dispatchedToLQ++;1011             }1012             if (inst-&gt;isStore() || inst-&gt;isAtomic()) {1013                 toRename-&gt;iewInfo[tid].dispatchedToSQ++;1014             }1015 1016             toRename-&gt;iewInfo[tid].dispatched++;1017    1018             continue;1019         }1020  1021         // Check for full conditions.1022         if (instQueue.isFull(tid)) {1023             DPRINTF(IEW, \"[tid:%i] Issue: IQ has become full.\\n\", tid);1024    1025             // Call function to start blocking.1026             block(tid);1027    1028             // Set unblock to false. Special case where we are using1029             // skidbuffer (unblocking) instructions but then we still1030             // get full in the IQ.1031             toRename-&gt;iewUnblock[tid] = false;1032    1033             ++iewIQFullEvents;1034             break;1035         }1036    1037         // Check LSQ if inst is LD/ST1038         if ((inst-&gt;isAtomic() &amp;&amp; ldstQueue.sqFull(tid)) ||1039             (inst-&gt;isLoad() &amp;&amp; ldstQueue.lqFull(tid)) ||1040             (inst-&gt;isStore() &amp;&amp; ldstQueue.sqFull(tid))) {1041             DPRINTF(IEW, \"[tid:%i] Issue: %s has become full.\\n\",tid,1042                     inst-&gt;isLoad() ? \"LQ\" : \"SQ\");1043    1044             // Call function to start blocking.1045             block(tid);1046    1047             // Set unblock to false. Special case where we are using1048             // skidbuffer (unblocking) instructions but then we still1049             // get full in the IQ.1050             toRename-&gt;iewUnblock[tid] = false;1051 1052             ++iewLSQFullEvents;1053             break;1054         }First, it checks whether the current instruction has been already squashed. If yes, then ignore the current instruction and jump to the next ones. If the instructions is not squashed, it checks the availability of resource required for issuing the instruction. Regardless of the instruction type, it requires one entry from the instruction queue.Also, if it is the memory related instruction, it require one entry from the load queue or store queue based on whether it is load or store instruction.Checking instruction type1056         // Otherwise issue the instruction just fine.1057         if (inst-&gt;isAtomic()) {1058             DPRINTF(IEW, \"[tid:%i] Issue: Memory instruction \"1059                     \"encountered, adding to LSQ.\\n\", tid);1060 1061             ldstQueue.insertStore(inst);1062 1063             ++iewDispStoreInsts;1064 1065             // AMOs need to be set as \"canCommit()\"1066             // so that commit can process them when they reach the1067             // head of commit.1068             inst-&gt;setCanCommit();1069             instQueue.insertNonSpec(inst);1070             add_to_iq = false;1071 1072             ++iewDispNonSpecInsts;1073 1074             toRename-&gt;iewInfo[tid].dispatchedToSQ++;1075         } else if (inst-&gt;isLoad()) {1076             DPRINTF(IEW, \"[tid:%i] Issue: Memory instruction \"1077                     \"encountered, adding to LSQ.\\n\", tid);1078 1079             // Reserve a spot in the load store queue for this1080             // memory access.1081             ldstQueue.insertLoad(inst);1082 1083             ++iewDispLoadInsts;1084 1085             add_to_iq = true;1086 1087             toRename-&gt;iewInfo[tid].dispatchedToLQ++;1088         } else if (inst-&gt;isStore()) {1089             DPRINTF(IEW, \"[tid:%i] Issue: Memory instruction \"1090                     \"encountered, adding to LSQ.\\n\", tid);1091 1092             ldstQueue.insertStore(inst);1093 1094             ++iewDispStoreInsts;1095 1096             if (inst-&gt;isStoreConditional()) {1097                 // Store conditionals need to be set as \"canCommit()\"1098                 // so that commit can process them when they reach the1099                 // head of commit.1100                 // @todo: This is somewhat specific to Alpha.1101                 inst-&gt;setCanCommit();1102                 instQueue.insertNonSpec(inst);1103                 add_to_iq = false;1104 1105                 ++iewDispNonSpecInsts;1106             } else {1107                 add_to_iq = true;1108             }1109 1110             toRename-&gt;iewInfo[tid].dispatchedToSQ++;1111         } else if (inst-&gt;isMemBarrier() || inst-&gt;isWriteBarrier()) {1112             // Same as non-speculative stores.1113             inst-&gt;setCanCommit();1114             instQueue.insertBarrier(inst);1115             add_to_iq = false;1116         } else if (inst-&gt;isNop()) {1117             DPRINTF(IEW, \"[tid:%i] Issue: Nop instruction encountered, \"1118                     \"skipping.\\n\", tid);1119 1120             inst-&gt;setIssued();1121             inst-&gt;setExecuted();1122             inst-&gt;setCanCommit();1123 1124             instQueue.recordProducer(inst);1125 1126             iewExecutedNop[tid]++;1127 1128             add_to_iq = false;1129         } else {1130             assert(!inst-&gt;isExecuted());1131             add_to_iq = true;1132         }Although it is not clear until we understand the internal of the instQueue and ldstQueue,but the above code pushes the instructions based on the instruction type.For example, for the load operation, it pushes the instructionto the ldstQueue with insertLoad function. For the write operation, it is inserted to the same queue through the insertStore function.For the normal instructions they will be just enqueued to the instQueue.Issuing instruction1134         if (add_to_iq &amp;&amp; inst-&gt;isNonSpeculative()) {1135             DPRINTF(IEW, \"[tid:%i] Issue: Nonspeculative instruction \"1136                     \"encountered, skipping.\\n\", tid);1137 1138             // Same as non-speculative stores.1139             inst-&gt;setCanCommit();1140 1141             // Specifically insert it as nonspeculative.1142             instQueue.insertNonSpec(inst);1143 1144             ++iewDispNonSpecInsts;1145 1146             add_to_iq = false;1147         }1148 1149         // If the instruction queue is not full, then add the1150         // instruction.1151         if (add_to_iq) {1152             instQueue.insert(inst);1153         }1154 1155         insts_to_dispatch.pop();1156 1157         toRename-&gt;iewInfo[tid].dispatched++;1158 1159         ++iewDispatchedInsts;1160 1161 #if TRACING_ON1162         inst-&gt;dispatchTick = curTick() - inst-&gt;fetchTick;1163 #endif1164         ppDispatch-&gt;notify(inst);1165     }After each instructions are handled by inserting them to the corresponding queues with the associated method provided by the queues, some of them should also be inserted to the instruction queue. Note that add_to_iq flag is setbased on the instruction type, When this flag is set, the instruction should be added to the instQueue (line 1151-1153).End of the dispatching1167     if (!insts_to_dispatch.empty()) {1168         DPRINTF(IEW,\"[tid:%i] Issue: Bandwidth Full. Blocking.\\n\", tid);1169         block(tid);1170         toRename-&gt;iewUnblock[tid] = false;1171     }1172 1173     if (dispatchStatus[tid] == Idle &amp;&amp; dis_num_inst) {1174         dispatchStatus[tid] = Running;1175 1176         updatedQueues = true;1177     }1178 1179     dis_num_inst = 0;1180 }After dispatching all renamed instructions, it should check whether it still has some instructions in the queue. When the instruction cannot be processed further because of throttling,it should block and handle rest of the instructions at the next cycle.Instruction Queue and Load/Store queueBefore moving on to the next stage, I’d like to cover some part of the IQ and LSQ.Instruction queue has several lists to keep issued instructionsMainly the job of the queue is managing instructions and providing some interfaces to process the enqueued instructions.gem5/src/cpu/o3/inst_queue.hh311     //////////////////////////////////////312     // Instruction lists, ready queues, and ordering313     //////////////////////////////////////314 315     /** List of all the instructions in the IQ (some of which may be issued). */316     std::list&lt;DynInstPtr&gt; instList[Impl::MaxThreads];317 318     /** List of instructions that are ready to be executed. */319     std::list&lt;DynInstPtr&gt; instsToExecute;320 321     /** List of instructions waiting for their DTB translation to322      *  complete (hw page table walk in progress).323      */324     std::list&lt;DynInstPtr&gt; deferredMemInsts;325 326     /** List of instructions that have been cache blocked. */327     std::list&lt;DynInstPtr&gt; blockedMemInsts;328 329     /** List of instructions that were cache blocked, but a retry has been seen330      * since, so they can now be retried. May fail again go on the blocked list.331      */332     std::list&lt;DynInstPtr&gt; retryMemInsts;Insert new entries to the instruction queueThe insert function is the essential example of the interface.It inserts new entries to the instruction list managed by the instruction queue. 578 template &lt;class Impl&gt; 579 void 580 InstructionQueue&lt;Impl&gt;::insert(const DynInstPtr &amp;new_inst) 581 { 582     if (new_inst-&gt;isFloating()) { 583         fpInstQueueWrites++; 584     } else if (new_inst-&gt;isVector()) { 585         vecInstQueueWrites++; 586     } else { 587         intInstQueueWrites++; 588     } 589     // Make sure the instruction is valid 590     assert(new_inst); 591  592     DPRINTF(IQ, \"Adding instruction [sn:%llu] PC %s to the IQ.\\n\", 593             new_inst-&gt;seqNum, new_inst-&gt;pcState()); 594  595     assert(freeEntries != 0); 596  597     instList[new_inst-&gt;threadNumber].push_back(new_inst); 598  599     --freeEntries; 600  601     new_inst-&gt;setInIQ(); 602  603     // Look through its source registers (physical regs), and mark any 604     // dependencies. 605     addToDependents(new_inst); 606  607     // Have this instruction set itself as the producer of its destination 608     // register(s). 609     addToProducers(new_inst); 610  611     if (new_inst-&gt;isMemRef()) { 612         memDepUnit[new_inst-&gt;threadNumber].insert(new_inst); 613     } else { 614         addIfReady(new_inst); 615     } 616  617     ++iqInstsAdded; 618  619     count[new_inst-&gt;threadNumber]++; 620  621     assert(freeEntries == (numEntries - countInsts())); 622 }Inserting the instruction to the list is done by simple push_back operation of the list.However, it invokes two important functions: addToProducers and addToDependents.These two functions generates producer and consumer dependency among instructions’s operands, registers. When one instruction waits until the specific register’s value become ready (consumer),it should be tracked by some hardware component. Also, when the data becomes ready as a result of execution of one instruction (producer), it should be forwarded to the consumers waiting for the value. For that purpose, GEM5 utilize the DependencyGraph. After producing dependency for the unavailable registers, if the instruction references memory while its execution,it should be specially handled by the memory dependency unit. The details will be explained together with the DependencyGraph later.1450 template &lt;class Impl&gt;1451 void1452 InstructionQueue&lt;Impl&gt;::addIfReady(const DynInstPtr &amp;inst)1453 {1454     // If the instruction now has all of its source registers1455     // available, then add it to the list of ready instructions.1456     if (inst-&gt;readyToIssue()) {1457 1458         //Add the instruction to the proper ready list.1459         if (inst-&gt;isMemRef()) {1460 1461             DPRINTF(IQ, \"Checking if memory instruction can issue.\\n\");1462 1463             // Message to the mem dependence unit that this instruction has1464             // its registers ready.1465             memDepUnit[inst-&gt;threadNumber].regsReady(inst);1466 1467             return;1468         }1469 1470         OpClass op_class = inst-&gt;opClass();1471 1472         DPRINTF(IQ, \"Instruction is ready to issue, putting it onto \"1473                 \"the ready list, PC %s opclass:%i [sn:%llu].\\n\",1474                 inst-&gt;pcState(), op_class, inst-&gt;seqNum);1475 1476         readyInsts[op_class].push(inst);1477 1478         // Will need to reorder the list if either a queue is not on the list,1479         // or it has an older instruction than last time.1480         if (!queueOnList[op_class]) {1481             addToOrderList(op_class);1482         } else if (readyInsts[op_class].top()-&gt;seqNum  &lt;1483                    (*readyIt[op_class]).oldestInst) {1484             listOrder.erase(readyIt[op_class]);1485             addToOrderList(op_class);1486         }1487     }1488 }At the end of the insert function, it adds instruction to the readyInsts buffer if all the registers are available (line 1476).If the instruction is not ready, which means the source registers are not available, the instruction should not be inqueued to the readyInsts buffer.The instructions waiting for the source register to become available will be added to the readyInsts buffer when other dependent instructions complete.ExecuteTo understand what should be done after dispatching the instructions,let’s go back to the tick function of the iew stage.1532     if (exeStatus != Squashing) {1533         executeInsts();1534 1535         writebackInsts();1536 1537         // Have the instruction queue try to schedule any ready instructions.1538         // (In actuality, this scheduling is for instructions that will1539         // be executed next cycle.)1540         instQueue.scheduleReadyInsts();1541 1542         // Also should advance its own time buffers if the stage ran.1543         // Not the best place for it, but this works (hopefully).1544         issueToExecQueue.advance();1545     }If the execution stage is not in the squashing state, it will execute instructions stored in the instQueue, particularly readyInsts queue. Here execute() function of the compute instruction is invoked and sent to commit. Please note execute() will write results to the destination registers.Therefore, after executeInsts is invoked, writebackInsts is called to write the result to destination registers.Furthermore, when there are dependent instructions to the currently executed one,those instructions will be added to the ready list for scheduling.executeInsts1205 template &lt;class Impl&gt;1206 void1207 DefaultIEW&lt;Impl&gt;::executeInsts()1208 {1209     wbNumInst = 0;1210     wbCycle = 0;1211 1212     list&lt;ThreadID&gt;::iterator threads = activeThreads-&gt;begin();1213     list&lt;ThreadID&gt;::iterator end = activeThreads-&gt;end();1214 1215     while (threads != end) {1216         ThreadID tid = *threads++;1217         fetchRedirect[tid] = false;1218     }1219 1220     // Uncomment this if you want to see all available instructions.1221     // @todo This doesn't actually work anymore, we should fix it.1222     // printAvailableInsts();1223 1224     // Execute/writeback any instructions that are available.1225     int insts_to_execute = fromIssue-&gt;size;1226     int inst_num = 0;1227     for (; inst_num &lt; insts_to_execute;1228           ++inst_num) {1229 1230         DPRINTF(IEW, \"Execute: Executing instructions from IQ.\\n\");1231 1232         DynInstPtr inst = instQueue.getInstToExecute();1233 1234         DPRINTF(IEW, \"Execute: Processing PC %s, [tid:%i] [sn:%llu].\\n\",1235                 inst-&gt;pcState(), inst-&gt;threadNumber,inst-&gt;seqNum);1236 1237         // Notify potential listeners that this instruction has started1238         // executing1239         ppExecute-&gt;notify(inst);1240 1241         // Check if the instruction is squashed; if so then skip it1242         if (inst-&gt;isSquashed()) {1243             DPRINTF(IEW, \"Execute: Instruction was squashed. PC: %s, [tid:%i]\"1244                          \" [sn:%llu]\\n\", inst-&gt;pcState(), inst-&gt;threadNumber,1245                          inst-&gt;seqNum);1246 1247             // Consider this instruction executed so that commit can go1248             // ahead and retire the instruction.1249             inst-&gt;setExecuted();1250 1251             // Not sure if I should set this here or just let commit try to1252             // commit any squashed instructions.  I like the latter a bit more.1253             inst-&gt;setCanCommit();1254 1255             ++iewExecSquashedInsts;1256 1257             continue;1258         }The executeInsts function execute an many instruction as it can afford,which is implemented as the loop in the line 1227 and after. First it retrieves instruction that can be executed by invokinggetInstToExecute function of the instQueue. After one instruction is retrieved, it checks if the instructionshould be squashed. Although the squashed instructions are not really executed,but it should be treated as executed because it should be committed. After this condition is checked, depending on the type of the instruction,it will process the instruction separately.execute memory instruction1259 1260         Fault fault = NoFault;1261 1262         // Execute instruction.1263         // Note that if the instruction faults, it will be handled1264         // at the commit stage.1265         if (inst-&gt;isMemRef()) {1266             DPRINTF(IEW, “Execute: Calculating address for memory “1267                     “reference.\\n”);1268 1269             // Tell the LDSTQ to execute this instruction (if it is a load).1270             if (inst-&gt;isAtomic()) {1271                 // AMOs are treated like store requests1272                 fault = ldstQueue.executeStore(inst);1273 1274                 if (inst-&gt;isTranslationDelayed() &amp;&amp;1275                     fault == NoFault) {1276                     // A hw page table walk is currently going on; the1277                     // instruction must be deferred.1278                     DPRINTF(IEW, “Execute: Delayed translation, deferring “1279                             “store.\\n”);1280                     instQueue.deferMemInst(inst);1281                     continue;1282                 }1283             } else if (inst-&gt;isLoad()) {1284                 // Loads will mark themselves as executed, and their writeback1285                 // event adds the instruction to the queue to commit1286                 fault = ldstQueue.executeLoad(inst);1287 1288                 if (inst-&gt;isTranslationDelayed() &amp;&amp;1289                     fault == NoFault) {1290                     // A hw page table walk is currently going on; the1291                     // instruction must be deferred.1292                     DPRINTF(IEW, “Execute: Delayed translation, deferring “1293                             “load.\\n”);1294                     instQueue.deferMemInst(inst);1295                     continue;1296                 }1297 1298                 if (inst-&gt;isDataPrefetch() || inst-&gt;isInstPrefetch()) {1299                     inst-&gt;fault = NoFault;1300                 }1301             } else if (inst-&gt;isStore()) {1302                 fault = ldstQueue.executeStore(inst);1303 1304                 if (inst-&gt;isTranslationDelayed() &amp;&amp;1305                     fault == NoFault) {1306                     // A hw page table walk is currently going on; the1307                     // instruction must be deferred.1308                     DPRINTF(IEW, “Execute: Delayed translation, deferring “1309                             “store.\\n”);1310                     instQueue.deferMemInst(inst);1311                     continue;1312                 }1313 1314                 // If the store had a fault then it may not have a mem req1315                 if (fault != NoFault || !inst-&gt;readPredicate() ||1316                         !inst-&gt;isStoreConditional()) {1317                     // If the instruction faulted, then we need to send it along1318                     // to commit without the instruction completing.1319                     // Send this instruction to commit, also make sure iew stage1320                     // realizes there is activity.1321                     inst-&gt;setExecuted();1322                     instToCommit(inst);1323                     activityThisCycle();1324                 }1325 1326                 // Store conditionals will mark themselves as1327                 // executed, and their writeback event will add the1328                 // instruction to the queue to commit.1329             } else {1330                 panic(“Unexpected memory type!\\n”);1331             }1332 1333         } else {For the memory operation, it can be one of three instruction type:atomic, load, store. Basically, the loadstore queue in charge of executing memory instructions,but based on the type of memory operation, it needs to handle instruction differently. Let's take a look at how the load and store instruction will be processed.### Execute load instruction```cpp1283             } else if (inst-&gt;isLoad()) {1284                 // Loads will mark themselves as executed, and their writeback1285                 // event adds the instruction to the queue to commit1286                 fault = ldstQueue.executeLoad(inst);12871288                 if (inst-&gt;isTranslationDelayed() &amp;&amp;1289                     fault == NoFault) {1290                     // A hw page table walk is currently going on; the1291                     // instruction must be deferred.1292                     DPRINTF(IEW, \"Execute: Delayed translation, deferring \"1293                             \"load.\\n\");1294                     instQueue.deferMemInst(inst);1295                     continue;1296                 }12971298                 if (inst-&gt;isDataPrefetch() || inst-&gt;isInstPrefetch()) {1299                     inst-&gt;fault = NoFault;1300                 }The main execution of the load instruction is done by the executeLoad function of the ldstQueue. After the execution, it needs to check whether the translation is the bottleneck of making progress on the load operation.Note that when the virtual to physical address resolution is delayed because of long TLB latency, it should be executed at the next or later clock cycle when the TLB is ready. Therefore, when the instruction cannot be executed at this moment,it should set the current load instruction is deferred (deferMemInst).Also, when the load operation was just prefetch,then any fault generated by this operation should be ignored (line 1298-1299). Let’s take our important function executeLoad in detail!gem5/src/o3/cpu/lsq_impl.hh 251 template&lt;class Impl&gt; 252 Fault 253 LSQ&lt;Impl&gt;::executeLoad(const DynInstPtr &amp;inst) 254 { 255     ThreadID tid = inst-&gt;threadNumber; 256 257     return thread[tid].executeLoad(inst); 258 }gem5/src/o3/cpu/lsq.hh  63 template &lt;class Impl&gt;  64 class LSQ  65   66 {......1104     /** Total Size of LQ Entries. */1105     unsigned LQEntries;1106     /** Total Size of SQ Entries. */1107     unsigned SQEntries;1108 1109     /** Max LQ Size - Used to Enforce Sharing Policies. */1110     unsigned maxLQEntries;1111 1112     /** Max SQ Size - Used to Enforce Sharing Policies. */1113     unsigned maxSQEntries;1114 1115     /** Data port. */1116     DcachePort dcachePort;1117 1118     /** The LSQ units for individual threads. */1119     std::vector&lt;LSQUnit&gt; thread;1120 1121     /** Number of Threads. */1122     ThreadID numThreads;1123 };gem5/src/o3/cpu/lsq_unit_impl.hh 558 template &lt;class Impl&gt; 559 Fault 560 LSQUnit&lt;Impl&gt;::executeLoad(const DynInstPtr &amp;inst) 561 {   562     using namespace TheISA; 563     // Execute a specific load. 564     Fault load_fault = NoFault; 565     566     DPRINTF(LSQUnit, \"Executing load PC %s, [sn:%lli]\\n\", 567             inst-&gt;pcState(), inst-&gt;seqNum); 568     569     assert(!inst-&gt;isSquashed()); 570     571     load_fault = inst-&gt;initiateAcc(); 572  573     if (load_fault == NoFault &amp;&amp; !inst-&gt;readMemAccPredicate()) { 574         assert(inst-&gt;readPredicate()); 575         inst-&gt;setExecuted(); 576         inst-&gt;completeAcc(nullptr); 577         iewStage-&gt;instToCommit(inst); 578         iewStage-&gt;activityThisCycle(); 579         return NoFault; 580     } 581         582     if (inst-&gt;isTranslationDelayed() &amp;&amp; load_fault == NoFault) 583         return load_fault; 584             585     if (load_fault != NoFault &amp;&amp; inst-&gt;translationCompleted() &amp;&amp; 586         inst-&gt;savedReq-&gt;isPartialFault() &amp;&amp; !inst-&gt;savedReq-&gt;isComplete()) { 587         assert(inst-&gt;savedReq-&gt;isSplit()); 588         // If we have a partial fault where the mem access is not complete yet 589         // then the cache must have been blocked. This load will be re-executed 590         // when the cache gets unblocked. We will handle the fault when the 591         // mem access is complete. 592         return NoFault; 593     }   594         595     // If the instruction faulted or predicated false, then we need to send it 596     // along to commit without the instruction completing. 597     if (load_fault != NoFault || !inst-&gt;readPredicate()) { 598         // Send this instruction to commit, also make sure iew stage 599         // realizes there is activity.  Mark it as executed unless it 600         // is a strictly ordered load that needs to hit the head of 601         // commit. 602         if (!inst-&gt;readPredicate()) 603             inst-&gt;forwardOldRegs(); 604         DPRINTF(LSQUnit, \"Load [sn:%lli] not executed from %s\\n\", 605                 inst-&gt;seqNum, 606                 (load_fault != NoFault ? \"fault\" : \"predication\")); 607         if (!(inst-&gt;hasRequest() &amp;&amp; inst-&gt;strictlyOrdered()) || 608             inst-&gt;isAtCommit()) { 609             inst-&gt;setExecuted(); 610         } 611         iewStage-&gt;instToCommit(inst); 612         iewStage-&gt;activityThisCycle(); 613     } else { 614         if (inst-&gt;effAddrValid()) { 615             auto it = inst-&gt;lqIt; 616             ++it; 617  618             if (checkLoads) 619                 return checkViolations(it, inst); 620         } 621     } 622  623     return load_fault; 624 }initiateAcc: handling TLB requestI already covered InitiateAcc of the memory instructions before. However, compared to simple processors, the O3 cpu have different way to process the initateAcc.147 template &lt;class Impl&gt;148 Fault149 BaseO3DynInst&lt;Impl&gt;::initiateAcc()150 {    151     // @todo: Pretty convoluted way to avoid squashing from happening152     // when using the TC during an instruction's execution153     // (specifically for instructions that have side-effects that use154     // the TC).  Fix this.155     bool no_squash_from_TC = this-&gt;thread-&gt;noSquashFromTC;156     this-&gt;thread-&gt;noSquashFromTC = true;157 158     this-&gt;fault = this-&gt;staticInst-&gt;initiateAcc(this, this-&gt;traceData);159 160     this-&gt;thread-&gt;noSquashFromTC = no_squash_from_TC;161 162     return this-&gt;fault;163 }    Because the staticInst stored in the dynamic instruction is the class object of a specific microoperation, it will invokes the initiateAcc function of that micro-load/store operation. For the memory read operation case, it invokes initiateMemRead function of architecture side.This will end up invoking initiateMemRead function of the CPU side. 42 namespace X86ISA 43 { 44  45 /// Initiate a read from memory in timing mode. 46 static Fault 47 initiateMemRead(ExecContext *xc, Trace::InstRecord *traceData, Addr addr, 48                 unsigned dataSize, Request::Flags flags) 49 { 50     return xc-&gt;initiateMemRead(addr, dataSize, flags); 51 } 962 template&lt;class Impl&gt; 963 Fault 964 BaseDynInst&lt;Impl&gt;::initiateMemRead(Addr addr, unsigned size, 965                                    Request::Flags flags, 966                                    const std::vector&lt;bool&gt;&amp; byte_enable) 967 { 968     assert(byte_enable.empty() || byte_enable.size() == size); 969     return cpu-&gt;pushRequest( 970             dynamic_cast&lt;typename DynInstPtr::PtrType&gt;(this), 971             /* ld */ true, nullptr, size, addr, flags, nullptr, nullptr, 972             byte_enable); 973 }Because the instruction of the O3 CPU is instance of BaseO3DynInstinheriting the BaseDynInst, when the instruction implementation invokes initateMemRead (invoked through the InitateAcc implementation of the instruction),it invokes the corresponding method implemented in the BaseDynInst class.pushRequest713     /** CPU pushRequest function, forwards request to LSQ. */714     Fault pushRequest(const DynInstPtr&amp; inst, bool isLoad, uint8_t *data,715                       unsigned int size, Addr addr, Request::Flags flags,716                       uint64_t *res, AtomicOpFunctorPtr amo_op = nullptr,717                       const std::vector&lt;bool&gt;&amp; byte_enable =718                           std::vector&lt;bool&gt;())719 720     {721         return iew.ldstQueue.pushRequest(inst, isLoad, data, size, addr,722                 flags, res, std::move(amo_op), byte_enable);723     }Instead of directly handling the load operation, initiateMemRead pushes the request to the load queue through the pushRequest function. This design seems to be odd because the initateAcc function has been invoked by the lsq at the first place, and the instruction forward the request to the loadstore queue once again. It might have been just implemented as simple function that handles the request directly without going throughmultiple different units. Anyway, initiateMemRead invokes the pushRequest of the CPU sideand it will end up invoking pushRequest of the LSQ. 693 template&lt;class Impl&gt; 694 Fault 695 LSQ&lt;Impl&gt;::pushRequest(const DynInstPtr&amp; inst, bool isLoad, uint8_t *data, 696                        unsigned int size, Addr addr, Request::Flags flags, 697                        uint64_t *res, AtomicOpFunctorPtr amo_op, 698                        const std::vector&lt;bool&gt;&amp; byte_enable) 699 { 700     // This comming request can be either load, store or atomic. 701     // Atomic request has a corresponding pointer to its atomic memory 702     // operation 703     bool isAtomic M5_VAR_USED = !isLoad &amp;&amp; amo_op; 704  705     ThreadID tid = cpu-&gt;contextToThread(inst-&gt;contextId()); 706     auto cacheLineSize = cpu-&gt;cacheLineSize(); 707     bool needs_burst = transferNeedsBurst(addr, size, cacheLineSize); 708     LSQRequest* req = nullptr; 709  710     // Atomic requests that access data across cache line boundary are 711     // currently not allowed since the cache does not guarantee corresponding 712     // atomic memory operations to be executed atomically across a cache line. 713     // For ISAs such as x86 that supports cross-cache-line atomic instructions, 714     // the cache needs to be modified to perform atomic update to both cache 715     // lines. For now, such cross-line update is not supported. 716     assert(!isAtomic || (isAtomic &amp;&amp; !needs_burst)); 717  718     if (inst-&gt;translationStarted()) { 719         req = inst-&gt;savedReq; 720         assert(req); 721     } else { 722         if (needs_burst) { 723             req = new SplitDataRequest(&amp;thread[tid], inst, isLoad, addr, 724                     size, flags, data, res); 725         } else { 726             req = new SingleDataRequest(&amp;thread[tid], inst, isLoad, addr, 727                     size, flags, data, res, std::move(amo_op)); 728         } 729         assert(req); 730         if (!byte_enable.empty()) { 731             req-&gt;_byteEnable = byte_enable; 732         } 733         inst-&gt;setRequest(); 734         req-&gt;taskId(cpu-&gt;taskId()); 735  736         // There might be fault from a previous execution attempt if this is 737         // a strictly ordered load 738         inst-&gt;getFault() = NoFault; 739  740         req-&gt;initiateTranslation(); 741     } 742  743     /* This is the place were instructions get the effAddr. */ 744     if (req-&gt;isTranslationComplete()) { 745         if (req-&gt;isMemAccessRequired()) { 746             inst-&gt;effAddr = req-&gt;getVaddr(); 747             inst-&gt;effSize = size; 748             inst-&gt;effAddrValid(true); 749  750             if (cpu-&gt;checker) { 751                 inst-&gt;reqToVerify = std::make_shared&lt;Request&gt;(*req-&gt;request()); 752             } 753             Fault fault; 754             if (isLoad) 755                 fault = cpu-&gt;read(req, inst-&gt;lqIdx); 756             else 757                 fault = cpu-&gt;write(req, data, inst-&gt;sqIdx); 758             // inst-&gt;getFault() may have the first-fault of a 759             // multi-access split request at this point. 760             // Overwrite that only if we got another type of fault 761             // (e.g. re-exec). 762             if (fault != NoFault) 763                 inst-&gt;getFault() = fault; 764         } else if (isLoad) { 765             inst-&gt;setMemAccPredicate(false); 766             // Commit will have to clean up whatever happened.  Set this 767             // instruction as executed. 768             inst-&gt;setExecuted(); 769         } 770     } 771  772     if (inst-&gt;traceData) 773         inst-&gt;traceData-&gt;setMem(addr, size, flags); 774  775     return inst-&gt;getFault(); 776 }The dynamic instruction can track whether the current instruction has started TLB translation by checking the flag stored in the instruction.It provide the interface to access that information, called translationStartedWhen the instruction set that flag, it means that the instruction already started the TLB access but waiting response.In the delayed TLB response case, the instruction stores the request information in its instruction object. Therefore, it can retrieve the request that has sent to TLB before. However, if it is the first time of execution, then it should generate new request.As shown in line 722-728, if the request should access two separate cache blocks,it generates SplitDataRequest request object. However, if it only access one block, then SingleDataRequest request object is generated instead. After the request has been produced, it should set proper flags of the instruction object to indicate the instruction initiated the TLB access (line 733). After that, the initiateTranslation function provided by the request object is invoked to actually generate accesses to the TLBs. 860 template&lt;class Impl&gt; 861 void 862 LSQ&lt;Impl&gt;::SingleDataRequest::initiateTranslation() 863 { 864     assert(_requests.size() == 0); 865  866     this-&gt;addRequest(_addr, _size, _byteEnable); 867  868     if (_requests.size() &gt; 0) { 869         _requests.back()-&gt;setReqInstSeqNum(_inst-&gt;seqNum); 870         _requests.back()-&gt;taskId(_taskId); 871         _inst-&gt;translationStarted(true); 872         setState(State::Translation); 873         flags.set(Flag::TranslationStarted); 874  875         _inst-&gt;savedReq = this; 876         sendFragmentToTranslation(0); 877     } else { 878         _inst-&gt;setMemAccPredicate(false); 879     } 880 }The addRequest just generates packet need to be sent to the TLB unit.Although the current object can be interpreted as just an request itself that can be directly sent to the TLB unit, but it is a wrapper for all the required interface and data structures to resolve TLB access.For example, it includes the ports connected with the TLB unit so that the generated request and its response can be communicated through that port.Anyway, the addRequest function just generates the real packet understandable by the TLB unit. 407         void 408         addRequest(Addr addr, unsigned size, 409                    const std::vector&lt;bool&gt;&amp; byte_enable) 410         { 411             if (byte_enable.empty() || 412                 isAnyActiveElement(byte_enable.begin(), byte_enable.end())) { 413                 auto request = std::make_shared&lt;Request&gt;(_inst-&gt;getASID(), 414                         addr, size, _flags, _inst-&gt;masterId(), 415                         _inst-&gt;instAddr(), _inst-&gt;contextId(), 416                         std::move(_amo_op)); 417                 if (!byte_enable.empty()) { 418                     request-&gt;setByteEnable(byte_enable); 419                 } 420                 _requests.push_back(request);                                                                                                                                                                   421             } 422         }The addRequest function of the LSQRquest classjust generates the request and save it to the _requests vector to send them later. After the request packets are generated, initiateTranslation invokes sendFragmentToTranslation to send the generated packet(s) to the TLB. 980 template&lt;class Impl&gt; 981 void 982 LSQ&lt;Impl&gt;::LSQRequest::sendFragmentToTranslation(int i) 983 { 984     numInTranslationFragments++; 985     _port.dTLB()-&gt;translateTiming( 986             this-&gt;request(i), 987             this-&gt;_inst-&gt;thread-&gt;getTC(), this, 988             this-&gt;isLoad() ? BaseTLB::Read : BaseTLB::Write); 989 }Remember that the SingleDataRequest has only one request packet.Therefore, it has only one entry in the _requests vector. This function sends the request stored in the _requests vector to the TLB.Note that the argument is used to index the entry stored in the _requests vector. You can see that it invokes the translateTiming of the dTLB connected to the LSQ.The details of the translateTiming function of the TLB is explained in the previous posting.Also, note that it passes the this as the translation object parameter. Because the translation object is used to invoke the finish function when the TLB access is resolved.Response of LSQ for the TLB resolution 778 template&lt;class Impl&gt; 779 void 780 LSQ&lt;Impl&gt;::SingleDataRequest::finish(const Fault &amp;fault, const RequestPtr &amp;req, 781         ThreadContext* tc, BaseTLB::Mode mode) 782 { 783     _fault.push_back(fault); 784     numInTranslationFragments = 0; 785     numTranslatedFragments = 1; 786     /* If the instruction has been squahsed, let the request know 787      * as it may have to self-destruct. */ 788     if (_inst-&gt;isSquashed()) { 789         this-&gt;squashTranslation(); 790     } else { 791         _inst-&gt;strictlyOrdered(req-&gt;isStrictlyOrdered()); 792  793         flags.set(Flag::TranslationFinished); 794         if (fault == NoFault) { 795             _inst-&gt;physEffAddr = req-&gt;getPaddr(); 796             _inst-&gt;memReqFlags = req-&gt;getFlags(); 797             if (req-&gt;isCondSwap()) { 798                 assert(_res); 799                 req-&gt;setExtraData(*_res); 800             } 801             setState(State::Request); 802         } else { 803             setState(State::Fault); 804         } 805  806         LSQRequest::_inst-&gt;fault = fault; 807         LSQRequest::_inst-&gt;translationCompleted(true); 808     } 809 }When the translation is completed, the finish function provided by the Request generated by the LSQ will be invoked at the end of the translation. As shwon in the above code, it first checks whether the instruction has been squashed while the TLB process the request. If it has not been squashed, it will set required flags indicating the translation is completed for specific instruction.Note that it sets various fields of the instruction that has initiated the TLB request (_inst in the line 790-808). One of the most important field changed by the finish function is _state field of the request.This field indicates current status of the TLB request and can be set to other state by using the setState function. Remind that how the simpleCPU starts memory access after the TLB is resolved. It initiates memory operation at the end of the finish function.However, O3 cpu does not invoke any related functions to generate actual memory request when the TLB is resolved. Then when and where the O3,especially the LSQ initiates the memory operation?The answer is in the pushRequest! 693 template&lt;class Impl&gt; 694 Fault 695 LSQ&lt;Impl&gt;::pushRequest(const DynInstPtr&amp; inst, bool isLoad, uint8_t *data, 696                        unsigned int size, Addr addr, Request::Flags flags, 697                        uint64_t *res, AtomicOpFunctorPtr amo_op, 698                        const std::vector&lt;bool&gt;&amp; byte_enable) 699 { ...... 743     /* This is the place were instructions get the effAddr. */ 744     if (req-&gt;isTranslationComplete()) { 745         if (req-&gt;isMemAccessRequired()) { 746             inst-&gt;effAddr = req-&gt;getVaddr(); 747             inst-&gt;effSize = size; 748             inst-&gt;effAddrValid(true); 749 750             if (cpu-&gt;checker) { 751                 inst-&gt;reqToVerify = std::make_shared&lt;Request&gt;(*req-&gt;request()); 752             } 753             Fault fault; 754             if (isLoad) 755                 fault = cpu-&gt;read(req, inst-&gt;lqIdx); 756             else 757                 fault = cpu-&gt;write(req, data, inst-&gt;sqIdx); 758             // inst-&gt;getFault() may have the first-fault of a 759             // multi-access split request at this point. 760             // Overwrite that only if we got another type of fault 761             // (e.g. re-exec). 762             if (fault != NoFault) 763                 inst-&gt;getFault() = fault; 764         } else if (isLoad) { 765             inst-&gt;setMemAccPredicate(false); 766             // Commit will have to clean up whatever happened.  Set this 767             // instruction as executed. 768             inst-&gt;setExecuted(); 769         } 770     } 771 772     if (inst-&gt;traceData) 773         inst-&gt;traceData-&gt;setMem(addr, size, flags); 774 775     return inst-&gt;getFault(); 776 }It first checks the TLB translation is finished by invoking isTranslationComplete. 586         bool 587         isInTranslation() 588         { 589             return _state == State::Translation; 590         } 591  592         bool 593         isTranslationComplete() 594         { 595             return flags.isSet(Flag::TranslationStarted) &amp;&amp; 596                    !isInTranslation(); 597         }You might remember that the _state field was changed when the finish function of the TLB request packet is invoked. Therefore, if the TLB request is already resolved, the isTranslationComplete function will return true.And then the actual memory read or write operation is made based on the instruction type. Because the translation packet req has translated physical address from the virtual address, it should also be passed to the operation because memory operation should target the physical address not the virtual address.Because we care currently dealing with the read operation, let’s take a look at how the O3 access the real memory.CPU-&gt;read-&gt;LSQ::read-&gt;LSQUnit::read725     /** CPU read function, forwards read to LSQ. */726     Fault read(LSQRequest* req, int load_idx)727     {728         return this-&gt;iew.ldstQueue.read(req, load_idx);729     }1125 template &lt;class Impl&gt;1126 Fault1127 LSQ&lt;Impl&gt;::read(LSQRequest* req, int load_idx)1128 {1129     ThreadID tid = cpu-&gt;contextToThread(req-&gt;request()-&gt;contextId());1130 1131     return thread.at(tid).read(req, load_idx);1132 }The processor load function handles four different memory load operations:LLSC (locked load/store), MappedIPR (memory mapped register), store forwarding, and just memory load operation. I will cover the plain memory load operation that will try to access the datafrom the cache and memory. The store forwarding case will be handled in the other posting.621 LSQUnit&lt;Impl&gt;::read(LSQRequest *req, int load_idx)622 {623     LQEntry&amp; load_req = loadQueue[load_idx];624     const DynInstPtr&amp; load_inst = load_req.instruction();625 626     load_req.setRequest(req);627     assert(load_inst);628 629     assert(!load_inst-&gt;isExecuted());630 631     // Make sure this isn't a strictly ordered load632     // A bit of a hackish way to get strictly ordered accesses to work633     // only if they're at the head of the LSQ and are ready to commit634     // (at the head of the ROB too).635 636     if (req-&gt;mainRequest()-&gt;isStrictlyOrdered() &amp;&amp;637         (load_idx != loadQueue.head() || !load_inst-&gt;isAtCommit())) {638         // Tell IQ/mem dep unit that this instruction will need to be639         // rescheduled eventually640         iewStage-&gt;rescheduleMemInst(load_inst);641         load_inst-&gt;clearIssued();642         load_inst-&gt;effAddrValid(false);643         ++lsqRescheduledLoads;644         DPRINTF(LSQUnit, \"Strictly ordered load [sn:%lli] PC %s\\n\",645                 load_inst-&gt;seqNum, load_inst-&gt;pcState());646 647         // Must delete request now that it wasn't handed off to648         // memory.  This is quite ugly.  @todo: Figure out the proper649         // place to really handle request deletes.650         load_req.setRequest(nullptr);651         req-&gt;discard();652         return std::make_shared&lt;GenericISA::M5PanicFault&gt;(653             \"Strictly ordered load [sn:%llx] PC %s\\n\",654             load_inst-&gt;seqNum, load_inst-&gt;pcState());655     }656 657     DPRINTF(LSQUnit, \"Read called, load idx: %i, store idx: %i, \"658             \"storeHead: %i addr: %#x%s\\n\",659             load_idx - 1, load_inst-&gt;sqIt._idx, storeQueue.head() - 1,660             req-&gt;mainRequest()-&gt;getPaddr(), req-&gt;isSplit() ? \" split\" : \"\");661 662     if (req-&gt;mainRequest()-&gt;isLLSC()) {663         // Disable recording the result temporarily.  Writing to misc664         // regs normally updates the result, but this is not the665         // desired behavior when handling store conditionals.666         load_inst-&gt;recordResult(false);667         TheISA::handleLockedRead(load_inst.get(), req-&gt;mainRequest());668         load_inst-&gt;recordResult(true);669     }670 671     if (req-&gt;mainRequest()-&gt;isMmappedIpr()) {672         assert(!load_inst-&gt;memData);673         load_inst-&gt;memData = new uint8_t[MaxDataBytes];674 675         ThreadContext *thread = cpu-&gt;tcBase(lsqID);676         PacketPtr main_pkt = new Packet(req-&gt;mainRequest(), MemCmd::ReadReq);677 678         main_pkt-&gt;dataStatic(load_inst-&gt;memData);679 680         Cycles delay = req-&gt;handleIprRead(thread, main_pkt);681 682         WritebackEvent *wb = new WritebackEvent(load_inst, main_pkt, this);683         cpu-&gt;schedule(wb, cpu-&gt;clockEdge(delay));684         return NoFault;685     }686 687     // Check the SQ for any previous stores that might lead to forwarding......840     // If there's no forwarding case, then go access memory841     DPRINTF(LSQUnit, \"Doing memory access for inst [sn:%lli] PC %s\\n\",842             load_inst-&gt;seqNum, load_inst-&gt;pcState());843 844     // Allocate memory if this is the first time a load is issued.845     if (!load_inst-&gt;memData) {846         load_inst-&gt;memData = new uint8_t[req-&gt;mainRequest()-&gt;getSize()];847     }848 849     // For now, load throughput is constrained by the number of850     // load FUs only, and loads do not consume a cache port (only851     // stores do).852     // @todo We should account for cache port contention853     // and arbitrate between loads and stores.854 855     // if we the cache is not blocked, do cache access856     if (req-&gt;senderState() == nullptr) {857         LQSenderState *state = new LQSenderState(858                 loadQueue.getIterator(load_idx));859         state-&gt;isLoad = true;860         state-&gt;inst = load_inst;861         state-&gt;isSplit = req-&gt;isSplit();862         req-&gt;senderState(state);863     }864     req-&gt;buildPackets();865     req-&gt;sendPacketToCache();866     if (!req-&gt;isSent())867         iewStage-&gt;blockMemInst(load_inst);868 869     return NoFault;870 }Execute store instructionExecute non-memory instruction1333         } else {1334             // If the instruction has already faulted, then skip executing it.1335             // Such case can happen when it faulted during ITLB translation.1336             // If we execute the instruction (even if it's a nop) the fault1337             // will be replaced and we will lose it.1338             if (inst-&gt;getFault() == NoFault) {1339                 inst-&gt;execute();1340                 if (!inst-&gt;readPredicate())1341                     inst-&gt;forwardOldRegs();1342             }1343 1344             inst-&gt;setExecuted();1345 1346             instToCommit(inst);1347         }1348 1349         updateExeInstStats(inst);1351         // Check if branch prediction was correct, if not then we need1352         // to tell commit to squash in flight instructions.  Only1353         // handle this if there hasn’t already been something that1354         // redirects fetch in this group of instructions.1355 1356         // This probably needs to prioritize the redirects if a different1357         // scheduler is used.  Currently the scheduler schedules the oldest1358         // instruction first, so the branch resolution order will be correct.1359         ThreadID tid = inst-&gt;threadNumber;1360 1361         if (!fetchRedirect[tid] ||1362             !toCommit-&gt;squash[tid] ||1363             toCommit-&gt;squashedSeqNum[tid] &gt; inst-&gt;seqNum) {1364 1365             // Prevent testing for misprediction on load instructions,1366             // that have not been executed.1367             bool loadNotExecuted = !inst-&gt;isExecuted() &amp;&amp; inst-&gt;isLoad();1368 1369             if (inst-&gt;mispredicted() &amp;&amp; !loadNotExecuted) {1370                 fetchRedirect[tid] = true;1371 1372                 DPRINTF(IEW, “[tid:%i] [sn:%llu] Execute: “1373                         “Branch mispredict detected.\\n”,1374                         tid,inst-&gt;seqNum);1375                 DPRINTF(IEW, “[tid:%i] [sn:%llu] “1376                         “Predicted target was PC: %s\\n”,1377                         tid,inst-&gt;seqNum,inst-&gt;readPredTarg());1378                 DPRINTF(IEW, “[tid:%i] [sn:%llu] Execute: “1379                         “Redirecting fetch to PC: %s\\n”,1380                         tid,inst-&gt;seqNum,inst-&gt;pcState());1381                 // If incorrect, then signal the ROB that it must be squashed.1382                 squashDueToBranch(inst, tid);1383 1384                 ppMispredict-&gt;notify(inst);1385 1386                 if (inst-&gt;readPredTaken()) {1387                     predictedTakenIncorrect++;1388                 } else {1389                     predictedNotTakenIncorrect++;1390                 }1391             } else if (ldstQueue.violation(tid)) {1392                 assert(inst-&gt;isMemRef());1393                 // If there was an ordering violation, then get the1394                 // DynInst that caused the violation.  Note that this1395                 // clears the violation signal.1396                 DynInstPtr violator;1397                 violator = ldstQueue.getMemDepViolator(tid);1398 1399                 DPRINTF(IEW, “LDSTQ detected a violation. Violator PC: %s “1400                         “[sn:%lli], inst PC: %s [sn:%lli]. Addr is: %#x.\\n”,1401                         violator-&gt;pcState(), violator-&gt;seqNum,1402                         inst-&gt;pcState(), inst-&gt;seqNum, inst-&gt;physEffAddr);1403 1404                 fetchRedirect[tid] = true;1405 1406                 // Tell the instruction queue that a violation has occured.1407                 instQueue.violation(inst, violator);1408 1409                 // Squash.1410                 squashDueToMemOrder(violator, tid);1411 1412                 ++memOrderViolationEvents;1413             }1414         } else {1415             // Reset any state associated with redirects that will not1416             // be used.1417             if (ldstQueue.violation(tid)) {1418                 assert(inst-&gt;isMemRef());1419 1420                 DynInstPtr violator = ldstQueue.getMemDepViolator(tid);1421 1422                 DPRINTF(IEW, “LDSTQ detected a violation.  Violator PC: “1423                         “%s, inst PC: %s.  Addr is: %#x.\\n”,1424                         violator-&gt;pcState(), inst-&gt;pcState(),1425                         inst-&gt;physEffAddr);1426                 DPRINTF(IEW, “Violation will not be handled because “1427                         “already squashing\\n”);1428 1429                 ++memOrderViolationEvents;1430             }1431         }1432     }1433 1434     // Update and record activity if we processed any instructions.1435     if (inst_num) {1436         if (exeStatus == Idle) {1437             exeStatus = Running;1438         }1439 1440         updatedQueues = true;1441 1442         cpu-&gt;activityThisCycle();1443     }1444 1445     // Need to reset this in case a writeback event needs to write into the1446     // iew queue.  That way the writeback event will write into the correct1447     // spot in the queue.1448     wbNumInst = 0;1449 1450 }```ScheduleSchedule (InstructionQueue::scheduleReadyInsts()) The IQ manages the ready instructions (operands ready) in a ready list, and schedules them to an available FU. The latency of the FU is set here, and instructions are sent to execution when the FU done."
  },
  
  {
    "title": "O3 Cpu Rename",
    "url": "/posts/O3-CPU-rename/",
    "categories": "GEM5, Pipeline, O3",
    "tags": "",
    "date": "2021-05-29 00:00:00 -0400",
    





    
    "snippet": "RenameIt maintains the rename history of all instructions with destination registers, storing the arch register, the new physical register, and the old physical register.The information is required...",
    "content": "RenameIt maintains the rename history of all instructions with destination registers, storing the arch register, the new physical register, and the old physical register.The information is required to enable undoing of mappings if squashing happens, orfreeing up registers upon commit. Rename stage can be blockedwhen the ROB, IQ, or LSQ is going to be full. Rename also handles barriers and serializing instructionsby stalling them in rename until the back-end drains.It blocks the stage until the ROB is empty,and there are no instructions in flight to the ROB.Interface of rename stagecpu/o3/rename_impl.hh 214 template &lt;class Impl&gt; 215 void 216 DefaultRename&lt;Impl&gt;::setTimeBuffer(TimeBuffer&lt;TimeStruct&gt; *tb_ptr) 217 { 218     timeBuffer = tb_ptr; 219  220     // Setup wire to read information from time buffer, from IEW stage. 221     fromIEW = timeBuffer-&gt;getWire(-iewToRenameDelay); 222  223     // Setup wire to read infromation from time buffer, from commit stage. 224     fromCommit = timeBuffer-&gt;getWire(-commitToRenameDelay); 225  226     // Setup wire to write information to previous stages. 227     toDecode = timeBuffer-&gt;getWire(0); 228 } 229  230 template &lt;class Impl&gt; 231 void 232 DefaultRename&lt;Impl&gt;::setRenameQueue(TimeBuffer&lt;RenameStruct&gt; *rq_ptr) 233 { 234     renameQueue = rq_ptr; 235  236     // Setup wire to write information to future stages. 237     toIEW = renameQueue-&gt;getWire(0); 238 } 239  240 template &lt;class Impl&gt; 241 void 242 DefaultRename&lt;Impl&gt;::setDecodeQueue(TimeBuffer&lt;DecodeStruct&gt; *dq_ptr) 243 { 244     decodeQueue = dq_ptr; 245  246     // Setup wire to get information from decode. 247     fromDecode = decodeQueue-&gt;getWire(-decodeToRenameDelay); 248 }Mainly, there are three interfaces connected to the rename stage.First of all, to deliver the information processed by the rename stageto the IEW stage, it has toIEW wire. Also, to read some information from two other stages, decode and commit,it sets up fromDecode and fromCommit wires.Tick function of the rename stage 427 template &lt;class Impl&gt; 428 void 429 DefaultRename&lt;Impl&gt;::tick() 430 { 431     wroteToTimeBuffer = false; 432  433     blockThisCycle = false; 434  435     bool status_change = false; 436  437     toIEWIndex = 0; 438  439     sortInsts(); 440  441     list&lt;ThreadID&gt;::iterator threads = activeThreads-&gt;begin(); 442     list&lt;ThreadID&gt;::iterator end = activeThreads-&gt;end(); 443  444     // Check stall and squash signals. 445     while (threads != end) { 446         ThreadID tid = *threads++; 447  448         DPRINTF(Rename, \"Processing [tid:%i]\\n\", tid); 449  450         status_change = checkSignalsAndUpdate(tid) || status_change; 451  452         rename(status_change, tid); 453     } 454  455     if (status_change) { 456         updateStatus(); 457     } 458  459     if (wroteToTimeBuffer) { 460         DPRINTF(Activity, \"Activity this cycle.\\n\"); 461         cpu-&gt;activityThisCycle(); 462     } 463  464     threads = activeThreads-&gt;begin(); 465  466     while (threads != end) { 467         ThreadID tid = *threads++; 468  469         // If we committed this cycle then doneSeqNum will be &gt; 0 470         if (fromCommit-&gt;commitInfo[tid].doneSeqNum != 0 &amp;&amp; 471             !fromCommit-&gt;commitInfo[tid].squash &amp;&amp; 472             renameStatus[tid] != Squashing) { 473  474             removeFromHistory(fromCommit-&gt;commitInfo[tid].doneSeqNum, 475                                   tid); 476         } 477     } 478  479     // @todo: make into updateProgress function 480     for (ThreadID tid = 0; tid &lt; numThreads; tid++) { 481         instsInProgress[tid] -= fromIEW-&gt;iewInfo[tid].dispatched; 482         loadsInProgress[tid] -= fromIEW-&gt;iewInfo[tid].dispatchedToLQ; 483         storesInProgress[tid] -= fromIEW-&gt;iewInfo[tid].dispatchedToSQ; 484         assert(loadsInProgress[tid] &gt;= 0); 485         assert(storesInProgress[tid] &gt;= 0); 486         assert(instsInProgress[tid] &gt;=0); 487     } 488  489 }sortInsts 836 template &lt;class Impl&gt; 837 void 838 DefaultRename&lt;Impl&gt;::sortInsts() 839 { 840     int insts_from_decode = fromDecode-&gt;size; 841     for (int i = 0; i &lt; insts_from_decode; ++i) { 842         const DynInstPtr &amp;inst = fromDecode-&gt;insts[i]; 843         insts[inst-&gt;threadNumber].push_back(inst); 844 #if TRACING_ON 845         if (DTRACE(O3PipeView)) { 846             inst-&gt;renameTick = curTick() - inst-&gt;fetchTick; 847         } 848 #endif 849     } 850 }Because the register maintains all instructions regardless of origin of the instructions (initiated by which thread), it should sort instructions based on the thread that instantiated the instruction. For that purpose, each instruction maintains information representing which thread is the owner of that instruction.checkSignalsAndUpdate1331 template &lt;class Impl&gt;1332 bool1333 DefaultRename&lt;Impl&gt;::checkSignalsAndUpdate(ThreadID tid)1334 {1335     // Check if there's a squash signal, squash if there is1336     // Check stall signals, block if necessary.1337     // If status was blocked1338     //     check if stall conditions have passed1339     //         if so then go to unblocking1340     // If status was Squashing1341     //     check if squashing is not high.  Switch to running this cycle.1342     // If status was serialize stall1343     //     check if ROB is empty and no insts are in flight to the ROB1344 1345     readFreeEntries(tid);1346     readStallSignals(tid);1347 1348     if (fromCommit-&gt;commitInfo[tid].squash) {1349         DPRINTF(Rename, \"[tid:%i] Squashing instructions due to squash from \"1350                 \"commit.\\n\", tid);1351 1352         squash(fromCommit-&gt;commitInfo[tid].doneSeqNum, tid);1353 1354         return true;1355     }1356 1357     if (checkStall(tid)) {1358         return block(tid);1359     }1360 1361     if (renameStatus[tid] == Blocked) {1362         DPRINTF(Rename, \"[tid:%i] Done blocking, switching to unblocking.\\n\",1363                 tid);1364 1365         renameStatus[tid] = Unblocking;1366 1367         unblock(tid);1368 1369         return true;1370     }1371 1372     if (renameStatus[tid] == Squashing) {1373         // Switch status to running if rename isn't being told to block or1374         // squash this cycle.1375         if (resumeSerialize) {1376             DPRINTF(Rename,1377                     \"[tid:%i] Done squashing, switching to serialize.\\n\", tid);1378 1379             renameStatus[tid] = SerializeStall;1380             return true;1381         } else if (resumeUnblocking) {1382             DPRINTF(Rename,1383                     \"[tid:%i] Done squashing, switching to unblocking.\\n\",1384                     tid);1385             renameStatus[tid] = Unblocking;1386             return true;1387         } else {1388             DPRINTF(Rename, \"[tid:%i] Done squashing, switching to running.\\n\",1389                     tid);1390             renameStatus[tid] = Running;1391             return false;1392         }1393     }1394 1395     if (renameStatus[tid] == SerializeStall) {1396         // Stall ends once the ROB is free.1397         DPRINTF(Rename, \"[tid:%i] Done with serialize stall, switching to \"1398                 \"unblocking.\\n\", tid);1399 1400         DynInstPtr serial_inst = serializeInst[tid];1401 1402         renameStatus[tid] = Unblocking;1403 1404         unblock(tid);1405 1406         DPRINTF(Rename, \"[tid:%i] Processing instruction [%lli] with \"1407                 \"PC %s.\\n\", tid, serial_inst-&gt;seqNum, serial_inst-&gt;pcState());1408 1409         // Put instruction into queue here.1410         serial_inst-&gt;clearSerializeBefore();1411 1412         if (!skidBuffer[tid].empty()) {1413             skidBuffer[tid].push_front(serial_inst);1414         } else {1415             insts[tid].push_front(serial_inst);1416         }1417 1418         DPRINTF(Rename, \"[tid:%i] Instruction must be processed by rename.\"1419                 \" Adding to front of list.\\n\", tid);1420 1421         serializeInst[tid] = NULL;1422 1423         return true;1424     }1425 1426     // If we've reached this point, we have not gotten any signals that1427     // cause rename to change its status.  Rename remains the same as before.1428     return false;1429 }Note that most of the operation sequence of the checkSignalsAndUpdate is very similar to the checkSignalsAndUpdate of the decode stage.It checks the stall and squash signal and execute associated code.For the stall, it executes the block function. For the squash, it invokes the squash function.However, in detail there are two noticeable differencesin the readFreeEntries and checkStall function.1295 template &lt;class Impl&gt;1296 void1297 DefaultRename&lt;Impl&gt;::readFreeEntries(ThreadID tid)1298 {1299     if (fromIEW-&gt;iewInfo[tid].usedIQ)1300         freeEntries[tid].iqEntries = fromIEW-&gt;iewInfo[tid].freeIQEntries;1301 1302     if (fromIEW-&gt;iewInfo[tid].usedLSQ) {1303         freeEntries[tid].lqEntries = fromIEW-&gt;iewInfo[tid].freeLQEntries;1304         freeEntries[tid].sqEntries = fromIEW-&gt;iewInfo[tid].freeSQEntries;1305     }1306 1307     if (fromCommit-&gt;commitInfo[tid].usedROB) {1308         freeEntries[tid].robEntries =1309             fromCommit-&gt;commitInfo[tid].freeROBEntries;1310         emptyROB[tid] = fromCommit-&gt;commitInfo[tid].emptyROB;1311     }1312 1313     DPRINTF(Rename, \"[tid:%i] Free IQ: %i, Free ROB: %i, \"1314                     \"Free LQ: %i, Free SQ: %i, FreeRM %i(%i %i %i %i %i)\\n\",1315             tid,1316             freeEntries[tid].iqEntries,1317             freeEntries[tid].robEntries,1318             freeEntries[tid].lqEntries,1319             freeEntries[tid].sqEntries,1320             renameMap[tid]-&gt;numFreeEntries(),1321             renameMap[tid]-&gt;numFreeIntEntries(),1322             renameMap[tid]-&gt;numFreeFloatEntries(),1323             renameMap[tid]-&gt;numFreeVecEntries(),1324             renameMap[tid]-&gt;numFreePredEntries(),1325             renameMap[tid]-&gt;numFreeCCEntries());1326 1327     DPRINTF(Rename, \"[tid:%i] %i instructions not yet in ROB\\n\",1328             tid, instsInProgress[tid]);1329 }After the IEW stage executes and commit the instructions,it should reports the Rename stage that it has used the allocated resources andgood to reassign them to other instructions for renaming. We will see how the freeEntries are retrievedto its resource pool and reassigned to other instructions.1263 template &lt;class Impl&gt;1264 bool1265 DefaultRename&lt;Impl&gt;::checkStall(ThreadID tid)1266 {1267     bool ret_val = false;1268 1269     if (stalls[tid].iew) {1270         DPRINTF(Rename,\"[tid:%i] Stall from IEW stage detected.\\n\", tid);1271         ret_val = true;1272     } else if (calcFreeROBEntries(tid) &lt;= 0) {1273         DPRINTF(Rename,\"[tid:%i] Stall: ROB has 0 free entries.\\n\", tid);1274         ret_val = true;1275     } else if (calcFreeIQEntries(tid) &lt;= 0) {1276         DPRINTF(Rename,\"[tid:%i] Stall: IQ has 0 free entries.\\n\", tid);1277         ret_val = true;1278     } else if (calcFreeLQEntries(tid) &lt;= 0 &amp;&amp; calcFreeSQEntries(tid) &lt;= 0) {1279         DPRINTF(Rename,\"[tid:%i] Stall: LSQ has 0 free entries.\\n\", tid);1280         ret_val = true;1281     } else if (renameMap[tid]-&gt;numFreeEntries() &lt;= 0) {1282         DPRINTF(Rename,\"[tid:%i] Stall: RenameMap has 0 free entries.\\n\", tid);1283         ret_val = true;1284     } else if (renameStatus[tid] == SerializeStall &amp;&amp;1285                (!emptyROB[tid] || instsInProgress[tid])) {1286         DPRINTF(Rename,\"[tid:%i] Stall: Serialize stall and ROB is not \"1287                 \"empty.\\n\",1288                 tid);1289         ret_val = true;1290     }1291 1292     return ret_val;1293 }There could be various reasons of the stall.It might be because of the stall signal received from other stagesor the internal issues of the rename stage due to lack of resources.rename 491 template&lt;class Impl&gt; 492 void 493 DefaultRename&lt;Impl&gt;::rename(bool &amp;status_change, ThreadID tid) 494 { 495     // If status is Running or idle, 496     //     call renameInsts() 497     // If status is Unblocking, 498     //     buffer any instructions coming from decode 499     //     continue trying to empty skid buffer 500     //     check if stall conditions have passed 501  502     if (renameStatus[tid] == Blocked) { 503         ++renameBlockCycles; 504     } else if (renameStatus[tid] == Squashing) { 505         ++renameSquashCycles; 506     } else if (renameStatus[tid] == SerializeStall) { 507         ++renameSerializeStallCycles; 508         // If we are currently in SerializeStall and resumeSerialize 509         // was set, then that means that we are resuming serializing 510         // this cycle.  Tell the previous stages to block. 511         if (resumeSerialize) { 512             resumeSerialize = false; 513             block(tid); 514             toDecode-&gt;renameUnblock[tid] = false; 515         } 516     } else if (renameStatus[tid] == Unblocking) { 517         if (resumeUnblocking) { 518             block(tid); 519             resumeUnblocking = false; 520             toDecode-&gt;renameUnblock[tid] = false; 521         } 522     } 523  524     if (renameStatus[tid] == Running || 525         renameStatus[tid] == Idle) { 526         DPRINTF(Rename, 527                 \"[tid:%i] \" 528                 \"Not blocked, so attempting to run stage.\\n\", 529                 tid); 530  531         renameInsts(tid); 532     } else if (renameStatus[tid] == Unblocking) { 533         renameInsts(tid); 534  535         if (validInsts()) { 536             // Add the current inputs to the skid buffer so they can be 537             // reprocessed when this stage unblocks. 538             skidInsert(tid); 539         } 540  541         // If we switched over to blocking, then there's a potential for 542         // an overall status change. 543         status_change = unblock(tid) || status_change || blockThisCycle; 544     } 545 }When there is no stall or blocking, now it can rename the instructions.When the current renameStatus is Running or Idle, it will invoke renameInsts function to rename the instructions Also, when the renameStatus is Unblocking,which means that the rename stage is being recovered from the Blocking status, it should also invoke the renameInsts function.renameInsts: the main rename functionThe most of the rename function operations are done by the renameInsts function. 547 template &lt;class Impl&gt; 548 void 549 DefaultRename&lt;Impl&gt;::renameInsts(ThreadID tid) 550 { 551     // Instructions can be either in the skid buffer or the queue of 552     // instructions coming from decode, depending on the status. 553     int insts_available = renameStatus[tid] == Unblocking ? 554         skidBuffer[tid].size() : insts[tid].size(); 555  556     // Check the decode queue to see if instructions are available. 557     // If there are no available instructions to rename, then do nothing. 558     if (insts_available == 0) { 559         DPRINTF(Rename, \"[tid:%i] Nothing to do, breaking out early.\\n\", 560                 tid); 561         // Should I change status to idle? 562         ++renameIdleCycles; 563         return; 564     } else if (renameStatus[tid] == Unblocking) { 565         ++renameUnblockCycles; 566     } else if (renameStatus[tid] == Running) { 567         ++renameRunCycles; 568     }First, it checks the current status of the rename stage. If the current status is Unblock, it should fetches instructions from the skidBuffer instead of the insts buffer. Also, even though it is running or idle status, it might not have available instructions because of stall, squash.Therefore, it first checks whether the instructions are ready to be renamed.Checking ROB and IQ space 570     // Will have to do a different calculation for the number of free 571     // entries. 572     int free_rob_entries = calcFreeROBEntries(tid); 573     int free_iq_entries  = calcFreeIQEntries(tid); 574     int min_free_entries = free_rob_entries; 575  576     FullSource source = ROB; 577  578     if (free_iq_entries &lt; min_free_entries) { 579         min_free_entries = free_iq_entries; 580         source = IQ; 581     } 582  583     // Check if there's any space left. 584     if (min_free_entries &lt;= 0) { 585         DPRINTF(Rename, 586                 \"[tid:%i] Blocking due to no free ROB/IQ/ entries.\\n\" 587                 \"ROB has %i free entries.\\n\" 588                 \"IQ has %i free entries.\\n\", 589                 tid, free_rob_entries, free_iq_entries); 590  591         blockThisCycle = true; 592  593         block(tid); 594  595         incrFullStat(source); 596  597         return; 598     } else if (min_free_entries &lt; insts_available) { 599         DPRINTF(Rename, 600                 \"[tid:%i] \" 601                 \"Will have to block this cycle. \" 602                 \"%i insts available, \" 603                 \"but only %i insts can be renamed due to ROB/IQ/LSQ limits.\\n\", 604                 tid, insts_available, min_free_entries); 605  606         insts_available = min_free_entries; 607  608         blockThisCycle = true; 609  610         incrFullStat(source); 611     }It needs to check ROB and instruction queue entries are availablebefore the renaming. When there is no space, it should stall right a way.However, if those entries are partially available,only the available parts of the instructions are processed and postpone stall to later(blockThisCycle = true).Checking serialization 613     InstQueue &amp;insts_to_rename = renameStatus[tid] == Unblocking ? 614         skidBuffer[tid] : insts[tid]; 615  616     DPRINTF(Rename, 617             \"[tid:%i] \" 618             \"%i available instructions to send iew.\\n\", 619             tid, insts_available); 620  621     DPRINTF(Rename, 622             \"[tid:%i] \" 623             \"%i insts pipelining from Rename | \" 624             \"%i insts dispatched to IQ last cycle.\\n\", 625             tid, instsInProgress[tid], fromIEW-&gt;iewInfo[tid].dispatched); 626  627     // Handle serializing the next instruction if necessary. 628     if (serializeOnNextInst[tid]) { 629         if (emptyROB[tid] &amp;&amp; instsInProgress[tid] == 0) { 630             // ROB already empty; no need to serialize. 631             serializeOnNextInst[tid] = false; 632         } else if (!insts_to_rename.empty()) { 633             insts_to_rename.front()-&gt;setSerializeBefore(); 634         } 635     }It also manages serializing instructions and generate stalls to enforce serialization operation. To this end, it provides associated functions and fields.I will not cover the details here because they are utilized later when each instruction is processedby the rename stage’s main loop,Checking availability of the LQ and SQ 637     int renamed_insts = 0; 638  639     while (insts_available &gt; 0 &amp;&amp;  toIEWIndex &lt; renameWidth) { 640         DPRINTF(Rename, \"[tid:%i] Sending instructions to IEW.\\n\", tid); 641  642         assert(!insts_to_rename.empty()); 643  644         DynInstPtr inst = insts_to_rename.front(); 645  646         //For all kind of instructions, check ROB and IQ first 647         //For load instruction, check LQ size and take into account the inflight loads 648         //For store instruction, check SQ size and take into account the inflight stores 649  650         if (inst-&gt;isLoad()) { 651             if (calcFreeLQEntries(tid) &lt;= 0) { 652                 DPRINTF(Rename, \"[tid:%i] Cannot rename due to no free LQ\\n\"); 653                 source = LQ; 654                 incrFullStat(source); 655                 break; 656             } 657         } 658  659         if (inst-&gt;isStore() || inst-&gt;isAtomic()) { 660             if (calcFreeSQEntries(tid) &lt;= 0) { 661                 DPRINTF(Rename, \"[tid:%i] Cannot rename due to no free SQ\\n\"); 662                 source = SQ; 663                 incrFullStat(source); 664                 break; 665             } 666         }Now we will take a look at the main loop of the rename stage.It traverse all instructions stored in the insts_to_rename. Note that instructions can be retrieved from the Insts or the skidBuffer depending on the status of the current rename stage. Although we already checked the availability of IQ and ROB,rename stage further checks the availability of the LoadQueue (LQ) and StoreQueue (SQ)if the instruction is memory related operation.Note that issuing memory operation will consume one entry from the corresponding queue.If the LQ or SQ is full, then set the source as LQ or SQ to let the rest of the decode stage to know that the instruction cannot be issued to the next stage due to the lack of LQ or SQ and break the loop and stallthe rename stage(For statistics).Consume one instruction and check register availability 668         insts_to_rename.pop_front(); 669  670         if (renameStatus[tid] == Unblocking) { 671             DPRINTF(Rename, 672                     \"[tid:%i] \" 673                     \"Removing [sn:%llu] PC:%s from rename skidBuffer\\n\", 674                     tid, inst-&gt;seqNum, inst-&gt;pcState()); 675         } 676  677         if (inst-&gt;isSquashed()) { 678             DPRINTF(Rename, 679                     \"[tid:%i] \" 680                     \"instruction %i with PC %s is squashed, skipping.\\n\", 681                     tid, inst-&gt;seqNum, inst-&gt;pcState()); 682  683             ++renameSquashedInsts; 684  685             // Decrement how many instructions are available. 686             --insts_available; 687  688             continue; 689         } 690  691         DPRINTF(Rename, 692                 \"[tid:%i] \" 693                 \"Processing instruction [sn:%llu] with PC %s.\\n\", 694                 tid, inst-&gt;seqNum, inst-&gt;pcState()); 695  696         // Check here to make sure there are enough destination registers 697         // to rename to.  Otherwise block. 698         if (!renameMap[tid]-&gt;canRename(inst-&gt;numIntDestRegs(), 699                                        inst-&gt;numFPDestRegs(), 700                                        inst-&gt;numVecDestRegs(), 701                                        inst-&gt;numVecElemDestRegs(), 702                                        inst-&gt;numVecPredDestRegs(), 703                                        inst-&gt;numCCDestRegs())) { 704             DPRINTF(Rename, 705                     \"Blocking due to \" 706                     \" lack of free physical registers to rename to.\\n\"); 707             blockThisCycle = true; 708             insts_to_rename.push_front(inst); 709             ++renameFullRegistersEvents; 710  711             break; 712         }After it is guaranteed that the resources such as ROB, IQ, LQ, SQ are suffice to rename new instruction,it consumes one instruction from the buffer (Line 668).However, if there are not enough physical registers to rename the instruction’s operands, then it should not be consumed.renameMapgem5/src/cpu/o3/cpu.hh583     /** The rename map. */584     typename CPUPolicy::RenameMap renameMap[Impl::MaxThreads];gem5/src/cpu/o3/cpu_policy.hh 60 template&lt;class Impl&gt; 61 struct SimpleCPUPolicy 62 { 63     /** Typedef for the freelist of registers. */ 64     typedef UnifiedFreeList FreeList; 65     /** Typedef for the rename map. */ 66     typedef UnifiedRenameMap RenameMap;The renameMap contains all the hardware registers accessible by the processor. For example, even though the ISA exposes only handful of registersto the users, there are lots of internal registers to execute instructions. The O3 CPU utilize the UnifiedRenameMap. Let’s take a look at the details.UnifiedRenameMap has different types of SimpleRenameMaps163 /**164  * Unified register rename map for all classes of registers.  Wraps a165  * set of class-specific rename maps.  Methods that do not specify a166  * register class (e.g., rename()) take register ids,167  * while methods that do specify a register class (e.g., renameInt())168  * take register indices.169  */170 class UnifiedRenameMap171 {172   private:173     static constexpr uint32_t NVecElems = TheISA::NumVecElemPerVecReg;174     using VecReg = TheISA::VecReg;175     using VecPredReg = TheISA::VecPredReg;176 177     /** The integer register rename map */178     SimpleRenameMap intMap;179 180     /** The floating-point register rename map */181     SimpleRenameMap floatMap;182 183     /** The condition-code register rename map */184     SimpleRenameMap ccMap;185 186     /** The vector register rename map */187     SimpleRenameMap vecMap;188 189     /** The vector element register rename map */190     SimpleRenameMap vecElemMap;191 192     /** The predicate register rename map */193     SimpleRenameMap predMap;194 195     using VecMode = Enums::VecRegRenameMode;196     VecMode vecMode;The renameMap used by the O3 is just a wrapper of the renameMap of each types of registers. As shown in the above class definition, it contains integer, float, vector, and other types of registes 237         renameMap[tid].init(&amp;regFile, TheISA::ZeroReg, fpZeroReg, 238                             &amp;freeList, vecMode); ...... 241     // Initialize rename map to assign physical registers to the 242     // architectural registers for active threads only. 243     for (ThreadID tid = 0; tid &lt; active_threads; tid++) { 244         for (RegIndex ridx = 0; ridx &lt; TheISA::NumIntRegs; ++ridx) { 245             // Note that we can't use the rename() method because we don't 246             // want special treatment for the zero register at this point 247             PhysRegIdPtr phys_reg = freeList.getIntReg(); 248             renameMap[tid].setEntry(RegId(IntRegClass, ridx), phys_reg); 249             commitRenameMap[tid].setEntry(RegId(IntRegClass, ridx), phys_reg); 250         } 251  252         for (RegIndex ridx = 0; ridx &lt; TheISA::NumFloatRegs; ++ridx) { 253             PhysRegIdPtr phys_reg = freeList.getFloatReg(); 254             renameMap[tid].setEntry(RegId(FloatRegClass, ridx), phys_reg); 255             commitRenameMap[tid].setEntry( 256                     RegId(FloatRegClass, ridx), phys_reg); 257         } 258  259         /* Here we need two 'interfaces' the 'whole register' and the 260          * 'register element'. At any point only one of them will be 261          * active. */ 262         if (vecMode == Enums::Full) { 263             /* Initialize the full-vector interface */ 264             for (RegIndex ridx = 0; ridx &lt; TheISA::NumVecRegs; ++ridx) { 265                 RegId rid = RegId(VecRegClass, ridx); 266                 PhysRegIdPtr phys_reg = freeList.getVecReg(); 267                 renameMap[tid].setEntry(rid, phys_reg); 268                 commitRenameMap[tid].setEntry(rid, phys_reg); 269             } 270         } else { 271             /* Initialize the vector-element interface */ 272             for (RegIndex ridx = 0; ridx &lt; TheISA::NumVecRegs; ++ridx) { 273                 for (ElemIndex ldx = 0; ldx &lt; TheISA::NumVecElemPerVecReg; 274                         ++ldx) { 275                     RegId lrid = RegId(VecElemClass, ridx, ldx); 276                     PhysRegIdPtr phys_elem = freeList.getVecElem(); 277                     renameMap[tid].setEntry(lrid, phys_elem); 278                     commitRenameMap[tid].setEntry(lrid, phys_elem); 279                 } 280             } 281         } 282  283         for (RegIndex ridx = 0; ridx &lt; TheISA::NumVecPredRegs; ++ridx) { 284             PhysRegIdPtr phys_reg = freeList.getVecPredReg(); 285             renameMap[tid].setEntry(RegId(VecPredRegClass, ridx), phys_reg); 286             commitRenameMap[tid].setEntry( 287                     RegId(VecPredRegClass, ridx), phys_reg); 288         } 289  290         for (RegIndex ridx = 0; ridx &lt; TheISA::NumCCRegs; ++ridx) { 291             PhysRegIdPtr phys_reg = freeList.getCCReg(); 292             renameMap[tid].setEntry(RegId(CCRegClass, ridx), phys_reg); 293             commitRenameMap[tid].setEntry(RegId(CCRegClass, ridx), phys_reg); 294         } 295     } 296  297     rename.setRenameMap(renameMap);The above code initialize all entries of the renameMap. When the setEntry is invoked through the UnifiedRenameMap, it invokes setEntry function of the SimpleRenameMapfor all register types of the O3 CPU.302     /**303      * Update rename map with a specific mapping.  Generally used to304      * roll back to old mappings on a squash.  This version takes a305      * flattened architectural register id and calls the306      * appropriate class-specific rename table.307      * @param arch_reg The architectural register to remap.308      * @param phys_reg The physical register to remap it to.309      */310     void setEntry(const RegId&amp; arch_reg, PhysRegIdPtr phys_reg)311     {312         switch (arch_reg.classValue()) {313           case IntRegClass:314             assert(phys_reg-&gt;isIntPhysReg());315             return intMap.setEntry(arch_reg, phys_reg);316 317           case FloatRegClass:318             assert(phys_reg-&gt;isFloatPhysReg());319             return floatMap.setEntry(arch_reg, phys_reg);320 321           case VecRegClass:322             assert(phys_reg-&gt;isVectorPhysReg());323             assert(vecMode == Enums::Full);324             return vecMap.setEntry(arch_reg, phys_reg);325 326           case VecElemClass:327             assert(phys_reg-&gt;isVectorPhysElem());328             assert(vecMode == Enums::Elem);329             return vecElemMap.setEntry(arch_reg, phys_reg);330 331           case VecPredRegClass:332             assert(phys_reg-&gt;isVecPredPhysReg());333             return predMap.setEntry(arch_reg, phys_reg);334 335           case CCRegClass:336             assert(phys_reg-&gt;isCCPhysReg());337             return ccMap.setEntry(arch_reg, phys_reg);338 339           case MiscRegClass:340             // Misc registers do not actually rename, so don't change341             // their mappings.  We end up here when a commit or squash342             // tries to update or undo a hardwired misc reg nmapping,343             // which should always be setting it to what it already is.344             assert(phys_reg == lookup(arch_reg));345             return;346 347           default:348             panic(\"rename setEntry(): unknown reg class %s\\n\",349                   arch_reg.className());350         }351     }The setEntry inserts new entry to the renameMap. However, because UnifiedRenameMap is just a wrapper class consisting of multiple SimpleRenameMaps with different types of registers, it inserts an entry to associated SimpleRenameMaps objectbased on the type of register.canRename checks availability of the register resource. 696         // Check here to make sure there are enough destination registers 697         // to rename to.  Otherwise block. 698         if (!renameMap[tid]-&gt;canRename(inst-&gt;numIntDestRegs(), 699                                        inst-&gt;numFPDestRegs(), 700                                        inst-&gt;numVecDestRegs(), 701                                        inst-&gt;numVecElemDestRegs(), 702                                        inst-&gt;numVecPredDestRegs(), 703                                        inst-&gt;numCCDestRegs())) { 704             DPRINTF(Rename, 705                     \"Blocking due to \" 706                     \" lack of free physical registers to rename to.\\n\"); 707             blockThisCycle = true; 708             insts_to_rename.push_front(inst); 709             ++renameFullRegistersEvents; 710 711             break; 712         }Before the rename its registers, it first checks whether the current physical resources are available    /**     * Return whether there are enough registers to serve the request.     */    bool canRename(uint32_t intRegs, uint32_t floatRegs, uint32_t vectorRegs,                   uint32_t vecElemRegs, uint32_t vecPredRegs,                   uint32_t ccRegs) const    {        return intRegs &lt;= intMap.numFreeEntries() &amp;&amp;            floatRegs &lt;= floatMap.numFreeEntries() &amp;&amp;            vectorRegs &lt;= vecMap.numFreeEntries() &amp;&amp;            vecElemRegs &lt;= vecElemMap.numFreeEntries() &amp;&amp;            vecPredRegs &lt;= predMap.numFreeEntries() &amp;&amp;            ccRegs &lt;= ccMap.numFreeEntries();    }Handle serialization instructionIf there is enough resources to rename instructions,now it checks whether current instructions should be serialized or protected by the memory barriers. 714         // Handle serializeAfter/serializeBefore instructions. 715         // serializeAfter marks the next instruction as serializeBefore. 716         // serializeBefore makes the instruction wait in rename until the ROB 717         // is empty. 718  719         // In this model, IPR accesses are serialize before 720         // instructions, and store conditionals are serialize after 721         // instructions.  This is mainly due to lack of support for 722         // out-of-order operations of either of those classes of 723         // instructions. 724         if ((inst-&gt;isIprAccess() || inst-&gt;isSerializeBefore()) &amp;&amp; 725             !inst-&gt;isSerializeHandled()) { 726             DPRINTF(Rename, \"Serialize before instruction encountered.\\n\"); 727  728             if (!inst-&gt;isTempSerializeBefore()) { 729                 renamedSerializing++; 730                 inst-&gt;setSerializeHandled(); 731             } else { 732                 renamedTempSerializing++; 733             } 734  735             // Change status over to SerializeStall so that other stages know 736             // what this is blocked on. 737             renameStatus[tid] = SerializeStall; 738  739             serializeInst[tid] = inst; 740  741             blockThisCycle = true; 742  743             break; 744         } else if ((inst-&gt;isStoreConditional() || inst-&gt;isSerializeAfter()) &amp;&amp; 745                    !inst-&gt;isSerializeHandled()) { 746             DPRINTF(Rename, \"Serialize after instruction encountered.\\n\"); 747  748             renamedSerializing++; 749  750             inst-&gt;setSerializeHandled(); 751  752             serializeAfter(insts_to_rename, tid); 753         }StaticInst class has flags member field which represents properties of one instruction such as serializing, memory barrier, load operation, etc. Also, it has corresponding get methods to retrieve those flags from the StaticInst objects. Remember that all the instructions we generated at the fetch stage was the object of the StaticInst. Also, its flags are set based on the implementation of the microops of different architectures. Therefore, the rename stage determines whether it should block the stage or moves to the next instructionby checking the isSerializeAfter and isSerializeBefore of the current static instruction,Note that the serializeBefore means that the current instruction should be blocked.Therefore, it sets current instruction as serializeInst (Line 739)and status of the current rename stage as SerializeStalland break the loop.1431 template&lt;class Impl&gt;1432 void1433 DefaultRename&lt;Impl&gt;::serializeAfter(InstQueue &amp;inst_list, ThreadID tid)1434 {1435     if (inst_list.empty()) {1436         // Mark a bit to say that I must serialize on the next instruction.1437         serializeOnNextInst[tid] = true;1438         return;1439     }1440 1441     // Set the next instruction as serializing.1442     inst_list.front()-&gt;setSerializeBefore();1443 }However, when the instruction has serializeAfter flag,the next instruction after the current instruction should be blockednot the current one. In this case we should consider two cases of the rename stage. When it has no instructions to be renamed after the current one,we cannot set the next instruction to be serialized.Therefore, just let the rename stage be aware of thatthe next instruction should be serialized (Line 1437).If there is another instruction in the queue, it directly set the flag of that instructionby invoking setSerializeBefore function (1442). 627     // Handle serializing the next instruction if necessary. 628     if (serializeOnNextInst[tid]) { 629         if (emptyROB[tid] &amp;&amp; instsInProgress[tid] == 0) { 630             // ROB already empty; no need to serialize. 631             serializeOnNextInst[tid] = false; 632         } else if (!insts_to_rename.empty()) { 633             insts_to_rename.front()-&gt;setSerializeBefore(); 634         } 635     }As shown in the above code (Line 632-633),at the next clock cycle,when the renameInsts function is executed,it checks whether the serializeOnNextInst has been set.which means that the last instruction was serializeAfter instruction at the previous clock cycle. In that case it sets the current instructionto be renamed as serializeBefore to make serialization.Wait until all issued instructions are resolvedRemember that the tick function of the rename stage always check the signal before invoking the rename function.This checkSignalsAndUpdate function checks whether the rename stage can be continued.1333 DefaultRename&lt;Impl&gt;::checkSignalsAndUpdate(ThreadID tid)1334 {1335     // Check if there's a squash signal, squash if there is1336     // Check stall signals, block if necessary.1337     // If status was blocked1338     //     check if stall conditions have passed1339     //         if so then go to unblocking1340     // If status was Squashing1341     //     check if squashing is not high.  Switch to running this cycle.1342     // If status was serialize stall1343     //     check if ROB is empty and no insts are in flight to the ROB1344 1345     readFreeEntries(tid);1346     readStallSignals(tid);1347 1348     if (fromCommit-&gt;commitInfo[tid].squash) {1349         DPRINTF(Rename, \"[tid:%i] Squashing instructions due to squash from \"1350                 \"commit.\\n\", tid);1351 1352         squash(fromCommit-&gt;commitInfo[tid].doneSeqNum, tid);1353 1354         return true;1355     }1356 1357     if (checkStall(tid)) {1358         return block(tid);1359     }One of the important function of checkSignalsAndUpdate is checkStall functionthat checks whether the current rename stage is ready to be free from the stall.If the rename stage should be stalled more, then it returns false and continue to block the rename stage at current clock cycle (Line 1358). However, if it turns out that previous stall doesn’t block the rename stage further,it tries to recover from the stall based on their previous stall reasons.1265 DefaultRename&lt;Impl&gt;::checkStall(ThreadID tid)1266 {   1267     bool ret_val = false;1268     1269     if (stalls[tid].iew) {1270         DPRINTF(Rename,\"[tid:%i] Stall from IEW stage detected.\\n\", tid);1271         ret_val = true;1272     } else if (calcFreeROBEntries(tid) &lt;= 0) {1273         DPRINTF(Rename,\"[tid:%i] Stall: ROB has 0 free entries.\\n\", tid);1274         ret_val = true;1275     } else if (calcFreeIQEntries(tid) &lt;= 0) {1276         DPRINTF(Rename,\"[tid:%i] Stall: IQ has 0 free entries.\\n\", tid);1277         ret_val = true;1278     } else if (calcFreeLQEntries(tid) &lt;= 0 &amp;&amp; calcFreeSQEntries(tid) &lt;= 0) {1279         DPRINTF(Rename,\"[tid:%i] Stall: LSQ has 0 free entries.\\n\", tid);1280         ret_val = true;1281     } else if (renameMap[tid]-&gt;numFreeEntries() &lt;= 0) {1282         DPRINTF(Rename,\"[tid:%i] Stall: RenameMap has 0 free entries.\\n\", tid);1283         ret_val = true;1284     } else if (renameStatus[tid] == SerializeStall &amp;&amp;1285                (!emptyROB[tid] || instsInProgress[tid])) {1286         DPRINTF(Rename,\"[tid:%i] Stall: Serialize stall and ROB is not \"1287                 \"empty.\\n\",1288                 tid);1289         ret_val = true;1290     }1291     1292     return ret_val;1293 }For example,when the current renameStatus is SerializeStall,the rename stage should not be executed again until all previous instruction in the pipeline will be dispatched to the execution units. Therefore, it invokes instsInProgress[tid]to check whether current hardware thread still have some remaining instructions to process. When the all previous instructions are resolved,it will return false and checkStall will return false.If the checkStall returns false and doesn’t block the rename stage anymore,it will try to recover from the stall based on their previous stall reasons.1395     if (renameStatus[tid] == SerializeStall) {1396         // Stall ends once the ROB is free.1397         DPRINTF(Rename, \"[tid:%i] Done with serialize stall, switching to \"1398                 \"unblocking.\\n\", tid);1399         1400         DynInstPtr serial_inst = serializeInst[tid];1401         1402         renameStatus[tid] = Unblocking;1403         1404         unblock(tid);1405         1406         DPRINTF(Rename, \"[tid:%i] Processing instruction [%lli] with \"1407                 \"PC %s.\\n\", tid, serial_inst-&gt;seqNum, serial_inst-&gt;pcState());1408         1409         // Put instruction into queue here.1410         serial_inst-&gt;clearSerializeBefore();1411         1412         if (!skidBuffer[tid].empty()) {1413             skidBuffer[tid].push_front(serial_inst);1414         } else {1415             insts[tid].push_front(serial_inst);1416         }1417         1418         DPRINTF(Rename, \"[tid:%i] Instruction must be processed by rename.\"1419                 \" Adding to front of list.\\n\", tid);1420         1421         serializeInst[tid] = NULL;1422         1423         return true;1424     }For example, if the previous reason of stall was SerializeStall,the Line 1395-1424 will be executed.Note that the serialized instruction could not been executed until all previous instructions had been dispatched by the IEW stage.Therefore, the serializing instruction should be reinserted into the instruction buffer of the rename stage (Line 1412-1416).Also, it cleans up the serializeInst field of the rename stage (line 1421).Next time when the rename stage is recovered from the unblocking stage,it will process the serializing instruction that have stalled the rename stage.X86 in GEM5 provides macro setting serialization147         def serializeBefore(self):148             self.serialize_before = True149         def serializeAfter(self):150             self.serialize_after = True151 152         def function_call(self):153             self.function_call = True154         def function_return(self):155             self.function_return = True156 157         def __init__(self, name):158             super(X86Macroop, self).__init__(name)159             self.directives = {160                 \"adjust_env\" : self.setAdjustEnv,161                 \"adjust_imm\" : self.adjustImm,162                 \"adjust_disp\" : self.adjustDisp,163                 \"serialize_before\" : self.serializeBefore,164                 \"serialize_after\" : self.serializeAfter,165                 \"function_call\" : self.function_call,166                 \"function_return\" : self.function_return167             }For macroop definition, when .serialize_before or .serialize_after keyword is found in their definition,the GEM5 parser invokes the self.serializeBefore and self.serializeAfter function respectively to set the serialize_before and serialize_after memeber field as true.205         def getDefinition(self, env):206             #FIXME This first parameter should be the mnemonic. I need to207             #write some code which pulls that out208             numMicroops = len(self.microops)209             allocMicroops = ''210             micropc = 0211             for op in self.microops:212                 flags = [\"IsMicroop\"]213                 if micropc == 0:214                     flags.append(\"IsFirstMicroop\")215 216                     if self.serialize_before:217                         flags.append(\"IsSerializing\")218                         flags.append(\"IsSerializeBefore\")219 220                 if micropc == numMicroops - 1:221                     flags.append(\"IsLastMicroop\")222 223                     if self.serialize_after:224                         flags.append(\"IsSerializing\")225                         flags.append(\"IsSerializeAfter\")226 227                     if self.function_call:228                         flags.append(\"IsCall\")229                         flags.append(\"IsUncondControl\")230                     if self.function_return:231                         flags.append(\"IsReturn\")232                         flags.append(\"IsUncondControl\")When the macroop definition is automatically generated, it checks those two flags and set IsSerializeBefore to the first microop and IsSerializeAfter to the last microop consisting of the macroop.Rename registers and pass the renamed instruction to the next stageAfter handling serialization instruction, it should rename registers of the instruction. 755         renameSrcRegs(inst, inst-&gt;threadNumber); 756  757         renameDestRegs(inst, inst-&gt;threadNumber); 758  759         if (inst-&gt;isAtomic() || inst-&gt;isStore()) { 760             storesInProgress[tid]++; 761         } else if (inst-&gt;isLoad()) { 762             loadsInProgress[tid]++; 763         } 764  765         ++renamed_insts; 766         // Notify potential listeners that source and destination registers for 767         // this instruction have been renamed. 768         ppRename-&gt;notify(inst); 769  770         // Put instruction in rename queue. 771         toIEW-&gt;insts[toIEWIndex] = inst; 772         ++(toIEW-&gt;size); 773  774         // Increment which instruction we're on. 775         ++toIEWIndex; 776  777         // Decrement how many instructions are available. 778         --insts_available; 779     }renameSrcRegs1064 template &lt;class Impl&gt;1065 inline void1066 DefaultRename&lt;Impl&gt;::renameSrcRegs(const DynInstPtr &amp;inst, ThreadID tid)1067 {1068     ThreadContext *tc = inst-&gt;tcBase();1069     RenameMap *map = renameMap[tid];1070     unsigned num_src_regs = inst-&gt;numSrcRegs();1071 1072     // Get the architectual register numbers from the source and1073     // operands, and redirect them to the right physical register.1074     for (int src_idx = 0; src_idx &lt; num_src_regs; src_idx++) {1075         const RegId&amp; src_reg = inst-&gt;srcRegIdx(src_idx);1076         PhysRegIdPtr renamed_reg;1077 1078         renamed_reg = map-&gt;lookup(tc-&gt;flattenRegId(src_reg));1079         switch (src_reg.classValue()) {1080           case IntRegClass:1081             intRenameLookups++;1082             break;1083           case FloatRegClass:1084             fpRenameLookups++;1085             break;1086           case VecRegClass:1087           case VecElemClass:1088             vecRenameLookups++;1089             break;1090           case VecPredRegClass:1091             vecPredRenameLookups++;1092             break;1093           case CCRegClass:1094           case MiscRegClass:1095             break;1096 1097           default:1098             panic(\"Invalid register class: %d.\", src_reg.classValue());1099         }1100 1101         DPRINTF(Rename,1102                 \"[tid:%i] \"1103                 \"Looking up %s arch reg %i, got phys reg %i (%s)\\n\",1104                 tid, src_reg.className(),1105                 src_reg.index(), renamed_reg-&gt;index(),1106                 renamed_reg-&gt;className());1107 1108         inst-&gt;renameSrcReg(src_idx, renamed_reg);1109 1110         // See if the register is ready or not.1111         if (scoreboard-&gt;getReg(renamed_reg)) {1112             DPRINTF(Rename,1113                     \"[tid:%i] \"1114                     \"Register %d (flat: %d) (%s) is ready.\\n\",1115                     tid, renamed_reg-&gt;index(), renamed_reg-&gt;flatIndex(),1116                     renamed_reg-&gt;className());1117 1118             inst-&gt;markSrcRegReady(src_idx);1119         } else {1120             DPRINTF(Rename,1121                     \"[tid:%i] \"1122                     \"Register %d (flat: %d) (%s) is not ready.\\n\",1123                     tid, renamed_reg-&gt;index(), renamed_reg-&gt;flatIndex(),1124                     renamed_reg-&gt;className());1125         }1126 1127         ++renameRenameLookups;1128     }1129 }The main operation of the renameSrcRegs is to look up register map and find out if the architecture registers used as the current instruction’s source have been renamed to the another physical registers. If it has been renamed to other physical registers,it should consider those registers instead of the architectural registers in the rest of the rename stages. This mapping will be stored in the instructionthrough the renameSrcReg function (Line 1108).lookup renameMap to find physical register if it has been renamed 66 class SimpleRenameMap 67 {......122     /**123      * Look up the physical register mapped to an architectural register.124      * @param arch_reg The architectural register to look up.125      * @return The physical register it is currently mapped to.126      */127     PhysRegIdPtr lookup(const RegId&amp; arch_reg) const128     {129         assert(arch_reg.flatIndex() &lt;= map.size());130         return map[arch_reg.flatIndex()];131     }......170 class UnifiedRenameMap171 {......261     /**262      * Look up the physical register mapped to an architectural register.263      * This version takes a flattened architectural register id264      * and calls the appropriate class-specific rename table.265      * @param arch_reg The architectural register to look up.266      * @return The physical register it is currently mapped to.267      */268     PhysRegIdPtr lookup(const RegId&amp; arch_reg) const269     {270         switch (arch_reg.classValue()) {271           case IntRegClass:272             return intMap.lookup(arch_reg);273 274           case FloatRegClass:275             return  floatMap.lookup(arch_reg);276 277           case VecRegClass:278             assert(vecMode == Enums::Full);279             return  vecMap.lookup(arch_reg);280 281           case VecElemClass:282             assert(vecMode == Enums::Elem);283             return  vecElemMap.lookup(arch_reg);284 285           case VecPredRegClass:286             return predMap.lookup(arch_reg);287 288           case CCRegClass:289             return ccMap.lookup(arch_reg);290 291           case MiscRegClass:292             // misc regs aren't really renamed, they keep the same293             // mapping throughout the execution.294             return regFile-&gt;getMiscRegId(arch_reg.flatIndex());295 296           default:297             panic(\"rename lookup(): unknown reg class %s\\n\",298                   arch_reg.className());299         }300     }If it has been renamed already, the lookup function returns actual physical register to which the architecture register has been mapped.The map used in the rename stage is UnifiedRenameMap and contains multiple SimpleRenameMap with the various register type.Therefore, it first invokes the lookup function of the UnifiedRenameMap,and further invokes the lookup function of the SimpleRenameMap depending on the register type that we are trying to rename.check scoreboardAfter the lookup, it checks the scoreboard if the target registers are available to be read.Note that the renamed register is passed to the scoreboard, getReg.O3 is out-of-order processor and renames the registersto eliminate register dependency such as write after read.The scoreboard let the processor know when the register is ready to be accessed. Particularly, the getReg interface of the scoreboard can check whether specific phyiscal register is currently available.If it returns true, it means that the asked register is ready to be used. It invokes the markSrcRegReady function (Line 1118)to mark that operand of instruction is ready to be used. Also, it sets the instruction to be issued when all operands of that instructions are ready.However, if the getReg returns false, it means that one source register is not available at that cycle, so it should not set the flag and make the instruction to wait until the register is ready. The scoreboard and its interfaces will be described in the below section.renameDestRegs1131 template &lt;class Impl&gt;1132 inline void1133 DefaultRename&lt;Impl&gt;::renameDestRegs(const DynInstPtr &amp;inst, ThreadID tid)1134 {1135     ThreadContext *tc = inst-&gt;tcBase();1136     RenameMap *map = renameMap[tid];1137     unsigned num_dest_regs = inst-&gt;numDestRegs();1138 1139     // Rename the destination registers.1140     for (int dest_idx = 0; dest_idx &lt; num_dest_regs; dest_idx++) {1141         const RegId&amp; dest_reg = inst-&gt;destRegIdx(dest_idx);1142         typename RenameMap::RenameInfo rename_result;1143 1144         RegId flat_dest_regid = tc-&gt;flattenRegId(dest_reg);1145         flat_dest_regid.setNumPinnedWrites(dest_reg.getNumPinnedWrites());1146 1147         rename_result = map-&gt;rename(flat_dest_regid);1148 1149         inst-&gt;flattenDestReg(dest_idx, flat_dest_regid);1150 1151         scoreboard-&gt;unsetReg(rename_result.first);1152 1153         DPRINTF(Rename,1154                 \"[tid:%i] \"1155                 \"Renaming arch reg %i (%s) to physical reg %i (%i).\\n\",1156                 tid, dest_reg.index(), dest_reg.className(),1157                 rename_result.first-&gt;index(),1158                 rename_result.first-&gt;flatIndex());1159 1160         // Record the rename information so that a history can be kept.1161         RenameHistory hb_entry(inst-&gt;seqNum, flat_dest_regid,1162                                rename_result.first,1163                                rename_result.second);1164 1165         historyBuffer[tid].push_front(hb_entry);1166 1167         DPRINTF(Rename, \"[tid:%i] [sn:%llu] \"1168                 \"Adding instruction to history buffer (size=%i).\\n\",1169                 tid,(*historyBuffer[tid].begin()).instSeqNum,1170                 historyBuffer[tid].size());1171 1172         // Tell the instruction to rename the appropriate destination1173         // register (dest_idx) to the new physical register1174         // (rename_result.first), and record the previous physical1175         // register that the same logical register was renamed to1176         // (rename_result.second).1177         inst-&gt;renameDestReg(dest_idx,1178                             rename_result.first,1179                             rename_result.second);1180 1181         ++renameRenamedOperands;1182     }1183 }Basically, the renameDestRegs function is similar to the renameSrcRegs However, renaming destination register can change register map and the scoreboardof the destination register. For the source registers,it should check the scoreboardbecause it might have dependencies on the previous instructions.However, because the destination register does not have dependencies onprevious instructions, it can map current destination register to any physical register if the resources are available. Therefore, instead of invoking lookup as we did for the source register,it invokes rename function.rename: renames specific register to the other221     /**222      * Tell rename map to get a new free physical register to remap223      * the specified architectural register. This version takes a224      * RegId and reads the  appropriate class-specific rename table.225      * @param arch_reg The architectural register id to remap.226      * @return A RenameInfo pair indicating both the new and previous227      * physical registers.228      */229     RenameInfo rename(const RegId&amp; arch_reg)230     {231         switch (arch_reg.classValue()) {232           case IntRegClass:233             return intMap.rename(arch_reg);234           case FloatRegClass:235             return floatMap.rename(arch_reg);236           case VecRegClass:237             assert(vecMode == Enums::Full);238             return vecMap.rename(arch_reg);239           case VecElemClass:240             assert(vecMode == Enums::Elem);241             return vecElemMap.rename(arch_reg);242           case VecPredRegClass:243             return predMap.rename(arch_reg);244           case CCRegClass:245             return ccMap.rename(arch_reg);246           case MiscRegClass:247             {248             // misc regs aren't really renamed, just remapped249             PhysRegIdPtr phys_reg = lookup(arch_reg);250             // Set the new register to the previous one to keep the same251             // mapping throughout the execution.252             return RenameInfo(phys_reg, phys_reg);253             }254 255           default:256             panic(\"rename rename(): unknown reg class %s\\n\",257                   arch_reg.className());258         }259     }Because rename stage has access on various physical registers, it should ask proper physical register map to rename the architecture register to its physical register. Note that the return value, RenameInfo,is a pair indicating both the new and previous.    /**     * Pair of a physical register and a physical register.  Used to     * return the physical register that a logical register has been     * renamed to, and the previous physical register that the same     * logical register was previously mapped to.     */    typedef std::pair&lt;PhysRegIdPtr, PhysRegIdPtr&gt; RenameInfo; 73 SimpleRenameMap::RenameInfo 74 SimpleRenameMap::rename(const RegId&amp; arch_reg) 75 { 76     PhysRegIdPtr renamed_reg; 77     // Record the current physical register that is renamed to the 78     // requested architected register. 79     PhysRegIdPtr prev_reg = map[arch_reg.flatIndex()]; 80  81     if (arch_reg == zeroReg) { 82         assert(prev_reg-&gt;isZeroReg()); 83         renamed_reg = prev_reg; 84     } else if (prev_reg-&gt;getNumPinnedWrites() &gt; 0) { 85         // Do not rename if the register is pinned 86         assert(arch_reg.getNumPinnedWrites() == 0);  // Prevent pinning the 87                                                      // same register twice 88         DPRINTF(Rename, \"Renaming pinned reg, numPinnedWrites %d\\n\", 89                 prev_reg-&gt;getNumPinnedWrites()); 90         renamed_reg = prev_reg; 91         renamed_reg-&gt;decrNumPinnedWrites(); 92     } else { 93         renamed_reg = freeList-&gt;getReg(); 94         map[arch_reg.flatIndex()] = renamed_reg; 95         renamed_reg-&gt;setNumPinnedWrites(arch_reg.getNumPinnedWrites()); 96         renamed_reg-&gt;setNumPinnedWritesToComplete( 97             arch_reg.getNumPinnedWrites() + 1); 98     } 99 100     DPRINTF(Rename, \"Renamed reg %d to physical reg %d (%d) old mapping was\"101             \" %d (%d)\\n\",102             arch_reg, renamed_reg-&gt;flatIndex(), renamed_reg-&gt;flatIndex(),103             prev_reg-&gt;flatIndex(), prev_reg-&gt;flatIndex());104 105     return RenameInfo(renamed_reg, prev_reg);106 }The prev_reg indicates the previous physical register mapped to the current architecture register.Also, it maps any available physical register to the current architecture register (Line 93)and update its mapping (Line 94).Make the renamed register as not readyIt invokes unsetReg to make the scoreboard mark the physical register previously mapped to currently remapped destination architecture registeras not ready.\\TODO{What is the purpose of it?}Populating history buffer entry per destination register renameAfter the renaming is done (line 1161-1163),it generates history entry for providing \\TODO{is it for precise exception?? what purpose is it?}303     struct RenameHistory {304         RenameHistory(InstSeqNum _instSeqNum, const RegId&amp; _archReg,305                       PhysRegIdPtr _newPhysReg,306                       PhysRegIdPtr _prevPhysReg)307             : instSeqNum(_instSeqNum), archReg(_archReg),308               newPhysReg(_newPhysReg), prevPhysReg(_prevPhysReg)309         {310         }311 312         /** The sequence number of the instruction that renamed. */313         InstSeqNum instSeqNum;314         /** The architectural register index that was renamed. */315         RegId archReg;316         /** The new physical register that the arch. register is renamed to. */317         PhysRegIdPtr newPhysReg;318         /** The old physical register that the arch. register was renamed to.319          */320         PhysRegIdPtr prevPhysReg;321     };                       322 323     /** A per-thread list of all destination register renames, used to either324      * undo rename mappings or free old physical registers.325      */326     std::list&lt;RenameHistory&gt; historyBuffer[Impl::MaxThreads];End of the main loopAfter renaming destination and source registers, it pushes the renamed instruction to the toIEW register. 781     instsInProgress[tid] += renamed_insts; 782     renameRenamedInsts += renamed_insts; 783  784     // If we wrote to the time buffer, record this. 785     if (toIEWIndex) { 786         wroteToTimeBuffer = true; 787     } 788  789     // Check if there's any instructions left that haven't yet been renamed. 790     // If so then block. 791     if (insts_available) { 792         blockThisCycle = true; 793     } 794  795     if (blockThisCycle) { 796         block(tid); 797         toDecode-&gt;renameUnblock[tid] = false; 798     } 799 }Now we are good to go to IEW stage!scoreboardscoreboard interfacegem5/src/cpu/o3/scoreboard.hh 46 /** 47  * Implements a simple scoreboard to track which registers are 48  * ready. This class operates on the unified physical register space, 49  * because the different classes of registers do not need to be distinguished. 50  * Registers being part of a fixed mapping are always considered ready. 51  */ 52 class Scoreboard 53 { 54   private: 55     /** The object name, for DPRINTF.  We have to declare this 56      *  explicitly because Scoreboard is not a SimObject. */ 57     const std::string _name; 58  59     /** Scoreboard of physical integer registers, saying whether or not they 60      *  are ready. */ 61     std::vector&lt;bool&gt; regScoreBoard; 62  63     /** The number of actual physical registers */ 64     unsigned M5_CLASS_VAR_USED numPhysRegs; 65  66   public: 67     /** Constructs a scoreboard. 68      *  @param _numPhysicalRegs Number of physical registers. 69      *  @param _numMiscRegs Number of miscellaneous registers. 70      */ 71     Scoreboard(const std::string &amp;_my_name, 72                unsigned _numPhysicalRegs); 73  74     /** Destructor. */ 75     ~Scoreboard() {} 76  77     /** Returns the name of the scoreboard. */ 78     std::string name() const { return _name; }; 79  80     /** Checks if the register is ready. */ 81     bool getReg(PhysRegIdPtr phys_reg) const 82     { 83         assert(phys_reg-&gt;flatIndex() &lt; numPhysRegs); 84  85         if (phys_reg-&gt;isFixedMapping()) { 86             // Fixed mapping regs are always ready 87             return true; 88         } 89  90         bool ready = regScoreBoard[phys_reg-&gt;flatIndex()]; 91  92         if (phys_reg-&gt;isZeroReg()) 93             assert(ready); 94  95         return ready; 96     } 97  98     /** Sets the register as ready. */ 99     void setReg(PhysRegIdPtr phys_reg)100     {101         assert(phys_reg-&gt;flatIndex() &lt; numPhysRegs);102 103         if (phys_reg-&gt;isFixedMapping()) {104             // Fixed mapping regs are always ready, ignore attempts to change105             // that106             return;107         }108 109         DPRINTF(Scoreboard, \"Setting reg %i (%s) as ready\\n\",110                 phys_reg-&gt;index(), phys_reg-&gt;className());111 112         regScoreBoard[phys_reg-&gt;flatIndex()] = true;113     }114 115     /** Sets the register as not ready. */116     void unsetReg(PhysRegIdPtr phys_reg)117     {118         assert(phys_reg-&gt;flatIndex() &lt; numPhysRegs);119 120         if (phys_reg-&gt;isFixedMapping()) {121             // Fixed mapping regs are always ready, ignore attempts to122             // change that123             return;124         }125 126         // zero reg should never be marked unready127         if (phys_reg-&gt;isZeroReg())128             return;129 130         regScoreBoard[phys_reg-&gt;flatIndex()] = false;131     }132 133 };Scoreboard is implemented as a simple vector (regScoreBoard) to indicate specific register is ready to be used or not. And it provide three interfaces to set or get the status of the specific register maintained by the scoreboard.scoreboard used by the O3 CPUgem5/src/cpu/o3/cpu.hh602     /** Integer Register Scoreboard */603     Scoreboard scoreboard; gem5/src/cpu/o3/cpu.cc 218     rename.setScoreboard(&amp;scoreboard); 219     iew.setScoreboard(&amp;scoreboard);When it comes to real hardware implementation, the scoreboard should be accessible by the multiple stages at the same time. However, because it is software emulation,GEM5 doesn’t provide port-wise emulation to service different modules at the same time.Note that GEM5 executes in single thread and cannot be executed in multi-threads. Anyway, the scoreboard is accessed by two different stages in the O3CPU: rename and iew."
  },
  
  {
    "title": "O3 Cpu Decode",
    "url": "/posts/O3-CPU-Decode/",
    "categories": "GEM5, Pipeline, O3",
    "tags": "",
    "date": "2021-05-28 00:00:00 -0400",
    





    
    "snippet": "Sending fetched instructions to decode stagegem5/src/cpu/o3/fetch_impl.hh 961  962     // Pick a random thread to start trying to grab instructions from 963     auto tid_itr = activeThreads-&gt;beg...",
    "content": "Sending fetched instructions to decode stagegem5/src/cpu/o3/fetch_impl.hh 961  962     // Pick a random thread to start trying to grab instructions from 963     auto tid_itr = activeThreads-&gt;begin(); 964     std::advance(tid_itr, random_mt.random&lt;uint8_t&gt;(0, activeThreads-&gt;size() - 1)); 965  966     while (available_insts != 0 &amp;&amp; insts_to_decode &lt; decodeWidth) { 967         ThreadID tid = *tid_itr; 968         if (!stalls[tid].decode &amp;&amp; !fetchQueue[tid].empty()) { 969             const auto&amp; inst = fetchQueue[tid].front(); 970             toDecode-&gt;insts[toDecode-&gt;size++] = inst; 971             DPRINTF(Fetch, \"[tid:%i] [sn:%llu] Sending instruction to decode \" 972                     \"from fetch queue. Fetch queue size: %i.\\n\", 973                     tid, inst-&gt;seqNum, fetchQueue[tid].size()); 974  975             wroteToTimeBuffer = true; 976             fetchQueue[tid].pop_front(); 977             insts_to_decode++; 978             available_insts--; 979         } 980  981         tid_itr++; 982         // Wrap around if at end of active threads list 983         if (tid_itr == activeThreads-&gt;end()) 984             tid_itr = activeThreads-&gt;begin(); 985     } 986  987     // If there was activity this cycle, inform the CPU of it. 988     if (wroteToTimeBuffer) { 989         DPRINTF(Activity, \"Activity this cycle.\\n\"); 990         cpu-&gt;activityThisCycle(); 991     } 992  993     // Reset the number of the instruction we've fetched. 994     numInst = 0; 995 }   //end of the fetch.tickThe last job of the fetch stage is passing the fetched instructionsto the next stage, decode stage. On the above code, toDecode member field of the fetch is used as an storage located in between the fetch and decode stage.FetchStruct: passing fetch stage’s information to decode stagegem5/src/cpu/o3/fetch.hh431     //Might be annoying how this name is different than the queue.432     /** Wire used to write any information heading to decode. */433     typename TimeBuffer&lt;FetchStruct&gt;::wire toDecode;The toDecode is declared as a wire class defined in the TimeBuffer class. Also, because the TimeBuffer is a template class, it passes the FetchStruct that contains all fetch stage’s informationrequired by the decode stage. Let’s take a look at the FetchStruct to understand which information is passed to the decode stage.gem5/src/cpu/o3/cpu_policy.hh 60 template&lt;class Impl&gt; 61 struct SimpleCPUPolicy 62 { ...... 89     /** The struct for communication between fetch and decode. */ 90     typedef DefaultFetchDefaultDecode&lt;Impl&gt; FetchStruct; 91  92     /** The struct for communication between decode and rename. */ 93     typedef DefaultDecodeDefaultRename&lt;Impl&gt; DecodeStruct; 94  95     /** The struct for communication between rename and IEW. */ 96     typedef DefaultRenameDefaultIEW&lt;Impl&gt; RenameStruct; 97  98     /** The struct for communication between IEW and commit. */ 99     typedef DefaultIEWDefaultCommit&lt;Impl&gt; IEWStruct;100 101     /** The struct for communication within the IEW stage. */102     typedef ::IssueStruct&lt;Impl&gt; IssueStruct;103 104     /** The struct for all backwards communication. */105     typedef TimeBufStruct&lt;Impl&gt; TimeStruct;gem5/src/cpu/o3/comm.h 55 /** Struct that defines the information passed from fetch to decode. */ 56 template&lt;class Impl&gt; 57 struct DefaultFetchDefaultDecode { 58     typedef typename Impl::DynInstPtr DynInstPtr; 59  60     int size; 61  62     DynInstPtr insts[Impl::MaxWidth]; 63     Fault fetchFault; 64     InstSeqNum fetchFaultSN; 65     bool clearFetchFault; 66 };As shown in the above code, it passes the instructions fetched from the Icache. Then how this information is passed to the decode stage?The answer is the TimeBuffer!TimeBuffer and wire sending the data between two stagesIn actual hardware implementation, the register should be placed in between the two pipeline stages to share the informationprocessed by the previous stage to the next stage. For that purpose, GEM5 utilize the TimeBuffer and Wire classes.TimeBuffer implementation and usageTimeBuffer is implemented as a template class designed to pass any information between two different stages. Also, it emulates actual behavior of registers.Therefore, at every clock tick, the TimeBuffer is advanced and points to different content of the registers.Constructor and Desctructor of the TimeBuffer 39 template &lt;class T&gt; 40 class TimeBuffer 41 { 42   protected: 43     int past; 44     int future; 45     unsigned size; 46     int _id; 47  48     char *data; 49     std::vector&lt;char *&gt; index; 50     unsigned base; 51  52     void valid(int idx) const 53     { 54         assert (idx &gt;= -past &amp;&amp; idx &lt;= future); 55     }......139   public:140     TimeBuffer(int p, int f)141         : past(p), future(f), size(past + future + 1),142           data(new char[size * sizeof(T)]), index(size), base(0)143     {   144         assert(past &gt;= 0 &amp;&amp; future &gt;= 0);145         char *ptr = data; 146         for (unsigned i = 0; i &lt; size; i++) {147             index[i] = ptr;148             std::memset(ptr, 0, sizeof(T));149             new (ptr) T;150             ptr += sizeof(T);151         }152         153         _id = -1;154     }155 156     TimeBuffer()157         : data(NULL)158     {159     }160 161     ~TimeBuffer()162     {163         for (unsigned i = 0; i &lt; size; ++i)164             (reinterpret_cast&lt;T *&gt;(index[i]))-&gt;~T();165         delete [] data;166     }Because the TimeBuffer needs to allocate and deallocate new class object at every clock cycle, it’s constructor is designed to utilize a preallocated memory called data member field. With the help of placement new, its constructor can initialize new object at specific location, index vector. As shown in its constructor, it populates T typed object, size times on the data array. It makes the index vector point to the allocated objects. At its desctructor, it deletes the data array and every objectspointed to by the index vector.advance TimeBuffer 542     //Tick each of the stages 543     fetch.tick(); 544  545     decode.tick(); 546  547     rename.tick(); 548  549     iew.tick(); 550  551     commit.tick(); 552  553     // Now advance the time buffers 554     timeBuffer.advance(); 555  556     fetchQueue.advance(); 557     decodeQueue.advance(); 558     renameQueue.advance(); 559     iewQueue.advance(); 560  561     activityRec.advance();The most important function of the TimeBuffer is the advance.This function is invoked at every clock cycle of the processor to advance the TimeBuffer. Let’s take a look at how the advance function emulates next clock tick.178     void179     advance()180     {181         if (++base &gt;= size)182             base = 0;183 184         int ptr = base + future;185         if (ptr &gt;= (int)size)186             ptr -= size;187         (reinterpret_cast&lt;T *&gt;(index[ptr]))-&gt;~T();188         std::memset(index[ptr], 0, sizeof(T));189         new (index[ptr]) T;190     }The base member field is initialized as zero at the construction and incremented at every clock cycle because the advance function is invoked at every clock cycle. Also, because it emulates circular storage, the base should be initialized as zerowhen it exceeds size (line 181-182). And the future is the fixed constant passed by the configuration python script.Therefore, after the first initialization with offset future, at every clock cycle, it allocates new object typed T. Before populating new object, it first invoke deconstructor (line 188) and initiate new object with the placement new (line 189).WireExample motivating interaction between fetch and decodegem5/src/cpu/o3/cpu.cc 182     // Also setup each of the stages' queues. 183     fetch.setFetchQueue(&amp;fetchQueue); 184     decode.setFetchQueue(&amp;fetchQueue);gem5/src/cpu/o3/fetch_impl.hh 312 template&lt;class Impl&gt; 313 void 314 DefaultFetch&lt;Impl&gt;::setFetchQueue(TimeBuffer&lt;FetchStruct&gt; *ftb_ptr) 315 { 316     // Create wire to write information to proper place in fetch time buf. 317     toDecode = ftb_ptr-&gt;getWire(0); 318 }gem5/src/cpu/o3/decode_impl.hh195 template&lt;class Impl&gt;196 void197 DefaultDecode&lt;Impl&gt;::setFetchQueue(TimeBuffer&lt;FetchStruct&gt; *fq_ptr)198 {199     fetchQueue = fq_ptr;200 201     // Setup wire to read information from fetch queue.202     fromFetch = fetchQueue-&gt;getWire(-fetchToDecodeDelay);203 }gem5/src/cpu/timebuf.hh234     wire getWire(int idx)235     {236         valid(idx);237 238         return wire(this, idx);239     }As shown in the above code, two different stages fetch and decode invoke setFetchQueue function with the same TimeBuffer, fetchQueue.However, note that those two invocations are serviced from different functions of each class. As shown in the above code, both function invokes getWire, but with different argument, 0 and -fetchToDecodeDelay respectively. The getWire function returns the wire object initialized with this and idx.Here this means the TimeBuffer itself and this will be assigned to the buffer member field of the wire object. Also, idx will be assigned to the index member field of the wire object.Because the index is a constant number and used to access the register managed by the buffer, it will generate fetchToDecodeDelay clock timing delays between the fetch and decode stage.Let’s see how this timing delay can be imposed on the register access in detail.Wire overloads the member reference operator to access the TimeBufferRemember that the wire has member field buffer which is the TimeBuffer that actually maintains all the register values that should be passed to the next stage. However, in general, the register is a flip-flop it cannot be read and writtenat the same cycle. Therefore, naturally, the next stage will get the data written to the register after n clock cycles are elapsed.This behavior of the register is emulated by the wire and TimeBuffer. 57   public: 58     friend class wire; 59     class wire 60     { 61         friend class TimeBuffer; 62       protected: 63         TimeBuffer&lt;T&gt; *buffer; 64         int index; 65  66         void set(int idx) 67         {    68             buffer-&gt;valid(idx); 69             index = idx; 70         } 71  72         wire(TimeBuffer&lt;T&gt; *buf, int i) 73             : buffer(buf), index(i) 74         { }......134         T &amp;operator*() const { return *buffer-&gt;access(index); }135         T *operator-&gt;() const { return buffer-&gt;access(index); }136     };When the wire is accessed by the -&gt; operator, it invokes access function of the TimeBuffer contained in the buffer member field. Also note that it passes the index argument set at the construction of the wire.192   protected:193     //Calculate the index into this-&gt;index for element at position idx194     //relative to now195     inline int calculateVectorIndex(int idx) const196     {197         //Need more complex math here to calculate index.198         valid(idx);199 200         int vector_index = idx + base;201         if (vector_index &gt;= (int)size) {202             vector_index -= size;203         } else if (vector_index &lt; 0) {204             vector_index += size;205         }206 207         return vector_index;208     }209 210   public:211     T *access(int idx)212     {213         int vector_index = calculateVectorIndex(idx);214 215         return reinterpret_cast&lt;T *&gt;(index[vector_index]);216     }When the access is invoked, it first calculates the index for the vector. Note that it adds two variable idx and base. The base member field is increased by 1 every clock cycle as we’ve seen in the advance function before. the idx field is passed from the wire class that embeds the TimeBuffer. For example, it is 0 and -1 for the fetch and decode stage respectively. Therefore, in this settings, the decode stage will access the register set by the previous clock cycle by the fetch stage. Therefore, by setting the index field of the wire at its initialization properly, we can set the delays of register access in two different stages.Decode stage pipeline analysisgem5/src/cpu/o3/decode_impl.hhtick of the decode stage567 template&lt;class Impl&gt;568 void569 DefaultDecode&lt;Impl&gt;::tick()570 {571     wroteToTimeBuffer = false;572 573     bool status_change = false;574 575     toRenameIndex = 0;576 577     list&lt;ThreadID&gt;::iterator threads = activeThreads-&gt;begin();578     list&lt;ThreadID&gt;::iterator end = activeThreads-&gt;end();579 580     sortInsts();581 582     //Check stall and squash signals.583     while (threads != end) {584         ThreadID tid = *threads++;585 586         DPRINTF(Decode,\"Processing [tid:%i]\\n\",tid);587         status_change =  checkSignalsAndUpdate(tid) || status_change;588 589         decode(status_change, tid);590     }591 592     if (status_change) {593         updateStatus();594     }595 596     if (wroteToTimeBuffer) {597         DPRINTF(Activity, \"Activity this cycle.\\n\");598 599         cpu-&gt;activityThisCycle();600     }601 }As we’ve seen before, the tick function of each stage is the most important functionbecause it is executed every core clock cycle. The tick function consists of three important functions: sortInsts, checkSignalsAndUpdate and decodesortInstsAt the end of the decode stage, it pushes the fetched instructions to the toDecode register buffers. Therefore, the decode stage should fetch those instructionsfrom the same register located in between the fetch and decode stage.483 template &lt;class Impl&gt;484 void485 DefaultDecode&lt;Impl&gt;::sortInsts()486 {487     int insts_from_fetch = fromFetch-&gt;size;488     for (int i = 0; i &lt; insts_from_fetch; ++i) {489         insts[fromFetch-&gt;insts[i]-&gt;threadNumber].push(fromFetch-&gt;insts[i]);490     }491 }   The sortInsts extracts the instructions stored in the register (fromFetch) and save them in the local instruction buffer (insts). Note that the register changes every tick, so each stage should copy and paste the register datato its local memory to process.checkSignalsAndUpdate507 template &lt;class Impl&gt;508 bool509 DefaultDecode&lt;Impl&gt;::checkSignalsAndUpdate(ThreadID tid)510 {511     // Check if there's a squash signal, squash if there is.512     // Check stall signals, block if necessary.513     // If status was blocked514     //     Check if stall conditions have passed515     //         if so then go to unblocking516     // If status was Squashing517     //     check if squashing is not high.  Switch to running this cycle.518519     // Update the per thread stall statuses.520     readStallSignals(tid);521522     // Check squash signals from commit.523     if (fromCommit-&gt;commitInfo[tid].squash) {524525         DPRINTF(Decode, \"[tid:%i] Squashing instructions due to squash \"526                 \"from commit.\\n\", tid);527528         squash(tid);529530         return true;531     }532533     if (checkStall(tid)) {534         return block(tid);535     }Before executing the decode function, it should first check whether the other stages has sent a signal to stall.readStallSignals493 template&lt;class Impl&gt;494 void495 DefaultDecode&lt;Impl&gt;::readStallSignals(ThreadID tid)496 {497     if (fromRename-&gt;renameBlock[tid]) {498         stalls[tid].rename = true;499     }500 501     if (fromRename-&gt;renameUnblock[tid]) {502         assert(stalls[tid].rename);503         stalls[tid].rename = false;504     }505 }Rename stage can send two signals to the decode stage, block signal and unblock signal through the fromRename wire.Based on the signal sent from the rename stage, it sets or unset an associated entry of the member field stalls.When stall, just block the decode and return234 template&lt;class Impl&gt;235 bool236 DefaultDecode&lt;Impl&gt;::checkStall(ThreadID tid) const237 {238     bool ret_val = false;239 240     if (stalls[tid].rename) {241         DPRINTF(Decode,\"[tid:%i] Stall fom Rename stage detected.\\n\", tid);242         ret_val = true;243     }244 245     return ret_val;246 }When the decode stage has received the stall signal, it returns true, which results in invoking block function and returning is result.255 template&lt;class Impl&gt;256 bool257 DefaultDecode&lt;Impl&gt;::block(ThreadID tid)258 {259     DPRINTF(Decode, \"[tid:%i] Blocking.\\n\", tid);260 261     // Add the current inputs to the skid buffer so they can be262     // reprocessed when this stage unblocks.263     skidInsert(tid);264 265     // If the decode status is blocked or unblocking then decode has not yet266     // signalled fetch to unblock. In that case, there is no need to tell267     // fetch to block.268     if (decodeStatus[tid] != Blocked) {269         // Set the status to Blocked.270         decodeStatus[tid] = Blocked;271 272         if (toFetch-&gt;decodeUnblock[tid]) {273             toFetch-&gt;decodeUnblock[tid] = false;274         } else {275             toFetch-&gt;decodeBlock[tid] = true;276             wroteToTimeBuffer = true;277         }278 279         return true;280     }281 282     return false;283 }When the decode stage has instruction to be processeddelivered from the fetch stage,it needs to be maintained in the skid buffer so that they can be reprocessed when the decode stage is unblocked.Note that different pipelines can still works even though the decode pipeline is blocked,and the input can continuously arrive to the decode stage.squash pipeline when the commit stage sent squash signalAfter reading the stall signal, it should also check whether the commit stage has sent a squash signal.The decode stage can check whether it needs to squash by checking the fromCommit wire.304 template&lt;class Impl&gt;305 void306 DefaultDecode&lt;Impl&gt;::squash(const DynInstPtr &amp;inst, ThreadID tid)307 {308     DPRINTF(Decode, \"[tid:%i] [sn:%llu] Squashing due to incorrect branch \"309             \"prediction detected at decode.\\n\", tid, inst-&gt;seqNum);310311     // Send back mispredict information.312     toFetch-&gt;decodeInfo[tid].branchMispredict = true;313     toFetch-&gt;decodeInfo[tid].predIncorrect = true;314     toFetch-&gt;decodeInfo[tid].mispredictInst = inst;315     toFetch-&gt;decodeInfo[tid].squash = true;316     toFetch-&gt;decodeInfo[tid].doneSeqNum = inst-&gt;seqNum;317     toFetch-&gt;decodeInfo[tid].nextPC = inst-&gt;branchTarget();318     toFetch-&gt;decodeInfo[tid].branchTaken = inst-&gt;pcState().branching();319     toFetch-&gt;decodeInfo[tid].squashInst = inst;320     if (toFetch-&gt;decodeInfo[tid].mispredictInst-&gt;isUncondCtrl()) {321             toFetch-&gt;decodeInfo[tid].branchTaken = true;322     }323324     InstSeqNum squash_seq_num = inst-&gt;seqNum;325326     // Might have to tell fetch to unblock.327     if (decodeStatus[tid] == Blocked ||328         decodeStatus[tid] == Unblocking) {329         toFetch-&gt;decodeUnblock[tid] = 1;330     }331332     // Set status to squashing.333     decodeStatus[tid] = Squashing;334335     for (int i=0; i&lt;fromFetch-&gt;size; i++) {336         if (fromFetch-&gt;insts[i]-&gt;threadNumber == tid &amp;&amp;337             fromFetch-&gt;insts[i]-&gt;seqNum &gt; squash_seq_num) {338             fromFetch-&gt;insts[i]-&gt;setSquashed();339         }340     }341342     // Clear the instruction list and skid buffer in case they have any343     // insts in them.344     while (!insts[tid].empty()) {345         insts[tid].pop();346     }347348     while (!skidBuffer[tid].empty()) {349         skidBuffer[tid].pop();350     }351352     // Squash instructions up until this one353     cpu-&gt;removeInstsUntil(squash_seq_num, tid);354 }Note that squash signal incurs complex operations compared to stalls.When the stall signal is received, the decode stage just waits until the stall signal is removed, receiving the unblock signal. However, when the stall signal is received, it should clear out the pipeline and associated data structures.When the decode stage finishes blocking and squashing operation508 bool509 DefaultDecode&lt;Impl&gt;::checkSignalsAndUpdate(ThreadID tid)510 {......537     if (decodeStatus[tid] == Blocked) {538         DPRINTF(Decode, \"[tid:%i] Done blocking, switching to unblocking.\\n\",539                 tid);540541         decodeStatus[tid] = Unblocking;542543         unblock(tid);544545         return true;546     }547548     if (decodeStatus[tid] == Squashing) {549         // Switch status to running if decode isn't being told to block or550         // squash this cycle.551         DPRINTF(Decode, \"[tid:%i] Done squashing, switching to running.\\n\",552                 tid);553554         decodeStatus[tid] = Running;555556         return false;557     }558559     // If we've reached this point, we have not gotten any signals that560     // cause decode to change its status.  Decode remains the same as before.561     return false;562 }After the decode stage is recovered from the stall or squashing. it needs to change the block or stall state to the Running state so that it canreceive the instructions to decode from the fetch stage. For the Blocked state, it will execute the line 537-546 And when the squash signal is turned off from commit stage, it will execute the rest of the code (548-557).Why we need another decode even though we decoded?It would be confusing because we already finished instruction decoding in the fetch stage. We already know which instructions are located in the fetch buffer. Why we need another decode function?The decode stage does not do much, but it should check any PC-relative branches are correct.Most of the decode operations are actually done by the decodeInsts function.600 template601 void602 DefaultDecode::decode(bool &amp;status_change, ThreadID tid)603 {604     // If status is Running or idle,605     //     call decodeInsts()606     // If status is Unblocking,607     //     buffer any instructions coming from fetch608     //     continue trying to empty skid buffer609     //     check if stall conditions have passed610 611     if (decodeStatus[tid] == Blocked) {612         ++decodeBlockedCycles;613     } else if (decodeStatus[tid] == Squashing) {614         ++decodeSquashCycles;615     }616 617     // Decode should try to decode as many instructions as its bandwidth618     // will allow, as long as it is not currently blocked.619     if (decodeStatus[tid] == Running ||620         decodeStatus[tid] == Idle) {621         DPRINTF(Decode, \"[tid:%i] Not blocked, so attempting to run \"622                 \"stage.\\n\",tid);623 624         decodeInsts(tid);625     } else if (decodeStatus[tid] == Unblocking) {626         // Make sure that the skid buffer has something in it if the627         // status is unblocking.628         assert(!skidsEmpty());629 630         // If the status was unblocking, then instructions from the skid631         // buffer were used.  Remove those instructions and handle632         // the rest of unblocking.633         decodeInsts(tid);634 635         if (fetchInstsValid()) {636             // Add the current inputs to the skid buffer so they can be637             // reprocessed when this stage unblocks.638             skidInsert(tid);639         }640 641         status_change = unblock(tid) || status_change;642     }643 }decode stage check buffers to retrieve instruction to decode645 template &lt;class Impl&gt;646 void647 DefaultDecode&lt;Impl&gt;::decodeInsts(ThreadID tid)648 {649     // Instructions can come either from the skid buffer or the list of650     // instructions coming from fetch, depending on decode's status.651     int insts_available = decodeStatus[tid] == Unblocking ?652         skidBuffer[tid].size() : insts[tid].size();653 654     if (insts_available == 0) {655         DPRINTF(Decode, \"[tid:%i] Nothing to do, breaking out\"656                 \" early.\\n\",tid);657         // Should I change the status to idle?658         ++decodeIdleCycles;659         return;660     } else if (decodeStatus[tid] == Unblocking) {661         DPRINTF(Decode, \"[tid:%i] Unblocking, removing insts from skid \"662                 \"buffer.\\n\",tid);663         ++decodeUnblockCycles;664     } else if (decodeStatus[tid] == Running) {665         ++decodeRunCycles;666     }667 668     std::queue&lt;DynInstPtr&gt;669         &amp;insts_to_decode = decodeStatus[tid] == Unblocking ?670         skidBuffer[tid] : insts[tid];671 672     DPRINTF(Decode, \"[tid:%i] Sending instruction to rename.\\n\",tid);Note that the decodeInsts can be invokedin two different state of the decode stage. The Running and Unblocking. Running status means that decode stage continuously receive the packet from the fetch stage.However, the Unblocking stage means that it was blocked and was recovering,which means the packets are still in the skidBuffer.Therefore, it should decode instructionsstacked in the skidBufferwhile it has been blocked.forwarding decoded instructions to rename stage185 template&lt;class Impl&gt;186 void187 DefaultDecode&lt;Impl&gt;::setDecodeQueue(TimeBuffer&lt;DecodeStruct&gt; *dq_ptr)188 {189     decodeQueue = dq_ptr;190 191     // Setup wire to write information to proper place in decode queue.192     toRename = decodeQueue-&gt;getWire(0);193 }Similar to the toDecode wire in the fetch stage, decode stage needs a wire to send the decoded instructions to another register connected with the rename stage. For that purpose, it declares toRename wire.674     while (insts_available &gt; 0 &amp;&amp; toRenameIndex &lt; decodeWidth) {675         assert(!insts_to_decode.empty());676 677         DynInstPtr inst = std::move(insts_to_decode.front());678 679         insts_to_decode.pop();680 681         DPRINTF(Decode, \"[tid:%i] Processing instruction [sn:%lli] with \"682                 \"PC %s\\n\", tid, inst-&gt;seqNum, inst-&gt;pcState());683 684         if (inst-&gt;isSquashed()) {685             DPRINTF(Decode, \"[tid:%i] Instruction %i with PC %s is \"686                     \"squashed, skipping.\\n\",687                     tid, inst-&gt;seqNum, inst-&gt;pcState());688             689             ++decodeSquashedInsts;690             691             --insts_available;692             693             continue;694         }695 696         // Also check if instructions have no source registers.  Mark697         // them as ready to issue at any time.  Not sure if this check698         // should exist here or at a later stage; however it doesn't matter699         // too much for function correctness.700         if (inst-&gt;numSrcRegs() == 0) {701             inst-&gt;setCanIssue();702         }703 704         // This current instruction is valid, so add it into the decode705         // queue.  The next instruction may not be valid, so check to706         // see if branches were predicted correctly.707         toRename-&gt;insts[toRenameIndex] = inst;708 709         ++(toRename-&gt;size);710         ++toRenameIndex;711         ++decodeDecodedInsts;712         --insts_available;The while loop selects one instruction from the buffer and sends it to the rename stagethrough the toRename wire.720         // Ensure that if it was predicted as a branch, it really is a721         // branch.722         if (inst-&gt;readPredTaken() &amp;&amp; !inst-&gt;isControl()) {723             panic(\"Instruction predicted as a branch!\");724 725             ++decodeControlMispred;726 727             // Might want to set some sort of boolean and just do728             // a check at the end729             squash(inst, inst-&gt;threadNumber);730 731             break;732         }733 734         // Go ahead and compute any PC-relative branches.735         // This includes direct unconditional control and736         // direct conditional control that is predicted taken.737         if (inst-&gt;isDirectCtrl() &amp;&amp;738            (inst-&gt;isUncondCtrl() || inst-&gt;readPredTaken()))739         {740             ++decodeBranchResolved;741 742             if (!(inst-&gt;branchTarget() == inst-&gt;readPredTarg())) {743                 ++decodeBranchMispred;744 745                 // Might want to set some sort of boolean and just do746                 // a check at the end747                 squash(inst, inst-&gt;threadNumber);748                 TheISA::PCState target = inst-&gt;branchTarget();749 750                 DPRINTF(Decode,751                         \"[tid:%i] [sn:%llu] \"752                         \"Updating predictions: PredPC: %s\\n\",753                         tid, inst-&gt;seqNum, target);754                 //The micro pc after an instruction level branch should be 0755                 inst-&gt;setPredTarg(target);756                 break;757             }758         }759     } //end of the while loopOne thing to note is it really decodes the instruction and check whether the current instruction is really branch.If it was predicted as a branch,but turned out to be a non-branch instruction,then it should squash the current instruction.761     // If we didn't process all instructions, then we will need to block762     // and put all those instructions into the skid buffer.763     if (!insts_to_decode.empty()) {764         block(tid);765     }766 767     // Record that decode has written to the time buffer for activity768     // tracking.769     if (toRenameIndex) {770         wroteToTimeBuffer = true;771     }"
  },
  
  {
    "title": "O3 Cpu Fetch",
    "url": "/posts/O3-CPU-Fetch/",
    "categories": "GEM5, Pipeline, O3",
    "tags": "",
    "date": "2021-05-27 00:00:00 -0400",
    





    
    "snippet": "Fetch 895 template &lt;class Impl&gt; 896 void 897 DefaultFetch&lt;Impl&gt;::tick() 898 { 899     list&lt;ThreadID&gt;::iterator threads = activeThreads-&gt;begin(); 900     list&lt;ThreadID&gt;::i...",
    "content": "Fetch 895 template &lt;class Impl&gt; 896 void 897 DefaultFetch&lt;Impl&gt;::tick() 898 { 899     list&lt;ThreadID&gt;::iterator threads = activeThreads-&gt;begin(); 900     list&lt;ThreadID&gt;::iterator end = activeThreads-&gt;end(); 901     bool status_change = false; 902  903     wroteToTimeBuffer = false; 904  905     for (ThreadID i = 0; i &lt; numThreads; ++i) { 906         issuePipelinedIfetch[i] = false; 907     } 908  909     while (threads != end) { 910         ThreadID tid = *threads++; 911  912         // Check the signals for each thread to determine the proper status 913         // for each thread. 914         bool updated_status = checkSignalsAndUpdate(tid); 915         status_change =  status_change || updated_status; 916     } 917  918     DPRINTF(Fetch, \"Running stage.\\n\"); 919  920     if (FullSystem) { 921         if (fromCommit-&gt;commitInfo[0].interruptPending) { 922             interruptPending = true; 923         } 924  925         if (fromCommit-&gt;commitInfo[0].clearInterrupt) { 926             interruptPending = false; 927         } 928     } 929  930     for (threadFetched = 0; threadFetched &lt; numFetchingThreads; 931          threadFetched++) { 932         // Fetch each of the actively fetching threads. 933         fetch(status_change); 934     } 935  936     // Record number of instructions fetched this cycle for distribution. 937     fetchNisnDist.sample(numInst); 938  939     if (status_change) { 940         // Change the fetch stage status if there was a status change. 941         _status = updateFetchStatus(); 942     } 943  944     // Issue the next I-cache request if possible. 945     for (ThreadID i = 0; i &lt; numThreads; ++i) { 946         if (issuePipelinedIfetch[i]) { 947             pipelineIcacheAccesses(i); 948         } 949     } 950  951     // Send instructions enqueued into the fetch queue to decode. 952     // Limit rate by fetchWidth.  Stall if decode is stalled. 953     unsigned insts_to_decode = 0; 954     unsigned available_insts = 0; 955  956     for (auto tid : *activeThreads) { 957         if (!stalls[tid].decode) { 958             available_insts += fetchQueue[tid].size(); 959         } 960     } 961  962     // Pick a random thread to start trying to grab instructions from 963     auto tid_itr = activeThreads-&gt;begin(); 964     std::advance(tid_itr, random_mt.random&lt;uint8_t&gt;(0, activeThreads-&gt;size() - 1)); 965  966     while (available_insts != 0 &amp;&amp; insts_to_decode &lt; decodeWidth) { 967         ThreadID tid = *tid_itr; 968         if (!stalls[tid].decode &amp;&amp; !fetchQueue[tid].empty()) { 969             const auto&amp; inst = fetchQueue[tid].front(); 970             toDecode-&gt;insts[toDecode-&gt;size++] = inst; 971             DPRINTF(Fetch, \"[tid:%i] [sn:%llu] Sending instruction to decode \" 972                     \"from fetch queue. Fetch queue size: %i.\\n\", 973                     tid, inst-&gt;seqNum, fetchQueue[tid].size()); 974  975             wroteToTimeBuffer = true; 976             fetchQueue[tid].pop_front(); 977             insts_to_decode++; 978             available_insts--; 979         } 980  981         tid_itr++; 982         // Wrap around if at end of active threads list 983         if (tid_itr == activeThreads-&gt;end()) 984             tid_itr = activeThreads-&gt;begin(); 985     } 986  987     // If there was activity this cycle, inform the CPU of it. 988     if (wroteToTimeBuffer) { 989         DPRINTF(Activity, \"Activity this cycle.\\n\"); 990         cpu-&gt;activityThisCycle(); 991     } 992  993     // Reset the number of the instruction we've fetched. 994     numInst = 0; 995 }fetch: resolving TLB and cache accesses to actually fetches instructions1157 void1158 DefaultFetch&lt;Impl&gt;::fetch(bool &amp;status_change)1159 {1160     //////////////////////////////////////////1161     // Start actual fetch1162     //////////////////////////////////////////1163     ThreadID tid = getFetchingThread();1164 1165     assert(!cpu-&gt;switchedOut());1166 1167     if (tid == InvalidThreadID) {1168         // Breaks looping condition in tick()1169         threadFetched = numFetchingThreads;1170 1171         if (numThreads == 1) {  // @todo Per-thread stats1172             profileStall(0);1173         }1174 1175         return;1176     }1177 1178     DPRINTF(Fetch, \"Attempting to fetch from [tid:%i]\\n\", tid);1179 1180     // The current PC.1181     TheISA::PCState thisPC = pc[tid];1182 1183     Addr pcOffset = fetchOffset[tid];1184     Addr fetchAddr = (thisPC.instAddr() + pcOffset) &amp; BaseCPU::PCMask;1185 1186     bool inRom = isRomMicroPC(thisPC.microPC());1187 1188     // If returning from the delay of a cache miss, then update the status1189     // to running, otherwise do the cache access.  Possibly move this up1190     // to tick() function.1191     if (fetchStatus[tid] == IcacheAccessComplete) {1192         DPRINTF(Fetch, \"[tid:%i] Icache miss is complete.\\n\", tid);1193 1194         fetchStatus[tid] = Running;1195         status_change = true;1196     } else if (fetchStatus[tid] == Running) {1197         // Align the fetch PC so its at the start of a fetch buffer segment.1198         Addr fetchBufferBlockPC = fetchBufferAlignPC(fetchAddr);1199 1200         // If buffer is no longer valid or fetchAddr has moved to point1201         // to the next cache block, AND we have no remaining ucode1202         // from a macro-op, then start fetch from icache.1203         if (!(fetchBufferValid[tid] &amp;&amp; fetchBufferBlockPC == fetchBufferPC[tid])1204             &amp;&amp; !inRom &amp;&amp; !macroop[tid]) {1205             DPRINTF(Fetch, \"[tid:%i] Attempting to translate and read \"1206                     \"instruction, starting at PC %s.\\n\", tid, thisPC);1207 1208             fetchCacheLine(fetchAddr, tid, thisPC.instAddr());1209 1210             if (fetchStatus[tid] == IcacheWaitResponse)1211                 ++icacheStallCycles;1212             else if (fetchStatus[tid] == ItlbWait)1213                 ++fetchTlbCycles;1214             else1215                 ++fetchMiscStallCycles;1216             return;1217         } else if ((checkInterrupt(thisPC.instAddr()) &amp;&amp; !delayedCommit[tid])) {1218             // Stall CPU if an interrupt is posted and we're not issuing1219             // an delayed commit micro-op currently (delayed commit instructions1220             // are not interruptable by interrupts, only faults)1221             ++fetchMiscStallCycles;1222             DPRINTF(Fetch, \"[tid:%i] Fetch is stalled!\\n\", tid);1223             return;1224         }1225     } else {1226         if (fetchStatus[tid] == Idle) {1227             ++fetchIdleCycles;1228             DPRINTF(Fetch, \"[tid:%i] Fetch is idle!\\n\", tid);1229         }1230 1231         // Status is Idle, so fetch should do nothing.1232         return;1233     }......1417 }The fetch function is pretty complex and long function to analyze at once. Therefore, we will divide the fetch function in two main parts to understandentire logic of the O3CPU’s fetch stage. The first main part will explain how the fetch stage generate request to ITLB and ICache to resolve virtual to physical address translation and access the cache using the translated address. After the fetch stage receive the instructions from the ICache, the remaining partwill prepare the data structure that will be passed to the next stage, decode. Let’s take a look at how the fetch function retrieve the instructions first.First part of the fetch: ITLB to ICache access.getFetchingThread: selecting thread to let it fetchIf there are multiple threads need to fetch next instructions, the processor should select one among them to continue fetching. Based on the policy adopted by the processor, it can return different thread based on the current status of threads.1445 ///////////////////////////////////////1446 //                                   //1447 //  SMT FETCH POLICY MAINTAINED HERE //1448 //                                   //1449 ///////////////////////////////////////1450 template&lt;class Impl&gt;1451 ThreadID1452 DefaultFetch&lt;Impl&gt;::getFetchingThread()1453 {1454     if (numThreads &gt; 1) {1455         switch (fetchPolicy) {1456           case FetchPolicy::RoundRobin:1457             return roundRobin();1458           case FetchPolicy::IQCount:1459             return iqCount();1460           case FetchPolicy::LSQCount:1461             return lsqCount();1462           case FetchPolicy::Branch:1463             return branchCount();1464           default:1465             return InvalidThreadID;1466         }1467     } else {1468         list&lt;ThreadID&gt;::iterator thread = activeThreads-&gt;begin();1469         if (thread == activeThreads-&gt;end()) {1470             return InvalidThreadID;1471         }1472 1473         ThreadID tid = *thread;1474 1475         if (fetchStatus[tid] == Running ||1476             fetchStatus[tid] == IcacheAccessComplete ||1477             fetchStatus[tid] == Idle) {1478             return tid;1479         } else {1480             return InvalidThreadID;1481         }1482     }1483 }Translating virtual to physical address using I-TLB 602 template &lt;class Impl&gt; 603 bool 604 DefaultFetch&lt;Impl&gt;::fetchCacheLine(Addr vaddr, ThreadID tid, Addr pc) 605 {    606     Fault fault = NoFault; 607      608     assert(!cpu-&gt;switchedOut()); 609      610     // @todo: not sure if these should block translation. 611     //AlphaDep 612     if (cacheBlocked) { 613         DPRINTF(Fetch, \"[tid:%i] Can't fetch cache line, cache blocked\\n\", 614                 tid); 615         return false; 616     } else if (checkInterrupt(pc) &amp;&amp; !delayedCommit[tid]) { 617         // Hold off fetch from getting new instructions when: 618         // Cache is blocked, or 619         // while an interrupt is pending and we're not in PAL mode, or 620         // fetch is switched out. 621         DPRINTF(Fetch, \"[tid:%i] Can't fetch cache line, interrupt pending\\n\", 622                 tid); 623         return false; 624     } 625      626     // Align the fetch address to the start of a fetch buffer segment. 627     Addr fetchBufferBlockPC = fetchBufferAlignPC(vaddr); 628      629     DPRINTF(Fetch, \"[tid:%i] Fetching cache line %#x for addr %#x\\n\", 630             tid, fetchBufferBlockPC, vaddr); 631      632     // Setup the memReq to do a read of the first instruction's address. 633     // Set the appropriate read size and flags as well. 634     // Build request here. 635     RequestPtr mem_req = std::make_shared&lt;Request&gt;( 636         tid, fetchBufferBlockPC, fetchBufferSize,  637         Request::INST_FETCH, cpu-&gt;instMasterId(), pc, 638         cpu-&gt;thread[tid]-&gt;contextId()); 639      640     mem_req-&gt;taskId(cpu-&gt;taskId()); 641      642     memReq[tid] = mem_req; 643      644     // Initiate translation of the icache block 645     fetchStatus[tid] = ItlbWait; 646     FetchTranslation *trans = new FetchTranslation(this); 647     cpu-&gt;itb-&gt;translateTiming(mem_req, cpu-&gt;thread[tid]-&gt;getTC(), 648                               trans, BaseTLB::Execute); 649     return true; 650 }One can ask how the fetch stage can understand when the translation is finished.Note that FetchTranslation object is instantiated and sent to the Instruction TLB (itb) which conveys functions that should be invoked after the Translation is resolved. Therefore, when the instruction TLB finishesthe translation, it invokes the function provided by the passed FetchTranslation object and let the fetch stage to process next step, initiating the cache access. Anyway, let’s take a look at which function is provided to the TLB.gem5/src/cpu/o3/fetch.hh115     class FetchTranslation : public BaseTLB::Translation116     {117       protected:118         DefaultFetch&lt;Impl&gt; *fetch;119 120       public:121         FetchTranslation(DefaultFetch&lt;Impl&gt; *_fetch)122             : fetch(_fetch)123         {}124 125         void126         markDelayed()127         {}128 129         void130         finish(const Fault &amp;fault, const RequestPtr &amp;req, ThreadContext *tc,131                BaseTLB::Mode mode)132         {133             assert(mode == BaseTLB::Execute);134             fetch-&gt;finishTranslation(fault, req);135             delete this;136         }137     };You might remember that the TLB invokes the finish function at the end of the translationYes the FetchTranslation object provide the finish function. When the TLB finishes translation,by invoking finish function, it can let the processor know the translation is resolved. The finish function further invokes the finishTranslation function defined in the DefaultFetch class.finishTranslation: finishing TLB access and generate cache accessAfter the request to the TLB has been resolved, the remaining job is accessing the cache to read the instruction to fetch. Let’s take a look at how the fetch stage of the O3 CPU access the instruction cache. 652 template &lt;class Impl&gt; 653 void 654 DefaultFetch&lt;Impl&gt;::finishTranslation(const Fault &amp;fault, 655                                       const RequestPtr &amp;mem_req) 656 { 657     ThreadID tid = cpu-&gt;contextToThread(mem_req-&gt;contextId()); 658     Addr fetchBufferBlockPC = mem_req-&gt;getVaddr(); 659  660     assert(!cpu-&gt;switchedOut()); 661  662     // Wake up CPU if it was idle 663     cpu-&gt;wakeCPU(); 664  665     if (fetchStatus[tid] != ItlbWait || mem_req != memReq[tid] || 666         mem_req-&gt;getVaddr() != memReq[tid]-&gt;getVaddr()) { 667         DPRINTF(Fetch, \"[tid:%i] Ignoring itlb completed after squash... fetchStatus:%d\\n\", 668                 tid,fetchStatus[tid]); 669         ++fetchTlbSquashes; 670         return; 671     }Compared to simple processor which doesn’t provide speculative execution, O3 processor utilize the branch prediction and out-of-order execution. Therefore, if the current TLB completion is notified to the O3CPU because of amisspeculation, it should drop the TLB response and stop accessing the cache.Note that the speculation can turn out to be false while it waits TLB response.Line 665-670 checks the misspeculation. 674     // If translation was successful, attempt to read the icache block. 675     if (fault == NoFault) { 676         // Check that we're not going off into random memory 677         // If we have, just wait around for commit to squash something and put 678         // us on the right track 679         if (!cpu-&gt;system-&gt;isMemAddr(mem_req-&gt;getPaddr())) { 680             warn(\"Address %#x is outside of physical memory, stopping fetch\\n\", 681                     mem_req-&gt;getPaddr()); 682             fetchStatus[tid] = NoGoodAddr; 683             memReq[tid] = NULL; 684             return; 685         } 686  687         // Build packet here to access the Icache. 688         PacketPtr data_pkt = new Packet(mem_req, MemCmd::ReadReq); 689         data_pkt-&gt;dataDynamic(new uint8_t[fetchBufferSize]); 690  691         fetchBufferPC[tid] = fetchBufferBlockPC; 692         fetchBufferValid[tid] = false; 693         DPRINTF(Fetch, \"Fetch: Doing instruction read.\\n\"); 694  695         fetchedCacheLines++; 696 697         // Access the cache. 698         if (!icachePort.sendTimingReq(data_pkt)) { 699             assert(retryPkt == NULL); 700             assert(retryTid == InvalidThreadID); 701             DPRINTF(Fetch, \"[tid:%i] Out of MSHRs!\\n\", tid); 702  703             fetchStatus[tid] = IcacheWaitRetry; 704             retryPkt = data_pkt; 705             retryTid = tid; 706             cacheBlocked = true; 707         } else { 708             DPRINTF(Fetch, \"[tid:%i] Doing Icache access.\\n\", tid); 709             DPRINTF(Activity, \"[tid:%i] Activity: Waiting on I-cache \" 710                     \"response.\\n\", tid); 711             lastIcacheStall[tid] = curTick(); 712             fetchStatus[tid] = IcacheWaitResponse; 713             // Notify Fetch Request probe when a packet containing a fetch 714             // request is successfully sent 715             ppFetchRequestSent-&gt;notify(mem_req); 716         } 717     } else {If the current TLB resolution response is valid and speculated successfully, it should generate read request packet and send it to the Instruction Cache. Line 687-695 builds the packet and send buffer to be used for containing instructions read from the cache. When the cache access request cannot be sent to the instruction cache (line 698-707)because of the cache is busy for handling previous requests,it should retry when the Instruction cache is available later. Based on the line 701, we can guess that the cache supports multiple cache accesses simultaneously,but the request can exceed the capacity of its simultaneous processing.We will see whether the GEM5 supports blocking cache access or non-blocking cache accesses in another posting.Anyway when the retry is required, it memorizes the request packet and tid. Also it changes current status as IcacheWaitRetry.When the Instruction cache is available to process the request (line 708-716), it sets current status as  IcacheWaitResponse and waits until the Instruction cache resolves the request and send the actual instructions. 717     } else { 718         // Don't send an instruction to decode if we can't handle it. 719         if (!(numInst &lt; fetchWidth) || !(fetchQueue[tid].size() &lt; fetchQueueSize)) { 720             assert(!finishTranslationEvent.scheduled()); 721             finishTranslationEvent.setFault(fault); 722             finishTranslationEvent.setReq(mem_req); 723             cpu-&gt;schedule(finishTranslationEvent, 724                           cpu-&gt;clockEdge(Cycles(1))); 725             return; 726         } 727         DPRINTF(Fetch, \"[tid:%i] Got back req with addr %#x but expected %#x\\n\", 728                 tid, mem_req-&gt;getVaddr(), memReq[tid]-&gt;getVaddr()); 729         // Translation faulted, icache request won't be sent. 730         memReq[tid] = NULL; 731  732         // Send the fault to commit.  This thread will not do anything 733         // until commit handles the fault.  The only other way it can 734         // wake up is if a squash comes along and changes the PC. 735         TheISA::PCState fetchPC = pc[tid]; 736  737         DPRINTF(Fetch, \"[tid:%i] Translation faulted, building noop.\\n\", tid); 738         // We will use a nop in ordier to carry the fault. 739         DynInstPtr instruction = buildInst(tid, StaticInst::nopStaticInstPtr, 740                                            NULL, fetchPC, fetchPC, false); 741         instruction-&gt;setNotAnInst(); 742  743         instruction-&gt;setPredTarg(fetchPC); 744         instruction-&gt;fault = fault; 745         wroteToTimeBuffer = true; 746  747         DPRINTF(Activity, \"Activity this cycle.\\n\"); 748         cpu-&gt;activityThisCycle(); 749  750         fetchStatus[tid] = TrapPending; 751  752         DPRINTF(Fetch, \"[tid:%i] Blocked, need to handle the trap.\\n\", tid); 753         DPRINTF(Fetch, \"[tid:%i] fault (%s) detected @ PC %s.\\n\", 754                 tid, fault-&gt;name(), pc[tid]); 755     } 756     _status = updateFetchStatus(); 757 }When the TLB translation emits fault instead of successful translation, it should be handled based on the reason of the fault. When the fetchQeueue is already full or XXX (line 719-726),instead of issuing cache access, it postpone the operation to later by scheduling the finishTranslationEvent. Note that the request packet received from the ITLB and fault structure is also included in the finishTranslationEvent to process it later.140     /* Event to delay delivery of a fetch translation result in case of141      * a fault and the nop to carry the fault cannot be generated142      * immediately */143     class FinishTranslationEvent : public Event144     { 145       private:146         DefaultFetch&lt;Impl&gt; *fetch;147         Fault fault;148         RequestPtr req;149       150       public:151         FinishTranslationEvent(DefaultFetch&lt;Impl&gt; *_fetch)152             : fetch(_fetch), req(nullptr)153         {}154         155         void setFault(Fault _fault)156         {   157             fault = _fault;158         }159         160         void setReq(const RequestPtr &amp;_req)161         {   162             req = _req;163         }164         165         /** Process the delayed finish translation */166         void process()167         {   168             assert(fetch-&gt;numInst &lt; fetch-&gt;fetchWidth);169             fetch-&gt;finishTranslation(fault, req);170         }171         172         const char *description() const173         {   174             return \"FullO3CPU FetchFinishTranslation\";175         }   176       };In detail, when the FinishTranslationEvent happens after the designated cycles passed,it invokes the process function defined in the class. As shown in the above code line 166-170,it calls finishTranslation with the passed fault and request again.For the other reason of faults, \\TODO{explanation required for the rest of the faulting code}.After the fetch stage handles the response from the ITLB, it should update the current status of the fetch stage by invoking the updateFetchStatus function. 841 template&lt;class Impl&gt; 842 typename DefaultFetch&lt;Impl&gt;::FetchStatus 843 DefaultFetch&lt;Impl&gt;::updateFetchStatus() 844 { 845     //Check Running 846     list&lt;ThreadID&gt;::iterator threads = activeThreads-&gt;begin(); 847     list&lt;ThreadID&gt;::iterator end = activeThreads-&gt;end(); 848  849     while (threads != end) { 850         ThreadID tid = *threads++; 851  852         if (fetchStatus[tid] == Running || 853             fetchStatus[tid] == Squashing || 854             fetchStatus[tid] == IcacheAccessComplete) { 855  856             if (_status == Inactive) { 857                 DPRINTF(Activity, \"[tid:%i] Activating stage.\\n\",tid); 858  859                 if (fetchStatus[tid] == IcacheAccessComplete) { 860                     DPRINTF(Activity, \"[tid:%i] Activating fetch due to cache\" 861                             \"completion\\n\",tid); 862                 } 863  864                 cpu-&gt;activateStage(O3CPU::FetchIdx); 865             } 866  867             return Active; 868         } 869     } 870  871     // Stage is switching from active to inactive, notify CPU of it. 872     if (_status == Active) { 873         DPRINTF(Activity, \"Deactivating stage.\\n\"); 874  875         cpu-&gt;deactivateStage(O3CPU::FetchIdx); 876     } 877  878     return Inactive; 879 }processCacheCompletion: completing ICache accessWhen the sendTimingReq is invoked through the icachePort, which means cache access request sent to the Instruction cache successfully, after few cycles elapsed, the O3CPU will be notified that the cache read completes.The cache access completion is handled by the recvTimingResp of the IcachePort allocated for the O3CPU.1676 template&lt;class Impl&gt;1677 bool1678 DefaultFetch&lt;Impl&gt;::IcachePort::recvTimingResp(PacketPtr pkt)1679 {1680     DPRINTF(O3CPU, \"Fetch unit received timing\\n\");1681     // We shouldn't ever get a cacheable block in Modified state1682     assert(pkt-&gt;req-&gt;isUncacheable() ||1683            !(pkt-&gt;cacheResponding() &amp;&amp; !pkt-&gt;hasSharers()));1684     fetch-&gt;processCacheCompletion(pkt);1685 1686     return true;1687 }When it receives the instructions from the cache, it invokes the processCacheCompletion function and ask this function to handle the response arrived from the cache. 389 DefaultFetch&lt;Impl&gt;::processCacheCompletion(PacketPtr pkt) 390 { 391     ThreadID tid = cpu-&gt;contextToThread(pkt-&gt;req-&gt;contextId()); 392  393     DPRINTF(Fetch, \"[tid:%i] Waking up from cache miss.\\n\", tid); 394     assert(!cpu-&gt;switchedOut()); 395  396     // Only change the status if it's still waiting on the icache access 397     // to return. 398     if (fetchStatus[tid] != IcacheWaitResponse || 399         pkt-&gt;req != memReq[tid]) { 400         ++fetchIcacheSquashes; 401         delete pkt; 402         return; 403     } 404  405     memcpy(fetchBuffer[tid], pkt-&gt;getConstPtr&lt;uint8_t&gt;(), fetchBufferSize); 406     fetchBufferValid[tid] = true; 407  408     // Wake up the CPU (if it went to sleep and was waiting on 409     // this completion event). 410     cpu-&gt;wakeCPU(); 411  412     DPRINTF(Activity, \"[tid:%i] Activating fetch due to cache completion\\n\", 413             tid); 414  415     switchToActive(); 416  417     // Only switch to IcacheAccessComplete if we're not stalled as well. 418     if (checkStall(tid)) { 419         fetchStatus[tid] = Blocked; 420     } else { 421         fetchStatus[tid] = IcacheAccessComplete; 422     } 423  424     pkt-&gt;req-&gt;setAccessLatency(); 425     cpu-&gt;ppInstAccessComplete-&gt;notify(pkt); 426     // Reset the mem req to NULL. 427     delete pkt; 428     memReq[tid] = NULL; 429 }When the instructions from the cache arrives, it could be the case where the misspeculation had initiated the cache access. In that case, it should drop the cache access by deleting the response packet.In other cases, the read instructions should be copied from the packet to the fetchBuffer containing the fetched instructions (line 405-406). When the current tid is stalled because of some events (we will cover which conditionmakes the thread to be stalled), it should be blocked until the stall is resolved.If there is no stall, then the fetchStatus can be changed to IcacheAccessComplete,which means the thread can finish the fetch stage.Now let’s go back to the fetch function again!Revisiting fetch stage to handle the instructions fetched from the cacheFetch tick happens every processor tickOne important thing to note is that Fetch stage is always executed at every clock cycle.However, based on the current status of the processor and other components such as TLB and cache, fetch stage cannot produce meaningful progress and should wait until the other component finish their operations. Although modern processors have multiple cores to execute, but if the all cores are waiting the cache accesses, no other hardware thread cannot execute the fetch stage. The getFetchingThreadfunction checks the status of the all hardware threads and returns thread if there is onethat can execute the fetch stage.1156 template&lt;class Impl&gt;1157 void1158 DefaultFetch&lt;Impl&gt;::fetch(bool &amp;status_change)1159 {1160     //////////////////////////////////////////1161     // Start actual fetch1162     //////////////////////////////////////////1163     ThreadID tid = getFetchingThread();1164 1165     assert(!cpu-&gt;switchedOut());1166 1167     if (tid == InvalidThreadID) {1168         // Breaks looping condition in tick()1169         threadFetched = numFetchingThreads;1170 1171         if (numThreads == 1) {  // @todo Per-thread stats1172             profileStall(0);1173         }1174 1175         return;1176     }As shown in the above code, when there is no available hardware thread to execute fetch stage,getFetchingThread returns InvalidThreadID, and no thread can produce progress at that clock cycle.Only the case where the getFetchingThread returns an available thread is the thread is in one of the three fetchStatus: Running, IcacheAccessComplete, or Idle.   1000: system.cpu.fetch: Running stage.   1000: system.cpu.fetch: Attempting to fetch from [tid:0]   1000: system.cpu.fetch: [tid:0] Attempting to translate and read instruction, starting at PC (0x7ffff8000090=&gt;0x7ffff8000098).(0=&gt;1).   1000: system.cpu.fetch: [tid:0] Fetching cache line 0x7ffff8000080 for addr 0x7ffff8000090   1000: system.cpu.fetch: Fetch: Doing instruction read.   1000: system.cpu.fetch: [tid:0] Doing Icache access.   1500: system.cpu.fetch: Running stage.   1500: system.cpu.fetch: There are no more threads available to fetch from.   1500: system.cpu.fetch: [tid:0] Fetch is waiting cache response!   2000: system.cpu.fetch: Running stage.   2000: system.cpu.fetch: There are no more threads available to fetch from.   2000: system.cpu.fetch: [tid:0] Fetch is waiting cache response!   2500: system.cpu.fetch: Running stage.   2500: system.cpu.fetch: There are no more threads available to fetch from.   2500: system.cpu.fetch: [tid:0] Fetch is waiting cache response!   3000: system.cpu.fetch: Running stage.   3000: system.cpu.fetch: There are no more threads available to fetch from.   3000: system.cpu.fetch: [tid:0] Fetch is waiting cache response!   3500: system.cpu.fetch: Running stage.   3500: system.cpu.fetch: There are no more threads available to fetch from.   3500: system.cpu.fetch: [tid:0] Fetch is waiting cache response!   4000: system.cpu.fetch: Running stage.   4000: system.cpu.fetch: There are no more threads available to fetch from.   4000: system.cpu.fetch: [tid:0] Fetch is waiting cache response!   4500: system.cpu.fetch: Running stage.   4500: system.cpu.fetch: There are no more threads available to fetch from.   4500: system.cpu.fetch: [tid:0] Fetch is waiting cache response!   5000: system.cpu.fetch: Running stage.   5000: system.cpu.fetch: There are no more threads available to fetch from.   5000: system.cpu.fetch: [tid:0] Fetch is waiting cache response!   5500: system.cpu.fetch: Running stage.   5500: system.cpu.fetch: There are no more threads available to fetch from.   5500: system.cpu.fetch: [tid:0] Fetch is waiting cache response!  78000: system.cpu.fetch: [tid:0] Waking up from cache miss.  78001: system.cpu.fetch: [tid:0] Waking up from cache miss.  78500: system.cpu.fetch: Running stage.  78500: system.cpu.fetch: Attempting to fetch from [tid:0]  78500: system.cpu.fetch: [tid:0] Icache miss is complete.In our current system, because we only have one hardware thread, while it waits for the ICache miss to be resolved, it cannot execute fetch stageto produce further progress. The described behavior of the fetch stage is described in the above log. After the thread first fetches the instructions at cycle 1000,it cannot produce any progress until the ICache miss is resolved at cycle 78000. After the ICache miss is resolved (after 78500 cycle), it can finally produce progress from the fetch stage.Remember that when a missed ICache is resolved by the processCacheCompletion function, it changes the fetchStatus of the thread from IcacheWaitResponse to IcacheAccessComplete.Therefore, when the fetch stage is executed once again, the undiscovered path will be executed.1188     // If returning from the delay of a cache miss, then update the status1189     // to running, otherwise do the cache access.  Possibly move this up1190     // to tick() function.1191     if (fetchStatus[tid] == IcacheAccessComplete) {1192         DPRINTF(Fetch, \"[tid:%i] Icache miss is complete.\\n\", tid);1193 1194         fetchStatus[tid] = Running;1195         status_change = true;1196     } else if (fetchStatus[tid] == Running) {Compared to the initial fetch execution that initiated the ITLB and ICache accesses, because the fetchStatus has been changed to IcacheAccessComplete,the fetch stage can execute the rest of the fetch function at this moment. Let’s take a look at the rest of the fetch function in detail.fetchBuffer contains actual instructions for a particular hardware thread1235     //when a requested instruction cache block is arrived(IcacheAccessComplete)1236     ++fetchCycles;1237 1238     TheISA::PCState nextPC = thisPC;1239 1240     StaticInstPtr staticInst = NULL;1241     StaticInstPtr curMacroop = macroop[tid];1242 1243     // If the read of the first instruction was successful, then grab the1244     // instructions from the rest of the cache line and put them into the1245     // queue heading to decode.1246 1247     DPRINTF(Fetch, \"[tid:%i] Adding instructions to queue to \"1248             \"decode.\\n\", tid);1249 1250     // Need to keep track of whether or not a predicted branch1251     // ended this fetch block.1252     bool predictedBranch = false;1253 1254     // Need to halt fetch if quiesce instruction detected1255     bool quiesce = false;1256 1257     TheISA::MachInst *cacheInsts =1258         reinterpret_cast&lt;TheISA::MachInst *&gt;(fetchBuffer[tid]);12591260     const unsigned numInsts = fetchBufferSize / instSize;1261     unsigned blkOffset = (fetchAddr - fetchBufferPC[tid]) / instSize;Remember that the fetchBuffer[tid] contains the actual instructions read fromthe ICache. Note that cacheInsts variable which is the TheISA::MachInst * type references the instruction buffer, fetchBuffer[tid].This variable is passed to the decoder to pass the instruction stream read from the ICache. Also, the TheISA::MachInst is a uint64_t in the x86 architecture (TheISA will be changed to the X86 namespace). Because X86 architecture adopts variable instruction length, it approximately set the instruction length as 8bytes and calculate the number of instructions in the instruction stream fetched from the ICache. Note that the numInsts is approximated as fetchBufferSize / instSize.The main fetchloop processing instructions1263     // Loop through instruction memory from the cache.1264     // Keep issuing while fetchWidth is available and branch is not1265     // predicted taken1266     while (numInst &lt; fetchWidth &amp;&amp; fetchQueue[tid].size() &lt; fetchQueueSize1267            &amp;&amp; !predictedBranch &amp;&amp; !quiesce) {......1382         // Re-evaluate whether the next instruction to fetch is in micro-op ROM1383         // or not.1384         inRom = isRomMicroPC(thisPC.microPC());1385     }The while loop (line 1266-1267) is the main body of processing instructions stored in the fetchBuffer. Be careful not to confuse numInst with numInsts.numInst means the number of instructions fetched at this cycle, and numInsts means the number of instructions that can possibly reside in the fetchBuffer. Also, fetchQueue is the CPP standard deque managing DynInstPtr which is the pointer of one macroop instruction. Therefore, the loop checks first whether the number of fetched instructions at this cycle exceed the deisgnated fetchWidth and examine whether the fetchQueue is overflowed, which means too many instructions have been fetched from the instruction cache. Because the instruction length can vary but the capacity of fetchQueue is limited, sometimes depending on which instructions actually residein the fetched instruction cache, it cannot process all instructions at that cycle. Based on the fact that it checks if the fetchQueue is overflowed at every iteration, we can assume that the loop insert instruction to the fetchQueue. We will take a look at the details soon!Also it checks the type of the previous instruction handled by the loop, whether it is predictedBranch or quiesce. If the previous instruction turns out to one of these type of instruction, then the loop should not process the instructionin the fetchBuffer further and stop.Decoder1268         // We need to process more memory if we aren't going to get a1269         // StaticInst from the rom, the current macroop, or what's already1270         // in the decoder.1271         bool needMem = !inRom &amp;&amp; !curMacroop &amp;&amp;1272             !decoder[tid]-&gt;instReady();1273         fetchAddr = (thisPC.instAddr() + pcOffset) &amp; BaseCPU::PCMask;1274         Addr fetchBufferBlockPC = fetchBufferAlignPC(fetchAddr);1275 1276         if (needMem) {1277             // If buffer is no longer valid or fetchAddr has moved to point1278             // to the next cache block then start fetch from icache.1279             if (!fetchBufferValid[tid] ||1280                 fetchBufferBlockPC != fetchBufferPC[tid])1281                 break;1282 1283             if (blkOffset &gt;= numInsts) {1284                 // We need to process more memory, but we've run out of the1285                 // current block.1286                 break;1287             }1288 1289             decoder[tid]-&gt;moreBytes(thisPC, fetchAddr, cacheInsts[blkOffset]);1290 1291             if (decoder[tid]-&gt;needMoreBytes()) {1292                 blkOffset++;1293                 fetchAddr += instSize;1294                 pcOffset += instSize;1295             }1296         }After the all conditions are met, each iteration of the loop processes the instruction one by one. For the first execution of the fetch stage, the inRom and curMacroop are set as false and NULL respectively. Also, when the decoder object embedded in the fetch stage is initialized, the instDone variable of the decoder is set as false, which will be returned as the result of instReady function of the decoder. Therefore, the needMem should be set for the initial execution. When the needMem flag is set, which means \\TODO{XXX},it invokes moreBytes function of the decoder to decode the instruction.306     //Use this to give data to the decoder. This should be used307     //when there is control flow.308     void moreBytes(const PCState &amp;pc, Addr fetchPC, MachInst data)309     {310         DPRINTF(Decoder, \"Getting more bytes.\\n\");311         basePC = fetchPC;312         offset = (fetchPC &gt;= pc.instAddr()) ? 0 : pc.instAddr() - fetchPC;313         fetchChunk = letoh(data);314         outOfBytes = false;315         process();316     } 74 Decoder::process() 75 { 76     //This function drives the decoder state machine. 77  78     //Some sanity checks. You shouldn't try to process more bytes if 79     //there aren't any, and you shouldn't overwrite an already 80     //decoder ExtMachInst. 81     assert(!outOfBytes); 82     assert(!instDone); 83  84     if (state == ResetState) 85         state = doResetState(); 86     if (state == FromCacheState) { 87         state = doFromCacheState(); 88     } else { 89         instBytes-&gt;chunks.push_back(fetchChunk); 90     } 91  92     //While there's still something to do... 93     while (!instDone &amp;&amp; !outOfBytes) { 94         uint8_t nextByte = getNextByte(); 95         switch (state) { 96           case PrefixState: 97             state = doPrefixState(nextByte); 98             break; 99           case Vex2Of2State:100             state = doVex2Of2State(nextByte);101             break;102           case Vex2Of3State:103             state = doVex2Of3State(nextByte);104             break;105           case Vex3Of3State:106             state = doVex3Of3State(nextByte);107             break;108           case VexOpcodeState:109             state = doVexOpcodeState(nextByte);110             break;111           case OneByteOpcodeState:112             state = doOneByteOpcodeState(nextByte);113             break;114           case TwoByteOpcodeState:115             state = doTwoByteOpcodeState(nextByte);116             break;117           case ThreeByte0F38OpcodeState:118             state = doThreeByte0F38OpcodeState(nextByte);119             break;120           case ThreeByte0F3AOpcodeState:121             state = doThreeByte0F3AOpcodeState(nextByte);122             break;123           case ModRMState:124             state = doModRMState(nextByte);125             break;126           case SIBState:127             state = doSIBState(nextByte);128             break;129           case DisplacementState:130             state = doDisplacementState();131             break;132           case ImmediateState:133             state = doImmediateState();134             break;135           case ErrorState:136             panic(\"Went to the error state in the decoder.\\n\");137           default:138             panic(\"Unrecognized state! %d\\n\", state);139         }140     }141 }Based on the instruction format, different doXXX function will be invoked to parse the macroop instruction. First of all, it invokes doResetState for every macroop to initialize the variables representing the parsed instruction. Also it sets the origPC field as the PC address of the macroop instruction.After the initialization, based on the instruction format, it will invoke different parsing code. Based on the n-1 byte(s) of the instruction,next n(+1) bytes of the instruction’s format will be determined. Therefore, by parsing each byte one by one,different format of the instruction can be fully decoded by the above process function.During the parsing, it invokes consumeByte(s) function when a particular part of the instruction could be successfully decoded. The consumeByte function increases the offset variable of the decoder to present the length of the currently being parsed macroop. After the moreBytes finish the early decoding of the macroop instruction,it sets the instDone as true. However, note that moreBytes and process function just parses the macroop instruction to excerpt some bytes dedicated for each part of the instruction such as Rex and modRM in x86 architecture. Therefore, we still need to decode the parsed instruction to understand what is this instruction!The second loop to process each instructionAfter the decoder finishing early-decode of the macroop instruction, it encounters another loop that translate the macroop instruction into multiple microops if possible. Note that the processor pipeline executes the microops not the macroop instructions. Therefore, instead of the macroop, the microops should be inserted into the fetch queue.1298         // Extract as many instructions and/or microops as we can from1299         // the memory we've processed so far.1300         do {......1378         } while ((curMacroop || decoder[tid]-&gt;instReady()) &amp;&amp;1379                  numInst &lt; fetchWidth &amp;&amp;1380                  fetchQueue[tid].size() &lt; fetchQueueSize);As shown in the above code, the second loop continues until the curMacroop is not a NULLor until the translation from the current macroop to the microops is fished and the fetchQueue is available to contain translated microops. Let’s take a look at the details of the second loop.1298         // Extract as many instructions and/or microops as we can from1299         // the memory we've processed so far.1300         do {1301             if (!(curMacroop || inRom)) {1302                 if (decoder[tid]-&gt;instReady()) {1303                     staticInst = decoder[tid]-&gt;decode(thisPC);1304 1305                     // Increment stat of fetched instructions.1306                     ++fetchedInsts;1307 1308                     if (staticInst-&gt;isMacroop()) {1309                         curMacroop = staticInst;1310                     } else {1311                         pcOffset = 0;1312                     }1313                 } else {1314                     // We need more bytes for this instruction so blkOffset and1315                     // pcOffset will be updated1316                     break;1317                 }1318             }Note that we haven’t assigned anything to curMacroop and executed the ROM code.Also, decoder[tid]-&gt;instReady is true because the moreBytes function successfully pre-decoded the macroop instruction. Therefore, it will invoke the decode function to understand which instruction actually it is.The decode function of the decoder generates the StaticInstPtr which has informationabout the current instruction located at thisPC. In our case, because we are firstly executing the macroop instruction,it should return the reference of the macroop instruction.Let’s briefly take a look at the decode function.gem5/src/arch/x86/decode.cc693 StaticInstPtr694 Decoder::decode(PCState &amp;nextPC)695 {696     if (!instDone)697         return NULL;698     instDone = false;699     updateNPC(nextPC);700 701     StaticInstPtr &amp;si = instBytes-&gt;si;702     if (si)703         return si;704 705     // We didn't match in the AddrMap, but we still populated an entry. Fix706     // up its byte masks.707     const int chunkSize = sizeof(MachInst);708 709     instBytes-&gt;lastOffset = offset;710 711     Addr firstBasePC = basePC - (instBytes-&gt;chunks.size() - 1) * chunkSize;712     Addr firstOffset = origPC - firstBasePC;713     Addr totalSize = instBytes-&gt;lastOffset - firstOffset +714         (instBytes-&gt;chunks.size() - 1) * chunkSize;715     int start = firstOffset;716     instBytes-&gt;masks.clear();717 718     while (totalSize) {719         int end = start + totalSize;720         end = (chunkSize &lt; end) ? chunkSize : end;721         int size = end - start;722         int idx = instBytes-&gt;masks.size();723 724         MachInst maskVal = mask(size * 8) &lt;&lt; (start * 8);725         assert(maskVal);726 727         instBytes-&gt;masks.push_back(maskVal);728         instBytes-&gt;chunks[idx] &amp;= instBytes-&gt;masks[idx];729         totalSize -= size;730         start = 0;731     }732 733     si = decode(emi, origPC);734     return si;735 }There are two important things to be done by the decode function. First, it invokes updateNPC to update the next pc based on the current instruction.Also remember that the basePC has been set as fetchAddr when the moreBytes has been invoked.328     void329     updateNPC(X86ISA::PCState &amp;nextPC)330     {331         if (!nextPC.size()) {332             int size = basePC + offset - origPC;333             DPRINTF(Decoder,334                     \"Calculating the instruction size: \"335                     \"basePC: %#x offset: %#x origPC: %#x size: %d\\n\",336                     basePC, offset, origPC, size);337             nextPC.size(size);338             nextPC.npc(nextPC.pc() + size);339         }340     }Because decoder already knows the length of the instruction,it can calculate the size of the instructionand set the nextPC value as current PC + sizeof(instruction).The npc function updates the _npc field of the nextPC, and it will be used to update the _pc member field of the PCState object later. Note that the nextPC is actually the thisPC variabledeclared in the fetch function. It could be confusing because the fetch function also declares the nextPC variable, but updateNPC updates the npc of the thisPC not the nextPC variable of the fetch.After updating the npc, the decode function invokes actual decode function.Also it is important that the updateNPC function is only invoked when the curMacroop is set as NULL. While the microops of the macroop is fetched,the npc will not be updated.681 StaticInstPtr682 Decoder::decode(ExtMachInst mach_inst, Addr addr)683 {684     auto iter = instMap-&gt;find(mach_inst);685     if (iter != instMap-&gt;end())686         return iter-&gt;second;687 688     StaticInstPtr si = decodeInst(mach_inst);689     (*instMap)[mach_inst] = si;690     return si;691 }It traverses decode cache instMap to find the instruction object cached if the same instruction has been decoded earlier.If not, it invokes decodeInst function automatically generated based on the python parser on the GEM5. We will not cover the details of the decodeInst function in this posting.Let’s go back to the second loop again!After the decode function execution, we can finally have the object associated with the decoded instruction. If the decoded instruction is the macroop, it sets the curMacroop as the returned staticInst.fetchMicroop: Fetching microops from the macroop or ROM1319             // Whether we're moving to a new macroop because we're at the1320             // end of the current one, or the branch predictor incorrectly1321             // thinks we are...1322             bool newMacro = false;1323             if (curMacroop || inRom) {1324                 if (inRom) {1325                     staticInst = cpu-&gt;microcodeRom.fetchMicroop(1326                             thisPC.microPC(), curMacroop);1327                 } else {1328                     staticInst = curMacroop-&gt;fetchMicroop(thisPC.microPC());1329                 }1330                 newMacro |= staticInst-&gt;isLastMicroop();1331             }The curMacroop is set as the macroop instruction pointed to by the PC.However, to execute the instruction on the pipeline,we should have access on the microops consisting of the current Macroop.You might remember that the macroop consists of multiple microops). Also, it might remind you of the ROM code. Yeah, there are two places where the microops are used. Therefore, based on the current status of the processor, whether it executes the macroop or ROM code, it needs to fetch the microops from the relevant places. Regardless of its location,GEM5 utilize the interface called fetchMicroop.When the processor is in the midst of execution of ROM code, it invokes the fetchMicroop function from the microcodeRom.gem5/src/arch/x86/microcode_rom.hh 60         StaticInstPtr 61         fetchMicroop(MicroPC microPC, StaticInstPtr curMacroop) 62         { 63             microPC = normalMicroPC(microPC); 64             if (microPC &gt;= numMicroops) 65                 return X86ISA::badMicroop; 66             else 67                 return genFuncs[microPC](curMacroop); 68         }Also when the processor is in the middle of executing the macroop,it should ask the macroop to return microops consisting of it.gem5/src/arch/x86/insts/macroop.hh 77     StaticInstPtr 78     fetchMicroop(MicroPC microPC) const 79     { 80         if (microPC &gt;= numMicroops) 81             return badMicroop; 82         else 83             return microops[microPC]; 84     }gem5/src/cpu/fetch_impl.hh1239     StaticInstPtr staticInst = NULL;The return value of the fetchMicroop function will be stored to the staticInst, which is the StaticInstPtr.Therefore, it can points to any instructions.Previously, the decoded macroops are pointed to by this staticInst variable. It provides a method to discern whether it is Macroop or Microop.Populating dynamic instruction object1332 1333             DynInstPtr instruction =1334                 buildInst(tid, staticInst, curMacroop,1335                           thisPC, nextPC, true);1336 1337             ppFetch-&gt;notify(instruction);1338             numInst++;1339 1340 #if TRACING_ON1341             if (DTRACE(O3PipeView)) {1342                 instruction-&gt;fetchTick = curTick();1343             }1344 #endifNow we have a macroop pointed to by curMacroop variable and its associated microop pointed to by staticInst. Using this information, the buildInst function populates the dynamic object representing one instructionthat can be really executed on the pipeline. One might ask why we need another object for instruction.However, note that these objects are static instruction object, butwe need a dynamic instruction object that conveys all information required for executing the instructionthrough the pipeline.The dynamic instruction objects are populated for passing information of the instruction in between different pipeline stages. Therefore, the buildInst function generates the dynamic instruction and enqueues the instruction into the fetch queue to pass the instruction information to the next pipeline stages.Let’s take a look at how the buildInst generates the dynamic instruction.buildInst: populating microops from the macroop1102 template&lt;class Impl&gt;1103 typename Impl::DynInstPtr1104 DefaultFetch&lt;Impl&gt;::buildInst(ThreadID tid, StaticInstPtr staticInst,1105                               StaticInstPtr curMacroop, TheISA::PCState thisPC,1106                               TheISA::PCState nextPC, bool trace)1107 {1108     // Get a sequence number.1109     InstSeqNum seq = cpu-&gt;getAndIncrementInstSeq();1110 1111     // Create a new DynInst from the instruction fetched.1112     DynInstPtr instruction =1113         new DynInst(staticInst, curMacroop, thisPC, nextPC, seq, cpu);1114     instruction-&gt;setTid(tid);1115 1116     instruction-&gt;setASID(tid);1117 1118     instruction-&gt;setThreadState(cpu-&gt;thread[tid]);1119 1120     DPRINTF(Fetch, \"[tid:%i] Instruction PC %#x (%d) created \"1121             \"[sn:%lli].\\n\", tid, thisPC.instAddr(),1122             thisPC.microPC(), seq);1123 1124     DPRINTF(Fetch, \"[tid:%i] Instruction is: %s\\n\", tid,1125             instruction-&gt;staticInst-&gt;1126             disassemble(thisPC.instAddr()));You can think of the DynInst as the meta data conveying all informationto execute one instruction.After the instruction generation, it sets the thread specific information of the instruction (tid, ASID). Those information is required later in the execution stageto understand which instruction has been issued by which thread. 53 struct O3CPUImpl 54 { 55     /** The type of MachInst. */ 56     typedef TheISA::MachInst MachInst; 57 58     /** The CPU policy to be used, which defines all of the CPU stages. */ 59     typedef SimpleCPUPolicy&lt;O3CPUImpl&gt; CPUPol; 60 61     /** The DynInst type to be used. */ 62     typedef BaseO3DynInst&lt;O3CPUImpl&gt; DynInst; 63 64     /** The refcounted DynInst pointer to be used.  In most cases this is 65      *  what should be used, and not DynInst *. 66      */ 67     typedef RefCountingPtr&lt;DynInst&gt; DynInstPtr;The constructor call of the DynInst invokes the constructor of the BaseO3DynInst class and initialize its member field as described in the following constructor.gem5/src/cpu/o3/dyn_inst_impl.hh 50 template &lt;class Impl&gt; 51 BaseO3DynInst&lt;Impl&gt;::BaseO3DynInst(const StaticInstPtr &amp;staticInst, 52                                    const StaticInstPtr &amp;macroop, 53                                    TheISA::PCState pc, TheISA::PCState predPC, 54                                    InstSeqNum seq_num, O3CPU *cpu) 55     : BaseDynInst&lt;Impl&gt;(staticInst, macroop, pc, predPC, seq_num, cpu) 56 { 57     initVars(); 58 }Let’s take a look at who derives the DynInstPtr then. 97 template &lt;class Impl&gt; 98 class FullO3CPU : public BaseO3CPU 99 {100   public:101     // Typedefs from the Impl here.102     typedef typename Impl::CPUPol CPUPolicy;103     typedef typename Impl::DynInstPtr DynInstPtr;104     typedef typename Impl::O3CPU O3CPU;As shown in the above code, the DynInstPtr is the Impl::DynInstPtr, which is the RefCountingPtr defined in the O3CPUImpl. The RefCountingPtr is the CPP template class defining all operationssuch as equal sign that can assign new object of the template type and member field reference operator -&gt; to access the assigned object.The only additional work done by this class is counting the reference for this object, and it can be utilized as the template typed object. Therefore, without knowing the details, the instruction variable can be utilized as a pointer referencing DynInst objects.Inserting generated dynamic instructions into the fetchQueue1127 1128 #if TRACING_ON1129     if (trace) {1130         instruction-&gt;traceData =1131             cpu-&gt;getTracer()-&gt;getInstRecord(curTick(), cpu-&gt;tcBase(tid),1132                     instruction-&gt;staticInst, thisPC, curMacroop);1133     }1134 #else1135     instruction-&gt;traceData = NULL;1136 #endif1137 1138     // Add instruction to the CPU's list of instructions.1139     instruction-&gt;setInstListIt(cpu-&gt;addInst(instruction));1140 1141     // Write the instruction to the first slot in the queue1142     // that heads to decode.1143     assert(numInst &lt; fetchWidth);1144     fetchQueue[tid].push_back(instruction);1145     assert(fetchQueue[tid].size() &lt;= fetchQueueSize);1146     DPRINTF(Fetch, \"[tid:%i] Fetch queue entry created (%i/%i).\\n\",1147             tid, fetchQueue[tid].size(), fetchQueueSize);1148     //toDecode-&gt;insts[toDecode-&gt;size++] = instruction;1149 1150     // Keep track of if we can take an interrupt at this boundary1151     delayedCommit[tid] = instruction-&gt;isDelayedCommit();1152 1153     return instruction;1154 }After the dynamic instruction is populated, it should be inserted into the fetchQueue to pass the generated instructions to the next stage. Now let’s go back to the second loop of the fetch functionUpdating nextPC and handling branch instruction1346             nextPC = thisPC;1347 1348             // If we're branching after this instruction, quit fetching1349             // from the same block.1350             predictedBranch |= thisPC.branching();1351             predictedBranch |=1352                 lookupAndUpdateNextPC(instruction, nextPC);1353             if (predictedBranch) {1354                 DPRINTF(Fetch, \"Branch detected with PC = %s\\n\", thisPC);1355             }Until now, we have populated the microops and enqueued the generated instructions into the fetchQueue. To repeat this sequence of operations and fill the fetchQueue, the second loop should determine the nextPC to lookup. First of all, if the current instruction is one of the branching instructions,the nextPC should be determined based on the execution result of branch prediction speculatively.lookupAndUpdateNextPC: determine the nextPC based on control flow instructionThe lookupAndUpdateNextPC determines the nextPC by checking whether the current instruction is the control flow instruction. Also, because O3 processor adopts branch predictor,the lookupAndUpdateNextPC asks branch predictor whether it needs to change the nextPC if the current instruction is the branching instruction. Note that the lookupAndUpdateNextPC accepts the dynamic instruction we generated in the buildInst function. 556 template &lt;class Impl&gt; 557 bool 558 DefaultFetch&lt;Impl&gt;::lookupAndUpdateNextPC( 559         const DynInstPtr &amp;inst, TheISA::PCState &amp;nextPC) 560 { 561     // Do branch prediction check here. 562     // A bit of a misnomer...next_PC is actually the current PC until 563     // this function updates it. 564     bool predict_taken; 565  566     if (!inst-&gt;isControl()) { 567         TheISA::advancePC(nextPC, inst-&gt;staticInst); 568         inst-&gt;setPredTarg(nextPC); 569         inst-&gt;setPredTaken(false); 570         return false; 571     }First of all, it can simply check if the current instruction affects execution control by invoking isControl method of the dynamic instruction. The isControl function of the dynamic instructionjust invokes the same method of the staticInst of the DynInst,which is the static class representing microop operation. If the current instruction is not a control flow instruction,it just updates nextPC by invoking advancePC function with the staticInst of the current dynamic instruction (because fetching is done with the macroop level).advancePC: advance micro pc or pc based on the architecturegem5/src/arch/x86/utility.hh 78     inline void 79     advancePC(PCState &amp;pc, const StaticInstPtr &amp;inst) 80     { 81         inst-&gt;advancePC(pc); 82     }The advancePC function invokes advancePC function of the StaticInstPtr class back to back. Because we are targeting X86 architecture,the inst should be the object of the X86StaticInst class.gem5/src/arch/x86/insts/static_inst.hh 77     /** 78      * Base class for all X86 static instructions. 79      */ 80  81     class X86StaticInst : public StaticInst 82     { 83       protected: 84         // Constructor. 85         X86StaticInst(const char *mnem, 86              ExtMachInst _machInst, OpClass __opClass) 87                 : StaticInst(mnem, _machInst, __opClass) 88             { 89             }......179         void180         advancePC(PCState &amp;pcState) const181         {182             pcState.advance();183         }184     };Also, remind that X86 architecture executes the microop instead of the macroop.Therefore, the StaticInstPtr points to microop object in x86.Thus X86 on GEM5 provide another class called X86MicroopBase inheriting X86StaticInst class.gem5/src/arch/x86/insts/microop.hh 88     //A class which is the base of all x86 micro ops. It provides a function to 89     //set necessary flags appropriately. 90     class X86MicroopBase : public X86StaticInst 91     { 92       protected: 93         const char * instMnem; 94         uint8_t opSize; 95         uint8_t addrSize; 96  97         X86MicroopBase(ExtMachInst _machInst, 98                 const char *mnem, const char *_instMnem, 99                 uint64_t setFlags, OpClass __opClass) :100             X86ISA::X86StaticInst(mnem, _machInst, __opClass),101             instMnem(_instMnem)102         {103             const int ChunkSize = sizeof(unsigned long);104             const int Chunks = sizeof(setFlags) / ChunkSize;105 106             // Since the bitset constructor can only handle unsigned long107             // sized chunks, feed it those one at a time while oring them in.108             for (int i = 0; i &lt; Chunks; i++) {109                 unsigned shift = i * ChunkSize * 8;110                 flags |= (std::bitset&lt;Num_Flags&gt;(setFlags &gt;&gt; shift) &lt;&lt; shift);111             }112         }113 114         std::string generateDisassembly(Addr pc,115                 const SymbolTable *symtab) const116         {117             std::stringstream ss;118 119             ccprintf(ss, \"\\t%s.%s\", instMnem, mnemonic);120 121             return ss.str();122         }123 124         bool checkCondition(uint64_t flags, int condition) const;125 126         void127         advancePC(PCState &amp;pcState) const128         {129             if (flags[IsLastMicroop])130                 pcState.uEnd();131             else132                 pcState.uAdvance();133         }134     };Based on whether it is the last microop, it invokes different function of the PCState, UEnd and uAdvance respectively. Here the pcState object is the architecture specific PCState object defined as below.PCState classgem5/src/arch/x86/types.hh289     class PCState : public GenericISA::UPCState&lt;MachInst&gt;290     {291       protected:292         typedef GenericISA::UPCState&lt;MachInst&gt; Base;......324         void325         advance()326         {327             Base::advance();328             _size = 0;329         }Because the PCState doesn’t implement the uEnd and uAdvance function, we should take a look at its parent class,GenericISA::UPCState.gem5/src/arch/generic/types.hh193 // A PC and microcode PC.194 template &lt;class MachInst&gt;195 class UPCState : public SimplePCState&lt;MachInst&gt;196 {197   protected:198     typedef SimplePCState&lt;MachInst&gt; Base;199 200     MicroPC _upc;201     MicroPC _nupc;202 203   public:204 205     MicroPC upc() const { return _upc; }206     void upc(MicroPC val) { _upc = val; }207 208     MicroPC nupc() const { return _nupc; }209     void nupc(MicroPC val) { _nupc = val; }......228     bool229     branching() const230     {231         return this-&gt;npc() != this-&gt;pc() + sizeof(MachInst) ||232                this-&gt;nupc() != this-&gt;upc() + 1;233     }234 235     // Advance the upc within the instruction.236     void237     uAdvance()238     {239         _upc = _nupc;240         _nupc++;241     }242 243     // End the macroop by resetting the upc and advancing the regular pc.244     void245     uEnd()246     {247         this-&gt;advance();248         _upc = 0;249         _nupc = 1;250     }When uAdvance function is invoked, it just updates the _upc member fieldrepresenting the micro pc of the current hardware thread. However, when the uEnd is invoked, it should update the pc instead of the micro pc (upc).Because UPCState doesn’t implement the PC related member fields and functions,it invokes the advance function of its parent, SimplePCState. Note that PC represents microop, and upc represent instruction pointer among the microops consisting of one macroop.gem5/src/arch/generic/types.hh139 // The most basic type of PC.140 template &lt;class MachInst&gt;141 class SimplePCState : public PCStateBase142 {143   protected:144     typedef PCStateBase Base;145 146   public:147 148     Addr pc() const { return _pc; }149     void pc(Addr val) { _pc = val; }150 151     Addr npc() const { return _npc; }152     void npc(Addr val) { _npc = val; }153 154     void155     set(Addr val)156     {157         pc(val);158         npc(val + sizeof(MachInst));159     };160 161     void162     setNPC(Addr val)163     {164         npc(val);165     }166 167     SimplePCState() {}168     SimplePCState(Addr val) { set(val); }169 170     bool171     branching() const172     {173         return this-&gt;npc() != this-&gt;pc() + sizeof(MachInst);174     }175 176     // Advance the PC.177     void178     advance()179     {180         _pc = _npc;181         _npc += sizeof(MachInst);182     }183 };It just updates the _pc as the _npc which was as a result of adding size of macroop instruction to the current pc. In other words, if it is not a control flow instruction, just adding the size of current instruction to the pc is enough to get the next pc address.Asking branch predictor for a control flow instructionNow let’s go back to the rest of the lookupAndUpdateNextPC function to understandwhat happens if the current instruction turns out to be control flow instruction. 572  573     ThreadID tid = inst-&gt;threadNumber; 574     predict_taken = branchPred-&gt;predict(inst-&gt;staticInst, inst-&gt;seqNum, 575                                         nextPC, tid); 576  577     if (predict_taken) { 578         DPRINTF(Fetch, \"[tid:%i] [sn:%llu] Branch at PC %#x \" 579                 \"predicted to be taken to %s\\n\", 580                 tid, inst-&gt;seqNum, inst-&gt;pcState().instAddr(), nextPC); 581     } else { 582         DPRINTF(Fetch, \"[tid:%i] [sn:%llu] Branch at PC %#x \" 583                 \"predicted to be not taken\\n\", 584                 tid, inst-&gt;seqNum, inst-&gt;pcState().instAddr()); 585     } 586  587     DPRINTF(Fetch, \"[tid:%i] [sn:%llu] Branch at PC %#x \" 588             \"predicted to go to %s\\n\", 589             tid, inst-&gt;seqNum, inst-&gt;pcState().instAddr(), nextPC); 590     inst-&gt;setPredTarg(nextPC); 591     inst-&gt;setPredTaken(predict_taken); 592  593     ++fetchedBranches; 594  595     if (predict_taken) { 596         ++predictedBranches; 597     } 598  599     return predict_taken; 600 }It invokes the predict function and store the return value to the predict_taken.The predict function returns the prediction result, whether the branching instruction should be taken or not-taken (when it is not a control flow instruction, it returns not-taken to allow the next following instructions to be executed sequentially). Also, note that the reference of the nextPC is passed to the branch predictor. This is because the prediction affects the next instruction’s address.Therefore, based on the prediction result, it changes the nextPC to make the fetch stage to fetch instructions from the proper location.End of the second loop1356 1357             newMacro |= thisPC.instAddr() != nextPC.instAddr();1358 Remind that we are currently executing the second loop to translate curMacroop to microops. However, when one of its microop turns out to be a control flow instructionand is predicted to be taken, it should change the PC.For that purpose, it checks the PC addresses of the thisPC and nextPC.Previously, before invoking the lookupAndUpdateNextPC function,it has allocated the thisPC to the nextPC (line 1346). However, when the prediction made as a taken,pc address of the nextPC will be changed to the location of the taken branch. Therefore, by comparing pc addresses of nextPC and thisPC, we can understand that whether we are facing another macroop or still executing the microops of the current macroop (line 1357). 53 // The guaranteed interface. 54 class PCStateBase : public Serializable 55 { 56   protected: 57     Addr _pc; 58     Addr _npc; 59  60     PCStateBase() : _pc(0), _npc(0) {} 61     PCStateBase(Addr val) : _pc(0), _npc(0) { set(val); } 62  63   public: 64     /** 65      * Returns the memory address the bytes of this instruction came from. 66      * 67      * @return Memory address of the current instruction's encoding. 68      */ 69     Addr 70     instAddr() const 71     { 72         return _pc; 73     } 74  75     /** 76      * Returns the memory address the bytes of the next instruction came from. 77      * 78      * @return Memory address of the next instruction's encoding. 79      */ 80     Addr 81     nextInstAddr() const 82     { 83         return _npc; 84     } 85  86     /** 87      * Returns the current micropc. 88      * 89      * @return The current micropc. 90      */ 91     MicroPC 92     microPC() const 93     { 94         return 0; 95     }After the newMacro flag has been set,it assigns the nextPC to the thisPC. One might think that nextPC will equal to the thisPCwhen the branch prediction is made to be not-taken, but the lookupAndUpdateNextPC advances micro-pc by invokingadvancePC function when the instruction is not a control flow or predicted as not-taken. ```cpp1359             // Move to the next instruction, unless we have a branch.1360             thisPC = nextPC;1361             inRom = isRomMicroPC(thisPC.microPC());1362 1363             if (newMacro) {1364                 fetchAddr = thisPC.instAddr() &amp; BaseCPU::PCMask;1365                 blkOffset = (fetchAddr - fetchBufferPC[tid]) / instSize;1366                 pcOffset = 0;1367                 curMacroop = NULL;1368             }1369 1370             if (instruction-&gt;isQuiesce()) {1371                 DPRINTF(Fetch,1372                         \"Quiesce instruction encountered, halting fetch!\\n\");1373                 fetchStatus[tid] = QuiescePending;1374                 status_change = true;1375                 quiesce = true;1376                 break;1377             }1378         } while ((curMacroop || decoder[tid]-&gt;instReady()) &amp;&amp;1379                  numInst &lt; fetchWidth &amp;&amp;1380                  fetchQueue[tid].size() &lt; fetchQueueSize);13811382         // Re-evaluate whether the next instruction to fetch is in micro-op ROM1383         // or not.If the newMacro flag is set to true, then it should updates the addresses required to fetch next instructionand set the curMacroop as NULL.Therefore, when the new macroop is found, the second loop will exit and try to continue executing the first loop.End of the first loop and rest1263     // Loop through instruction memory from the cache.1264     // Keep issuing while fetchWidth is available and branch is not1265     // predicted taken1266     while (numInst &lt; fetchWidth &amp;&amp; fetchQueue[tid].size() &lt; fetchQueueSize1267            &amp;&amp; !predictedBranch &amp;&amp; !quiesce) {......1382         // Re-evaluate whether the next instruction to fetch is in micro-op ROM1383         // or not.1384         inRom = isRomMicroPC(thisPC.microPC());1385     }After translating macroop to microops by executing the second loop,it should continue execution on the first loop. As we checked before,when the number of fetched instruction does not exceed the fetchWidth (bandwidth)and fetchQueue does not overflow and branch prediction is not made, it will continue the all the logic that we checked until now will be repeated. Then what should be done when the first loop exits?13861387     if (predictedBranch) {1388         DPRINTF(Fetch, \"[tid:%i] Done fetching, predicted branch \"1389                 \"instruction encountered.\\n\", tid);1390     } else if (numInst &gt;= fetchWidth) {1391         DPRINTF(Fetch, \"[tid:%i] Done fetching, reached fetch bandwidth \"1392                 \"for this cycle.\\n\", tid);1393     } else if (blkOffset &gt;= fetchBufferSize) {1394         DPRINTF(Fetch, \"[tid:%i] Done fetching, reached the end of the\"1395                 \"fetch buffer.\\n\", tid);1396     }1397 1398     macroop[tid] = curMacroop;1399     fetchOffset[tid] = pcOffset;First it prints out debugging messages based on the exit condition of the first loop.And then it updates the macroop of the current hardware thread with the curMacroop. Also the fetchOffset will be updated with pcOffset.1400 1401     if (numInst &gt; 0) {1402         wroteToTimeBuffer = true;1403     }1404 1405     pc[tid] = thisPC;1406 1407     // pipeline a fetch if we're crossing a fetch buffer boundary and not in1408     // a state that would preclude fetching1409     fetchAddr = (thisPC.instAddr() + pcOffset) &amp; BaseCPU::PCMask;1410     Addr fetchBufferBlockPC = fetchBufferAlignPC(fetchAddr);1411     issuePipelinedIfetch[tid] = fetchBufferBlockPC != fetchBufferPC[tid] &amp;&amp;1412         fetchStatus[tid] != IcacheWaitResponse &amp;&amp;1413         fetchStatus[tid] != ItlbWait &amp;&amp;1414         fetchStatus[tid] != IcacheWaitRetry &amp;&amp;1415         fetchStatus[tid] != QuiescePending &amp;&amp;1416         !curMacroop;1417 }Rest of the tick function of the fetch.Issuing the Icache access for split access?\\XXX{TODO} 936     // Record number of instructions fetched this cycle for distribution. 937     fetchNisnDist.sample(numInst); 938  939     if (status_change) { 940         // Change the fetch stage status if there was a status change. 941         _status = updateFetchStatus(); 942     } 943  944     // Issue the next I-cache request if possible. 945     for (ThreadID i = 0; i &lt; numThreads; ++i) { 946         if (issuePipelinedIfetch[i]) { 947             pipelineIcacheAccesses(i); 948         } 949     } 950  951     // Send instructions enqueued into the fetch queue to decode. 952     // Limit rate by fetchWidth.  Stall if decode is stalled. 953     unsigned insts_to_decode = 0; 954     unsigned available_insts = 0; 955  956     for (auto tid : *activeThreads) { 957         if (!stalls[tid].decode) { 958             available_insts += fetchQueue[tid].size(); 959         } 960     }Sending fetched instructions to decode stagegem5/src/cpu/o3/fetch_impl.hh 961  962     // Pick a random thread to start trying to grab instructions from 963     auto tid_itr = activeThreads-&gt;begin(); 964     std::advance(tid_itr, random_mt.random&lt;uint8_t&gt;(0, activeThreads-&gt;size() - 1)); 965  966     while (available_insts != 0 &amp;&amp; insts_to_decode &lt; decodeWidth) { 967         ThreadID tid = *tid_itr; 968         if (!stalls[tid].decode &amp;&amp; !fetchQueue[tid].empty()) { 969             const auto&amp; inst = fetchQueue[tid].front(); 970             toDecode-&gt;insts[toDecode-&gt;size++] = inst; 971             DPRINTF(Fetch, \"[tid:%i] [sn:%llu] Sending instruction to decode \" 972                     \"from fetch queue. Fetch queue size: %i.\\n\", 973                     tid, inst-&gt;seqNum, fetchQueue[tid].size()); 974  975             wroteToTimeBuffer = true; 976             fetchQueue[tid].pop_front(); 977             insts_to_decode++; 978             available_insts--; 979         } 980  981         tid_itr++; 982         // Wrap around if at end of active threads list 983         if (tid_itr == activeThreads-&gt;end()) 984             tid_itr = activeThreads-&gt;begin(); 985     } 986  987     // If there was activity this cycle, inform the CPU of it. 988     if (wroteToTimeBuffer) { 989         DPRINTF(Activity, \"Activity this cycle.\\n\"); 990         cpu-&gt;activityThisCycle(); 991     } 992  993     // Reset the number of the instruction we've fetched. 994     numInst = 0; 995 }   //end of the fetch.tickThe last job of the fetch stage is passing the fetched instructionsto the next stage, decode stage. One the above code, toDecode member field of the fetch is used as an storage located in between the fetch and decode stage.FetchStruct: passing fetch stage’s information to decode stagegem5/src/cpu/o3/fetch.hh431     //Might be annoying how this name is different than the queue.432     /** Wire used to write any information heading to decode. */433     typename TimeBuffer&lt;FetchStruct&gt;::wire toDecode;......458     /** Source of possible stalls. */459     struct Stalls {460         bool decode;461         bool drain;462     };463 464     /** Tracks which stages are telling fetch to stall. */465     Stalls stalls[Impl::MaxThreads];The toDecode is declared as a wire class defined in the TimeBuffer class. Also, because the TimerBuffer is a template class, it passes the FetchStruct that contains all fetch stage’s informationrequired by the decode stage. Let’s take a look at the FetchStruct to understand which information is passed to the decode stage.gem5/src/cpu/o3/cpu_policy.hh 60 template&lt;class Impl&gt; 61 struct SimpleCPUPolicy 62 { ...... 89     /** The struct for communication between fetch and decode. */ 90     typedef DefaultFetchDefaultDecode&lt;Impl&gt; FetchStruct; 91  92     /** The struct for communication between decode and rename. */ 93     typedef DefaultDecodeDefaultRename&lt;Impl&gt; DecodeStruct; 94  95     /** The struct for communication between rename and IEW. */ 96     typedef DefaultRenameDefaultIEW&lt;Impl&gt; RenameStruct; 97  98     /** The struct for communication between IEW and commit. */ 99     typedef DefaultIEWDefaultCommit&lt;Impl&gt; IEWStruct;100 101     /** The struct for communication within the IEW stage. */102     typedef ::IssueStruct&lt;Impl&gt; IssueStruct;103 104     /** The struct for all backwards communication. */105     typedef TimeBufStruct&lt;Impl&gt; TimeStruct;gem5/src/cpu/o3/comm.h 55 /** Struct that defines the information passed from fetch to decode. */ 56 template&lt;class Impl&gt; 57 struct DefaultFetchDefaultDecode { 58     typedef typename Impl::DynInstPtr DynInstPtr; 59  60     int size; 61  62     DynInstPtr insts[Impl::MaxWidth]; 63     Fault fetchFault; 64     InstSeqNum fetchFaultSN; 65     bool clearFetchFault; 66 };Most importantly, it passes the instructions fetched from the Icache.TimeBuffer::wire generic class representing wireThe information passed from the decode stage to fetch stage is represented as multiple wires conveying bits of information. For that purpose, GEM5 provides wire class. 39 template &lt;class T&gt; 40 class TimeBuffer 41 { 42   protected: 43     int past; 44     int future; 45     unsigned size; 46     int _id; 47  48     char *data; 49     std::vector&lt;char *&gt; index; 50     unsigned base; 51  52     void valid(int idx) const 53     { 54         assert (idx &gt;= -past &amp;&amp; idx &lt;= future); 55     } 56  57   public: 58     friend class wire; 59     class wire 60     { 61         friend class TimeBuffer; 62       protected: 63         TimeBuffer&lt;T&gt; *buffer; 64         int index; 65  66         void set(int idx) 67         {    68             buffer-&gt;valid(idx); 69             index = idx; 70         } 71  72         wire(TimeBuffer&lt;T&gt; *buf, int i) 73             : buffer(buf), index(i) 74         { } 75  76       public: 77         wire() 78         { } 79  80         wire(const wire &amp;i) 81             : buffer(i.buffer), index(i.index) 82         { } 83  84         const wire &amp;operator=(const wire &amp;i) 85         { 86             buffer = i.buffer; 87             set(i.index); 88             return *this; 89         } 90  91         const wire &amp;operator=(int idx) 92         { 93             set(idx); 94             return *this; 95         } 96  97         const wire &amp;operator+=(int offset) 98         { 99             set(index + offset);100             return *this;101         }102 103         const wire &amp;operator-=(int offset)104         {105             set(index - offset);106             return *this;107         }108 109         wire &amp;operator++()110         {111             set(index + 1);112             return *this;113         }114 115         wire &amp;operator++(int)116         {117             int i = index;118             set(index + 1);119             return wire(this, i);120         }121 122         wire &amp;operator--()123         {124             set(index - 1);125             return *this;126         }127 128         wire &amp;operator--(int)129         {130             int i = index;131             set(index - 1);132             return wire(this, i);133         }134         T &amp;operator*() const { return *buffer-&gt;access(index); }135         T *operator-&gt;() const { return buffer-&gt;access(index); }136     };......192   protected:193     //Calculate the index into this-&gt;index for element at position idx194     //relative to now195     inline int calculateVectorIndex(int idx) const196     {197         //Need more complex math here to calculate index.198         valid(idx);199 200         int vector_index = idx + base;201         if (vector_index &gt;= (int)size) {202             vector_index -= size;203         } else if (vector_index &lt; 0) {204             vector_index += size;205         }206 207         return vector_index;208     }209 210   public:211     T *access(int idx)212     {213         int vector_index = calculateVectorIndex(idx);214 215         return reinterpret_cast&lt;T *&gt;(index[vector_index]);216     }As shown in the Line 970 of the tick function of the fetch stage,it references insts member field through the -&gt; operator. Because toDecode is declared as the TimeBuffer::wire, and this class overrides the -&gt; operator, it will invoke the`operator function shown in line 135. \\XXX{ it needs to be explained more clearly with smartpointer..}"
  },
  
  {
    "title": "O3 Cpu Gem5",
    "url": "/posts/O3-CPU-GEM5/",
    "categories": "GEM5, O3",
    "tags": "",
    "date": "2021-05-26 00:00:00 -0400",
    





    
    "snippet": "Python derives O3CPU through DerivO3CPU classTo understand particular processor in the GEM5, it is easy to start from the script that instantiate the processor. We can easily find that lots of GEM5...",
    "content": "Python derives O3CPU through DerivO3CPU classTo understand particular processor in the GEM5, it is easy to start from the script that instantiate the processor. We can easily find that lots of GEM5 provided default script utilize DerivO3CPU to attach the O3 CPU to the system.gem5/src/cpu/o3/O3CPU.py 61 class DerivO3CPU(BaseCPU): 62     type = 'DerivO3CPU' 63     cxx_header = 'cpu/o3/deriv.hh' 64  65     @classmethod 66     def memory_mode(cls): 67         return 'timing' 68  69     @classmethod 70     def require_caches(cls): 71         return True 72  73     @classmethod 74     def support_take_over(cls): 75         return True 76  77     activity = Param.Unsigned(0, \"Initial count\") 78  79     cacheStorePorts = Param.Unsigned(200, \"Cache Ports. \" 80           \"Constrains stores only.\") 81     cacheLoadPorts = Param.Unsigned(200, \"Cache Ports. \" 82           \"Constrains loads only.\") 83  84     decodeToFetchDelay = Param.Cycles(1, \"Decode to fetch delay\") 85     renameToFetchDelay = Param.Cycles(1 ,\"Rename to fetch delay\") 86     iewToFetchDelay = Param.Cycles(1, \"Issue/Execute/Writeback to fetch \" 87                                    \"delay\") 88     commitToFetchDelay = Param.Cycles(1, \"Commit to fetch delay\") 89     fetchWidth = Param.Unsigned(8, \"Fetch width\") 90     fetchBufferSize = Param.Unsigned(64, \"Fetch buffer size in bytes\") 91     fetchQueueSize = Param.Unsigned(32, \"Fetch queue size in micro-ops \" 92                                     \"per-thread\")DerivO3CPU is used to instantiate the O3CPU in the runscript of the GEM5.Similar to other m5 objects of the processors, it also inherits from the BaseCPU m5 class. Also it sets the parameters of the O3CPU which will be accessed by the DerivO3CPUParams later in the CPP implementation of this class.gem5/src/cpu/o3/derive.hh#ifndef __CPU_O3_DERIV_HH__#define __CPU_O3_DERIV_HH__#include \"cpu/o3/cpu.hh\"#include \"cpu/o3/impl.hh\"#include \"params/DerivO3CPU.hh\"class DerivO3CPU : public FullO3CPU&lt;O3CPUImpl&gt;{  public:    DerivO3CPU(DerivO3CPUParams *p)        : FullO3CPU&lt;O3CPUImpl&gt;(p)    { }};#endif // __CPU_O3_DERIV_HH__Contrary to my expectation, the DerivO3CPU class doesn’t have any definitions to emulate the O3CPU, but just inherits from the FullO3CPU with O3CPUImpl for the class template instantiation. Therefore, we can reasonably guess that all the implementations are done by the FullO3CPU. Before we go deep down, let’s take a look at the class hierarchies of this CPU.gem5/src/o3/cpu.hh83 class BaseO3CPU : public BaseCPU 84 { 85     //Stuff that's pretty ISA independent will go here. 86   public: 87     BaseO3CPU(BaseCPUParams *params); 88  89     void regStats(); 90 }; 91  92 /** 93  * FullO3CPU class, has each of the stages (fetch through commit) 94  * within it, as well as all of the time buffers between stages.  The 95  * tick() function for the CPU is defined here. 96  */ 97 template &lt;class Impl&gt; 98 class FullO3CPU : public BaseO3CPU 99 {100   public:101     // Typedefs from the Impl here.102     typedef typename Impl::CPUPol CPUPolicy;103     typedef typename Impl::DynInstPtr DynInstPtr;104     typedef typename Impl::O3CPU O3CPU;105 106     using VecElem =  TheISA::VecElem;107     using VecRegContainer =  TheISA::VecRegContainer;108 109     using VecPredRegContainer = TheISA::VecPredRegContainer;110 111     typedef O3ThreadState&lt;Impl&gt; ImplState;112     typedef O3ThreadState&lt;Impl&gt; Thread;113 114     typedef typename std::list&lt;DynInstPtr&gt;::iterator ListIt;115 116     friend class O3ThreadContext&lt;Impl&gt;;......The FullO3CPU is the main CPU class for the O3 CPU. We can find that this FullO3CPU inherits BaseO3CPU inheriting BaseCPU. In GEM5, all the CPU classes basically inherits the BaseCPU class. Remember that the DerivO3CPU also inherits from the BaseCPU m5 object,which generates the proper interfaces to access the parameters of the DerivO3CPU and BaseCPU in the CPP implementation of those classes. To understand the relationship of those two classes, it would be good to take a look at the constructor of those two classes.gem5/src/o3/cpu.cc  82 BaseO3CPU::BaseO3CPU(BaseCPUParams *params)  83     : BaseCPU(params)  84 {  85 }......  93 template &lt;class Impl&gt;  94 FullO3CPU&lt;Impl&gt;::FullO3CPU(DerivO3CPUParams *params)  95     : BaseO3CPU(params),  96       itb(params-&gt;itb),  97       dtb(params-&gt;dtb),  98       tickEvent([this]{ tick(); }, \"FullO3CPU tick\",  99                 false, Event::CPU_Tick_Pri), 100       threadExitEvent([this]{ exitThreads(); }, \"FullO3CPU exit threads\", 101                 false, Event::CPU_Exit_Pri), 102 #ifndef NDEBUG 103       instcount(0), 104 #endif 105       removeInstsThisCycle(false), 106       fetch(this, params), 107       decode(this, params), 108       rename(this, params), 109       iew(this, params), 110       commit(this, params), 111  112       /* It is mandatory that all SMT threads use the same renaming mode as 113        * they are sharing registers and rename */ 114       vecMode(RenameMode&lt;TheISA::ISA&gt;::init(params-&gt;isa[0])), 115       regFile(params-&gt;numPhysIntRegs, 116               params-&gt;numPhysFloatRegs, 117               params-&gt;numPhysVecRegs, 118               params-&gt;numPhysVecPredRegs, 119               params-&gt;numPhysCCRegs, 120               vecMode), 121  122       freeList(name() + \".freelist\", &amp;regFile), 123  124       rob(this, params), 125  126       scoreboard(name() + \".scoreboard\", 127                  regFile.totalNumPhysRegs()), 128  129       isa(numThreads, NULL), 130  131       timeBuffer(params-&gt;backComSize, params-&gt;forwardComSize), 132       fetchQueue(params-&gt;backComSize, params-&gt;forwardComSize), 133       decodeQueue(params-&gt;backComSize, params-&gt;forwardComSize), 134       renameQueue(params-&gt;backComSize, params-&gt;forwardComSize), 135       iewQueue(params-&gt;backComSize, params-&gt;forwardComSize), 136       activityRec(name(), NumStages, 137                   params-&gt;backComSize + params-&gt;forwardComSize, 138                   params-&gt;activity), 139  140       globalSeqNum(1), 141       system(params-&gt;system), 142       lastRunningCycle(curCycle())Note that FullO3CPU passes the params to the BaseO3CPU which further passes the paramsto the BaseCPU’s constructor. Remember that all the processors on the GEM5 should implement the BaseCPU in addition to their additional semantic. Therefore, after FullO3CPU first initializes its member field using the passed parameters, it should pass the parameter to the BaseCPU to finish base processor configurations. One interesting thing to note is that FullO3CPU is a template class which adopts another class called Impl that should be replaced with proper class to be used. Therefore, DerivO3CPU inherits the FullO3CPU not the FullO3CPU alone. Let's take a look at what is the O3CPUImpl.gem5/src/cpu/o3/impl.hh 38 // Forward declarations. 39 template &lt;class Impl&gt; 40 class BaseO3DynInst; 41  42 template &lt;class Impl&gt; 43 class FullO3CPU; 44  45 /** Implementation specific struct that defines several key types to the 46  *  CPU, the stages within the CPU, the time buffers, and the DynInst. 47  *  The struct defines the ISA, the CPU policy, the specific DynInst, the 48  *  specific O3CPU, and all of the structs from the time buffers to do 49  *  communication. 50  *  This is one of the key things that must be defined for each hardware 51  *  specific CPU implementation. 52  */ 53 struct O3CPUImpl 54 { 55     /** The type of MachInst. */ 56     typedef TheISA::MachInst MachInst; 57  58     /** The CPU policy to be used, which defines all of the CPU stages. */ 59     typedef SimpleCPUPolicy&lt;O3CPUImpl&gt; CPUPol; 60  61     /** The DynInst type to be used. */ 62     typedef BaseO3DynInst&lt;O3CPUImpl&gt; DynInst; 63  64     /** The refcounted DynInst pointer to be used.  In most cases this is 65      *  what should be used, and not DynInst *. 66      */ 67     typedef RefCountingPtr&lt;DynInst&gt; DynInstPtr; 68     typedef RefCountingPtr&lt;const DynInst&gt; DynInstConstPtr; 69  70     /** The O3CPU type to be used. */ 71     typedef FullO3CPU&lt;O3CPUImpl&gt; O3CPU; 72  73     /** Same typedef, but for CPUType.  BaseDynInst may not always use 74      * an O3 CPU, so it's clearer to call it CPUType instead in that 75      * case. 76      */ 77     typedef O3CPU CPUType; 78  79     enum { 80       MaxWidth = 8, 81       MaxThreads = 4 82     }; 83 };GEM5 defines structure called O3CPUImpl that instantiates all template classes associated with O3 CPU. One of the instantiated template class is FullO3CPU (Line 71).By instantiating the FullO3CPU class with the O3CPUImpl class, it defines complete FullO3CPU class called O3CPU. The O3CPU type will be used later frequently to indicate the O3CPU in the other parts of the O3CPU implementations. Also, other uncompleted class templates are instantiated with the O3CPUImpl class.Let’s revisit the FullO3CPU class once again to take a look at how the O3CPUImpl class will be utilized as an Impl template.gem5/src/o3/cpu.hh 97 template &lt;class Impl&gt; 98 class FullO3CPU : public BaseO3CPU 99 {100   public:101     // Typedefs from the Impl here.102     typedef typename Impl::CPUPol CPUPolicy;103     typedef typename Impl::DynInstPtr DynInstPtr;104     typedef typename Impl::O3CPU O3CPU;......558   protected:559     /** The fetch stage. */560     typename CPUPolicy::Fetch fetch;561 562     /** The decode stage. */563     typename CPUPolicy::Decode decode;564 565     /** The dispatch stage. */566     typename CPUPolicy::Rename rename;567 568     /** The issue/execute/writeback stages. */569     typename CPUPolicy::IEW iew;570 571     /** The commit stage. */572     typename CPUPolicy::Commit commit;First of all, it defines new typenames by utilizing the member field of the Impl class. Note that those are also typedef of some classes retrieved by instantiating specific class templates defined for O3 CPU. For example, CPUPolicy is set an alias of Impl::CPUPol which is atypedef of SimpleCPUPolicy defined in the O3CPUImpl.Therefore, in short, CPUPolicy equals SimpleCPUPolicy. Those types in the FullO3CPU class will be used in declaring pipeline stages of the O3CPU. As shown in the line 558-572, it utilizes the CPUPolicy type to define different pipeline stages (e.g., fetch, decode).Therefore, to understand the each stage of the pipeline of the O3CPU, we should take a look at the SimpleCPUPolicy class.gem5/src/cpu/o3/cpu_policy.hh 51 /** 52  * Struct that defines the key classes to be used by the CPU.  All 53  * classes use the typedefs defined here to determine what are the 54  * classes of the other stages and communication buffers.  In order to 55  * change a structure such as the IQ, simply change the typedef here 56  * to use the desired class instead, and recompile.  In order to 57  * create a different CPU to be used simultaneously with this one, see 58  * the alpha_impl.hh file for instructions. 59  */ 60 template&lt;class Impl&gt; 61 struct SimpleCPUPolicy 62 { 63     /** Typedef for the freelist of registers. */ 64     typedef UnifiedFreeList FreeList; 65     /** Typedef for the rename map. */ 66     typedef UnifiedRenameMap RenameMap; 67     /** Typedef for the ROB. */ 68     typedef ::ROB&lt;Impl&gt; ROB; 69     /** Typedef for the instruction queue/scheduler. */ 70     typedef InstructionQueue&lt;Impl&gt; IQ; 71     /** Typedef for the memory dependence unit. */ 72     typedef ::MemDepUnit&lt;StoreSet, Impl&gt; MemDepUnit; 73     /** Typedef for the LSQ. */ 74     typedef ::LSQ&lt;Impl&gt; LSQ; 75     /** Typedef for the thread-specific LSQ units. */ 76     typedef ::LSQUnit&lt;Impl&gt; LSQUnit; 77  78     /** Typedef for fetch. */ 79     typedef DefaultFetch&lt;Impl&gt; Fetch; 80     /** Typedef for decode. */ 81     typedef DefaultDecode&lt;Impl&gt; Decode; 82     /** Typedef for rename. */ 83     typedef DefaultRename&lt;Impl&gt; Rename; 84     /** Typedef for Issue/Execute/Writeback. */ 85     typedef DefaultIEW&lt;Impl&gt; IEW; 86     /** Typedef for commit. */ 87     typedef DefaultCommit&lt;Impl&gt; Commit; 88  89     /** The struct for communication between fetch and decode. */ 90     typedef DefaultFetchDefaultDecode&lt;Impl&gt; FetchStruct; 91  92     /** The struct for communication between decode and rename. */ 93     typedef DefaultDecodeDefaultRename&lt;Impl&gt; DecodeStruct; 94  95     /** The struct for communication between rename and IEW. */ 96     typedef DefaultRenameDefaultIEW&lt;Impl&gt; RenameStruct; 97  98     /** The struct for communication between IEW and commit. */ 99     typedef DefaultIEWDefaultCommit&lt;Impl&gt; IEWStruct;100 101     /** The struct for communication within the IEW stage. */102     typedef ::IssueStruct&lt;Impl&gt; IssueStruct;103 104     /** The struct for all backwards communication. */105     typedef TimeBufStruct&lt;Impl&gt; TimeStruct;106 107 };In the above code, I can find that each stage is defined as an instantiation of one class template. For example, Fetch type is defined as an DefaultFetchand Decode type is defined as an DefaultDecode. Remember that we are currently looking at this class because of the instantiation, SimpleCPUPolicy.Therefore, DefaultDecode can be translated into DefaultDecode.In the FullO3CPU, instead of implementing entire pipeline in one class, it embeds different classes' instances implementing each part of the O3CPU's pipeline. Therefore, to understand the fetch part of the O3CPU, we should take a look at the DefaultFetch class.Fetch of the O3CPUTick 529 template &lt;class Impl&gt; 530 void 531 FullO3CPU&lt;Impl&gt;::tick() 532 { 533     DPRINTF(O3CPU, \"\\n\\nFullO3CPU: Ticking main, FullO3CPU.\\n\"); 534     assert(!switchedOut()); 535     assert(drainState() != DrainState::Drained); 536  537     ++numCycles; 538     updateCycleCounters(BaseCPU::CPU_STATE_ON); 539  540 //    activity = false; 541  542     //Tick each of the stages 543     fetch.tick(); 544  545     decode.tick(); 546  547     rename.tick(); 548  549     iew.tick(); 550  551     commit.tick(); 552  553     // Now advance the time buffers 554     timeBuffer.advance(); 555  556     fetchQueue.advance(); 557     decodeQueue.advance(); 558     renameQueue.advance(); 559     iewQueue.advance(); 560  561     activityRec.advance(); 562  563     if (removeInstsThisCycle) { 564         cleanUpRemovedInsts(); 565     } 566  567     if (!tickEvent.scheduled()) { 568         if (_status == SwitchedOut) { 569             DPRINTF(O3CPU, \"Switched out!\\n\"); 570             // increment stat 571             lastRunningCycle = curCycle(); 572         } else if (!activityRec.active() || _status == Idle) { 573             DPRINTF(O3CPU, \"Idle!\\n\"); 574             lastRunningCycle = curCycle(); 575             timesIdled++; 576         } else { 577             schedule(tickEvent, clockEdge(Cycles(1))); 578             DPRINTF(O3CPU, \"Scheduling next tick!\\n\"); 579         } 580     } 581  582     if (!FullSystem) 583         updateThreadPriority(); 584  585     tryDrain(); 586 }"
  },
  
  {
    "title": "Usb Device Add",
    "url": "/posts/usb-device-add/",
    "categories": "linux,, embedded-linux",
    "tags": "",
    "date": "2021-05-07 00:00:00 -0400",
    





    
    "snippet": "USB drivers are designed for interfacesFor USB device,the drivers are not designed for the USB device itself,but for its interfaces provided by the USB devices. Therefore, when a new USB device is ...",
    "content": "USB drivers are designed for interfacesFor USB device,the drivers are not designed for the USB device itself,but for its interfaces provided by the USB devices. Therefore, when a new USB device is plugged in to the usb-bus,the first thing needs to done by the usb core system is enumerating configuration, interfaces, and endponints supported by the plugged device.However, in the Linux usb-subsystem, whether it is a physical usb device or its interfaces,they are all handles as a usb deviceand processed by a single unified APIs to register the devices.In this posting,we will see how the Interface driver is designedand registered to the usb-bus,and how those usb drivers can be probed and bound later when the new usb device is plugged in.Registering USB interface driver/** * module_usb_driver() - Helper macro for registering a USB driver * @__usb_driver: usb_driver struct *  * Helper macro for USB drivers which do not do anything special in module * init/exit. This eliminates a lot of boilerplate. Each module may only * use this macro once, and calling it replaces module_init() and module_exit() */#define module_usb_driver(__usb_driver) \\        module_driver(__usb_driver, usb_register, \\                       usb_deregister)To register interface usb driver,you can utilize the module_usb_driver macro that automatically implements the initialization function of the interface driver.This initialiation function only contains the usb_register macrothat invokes usb_register_driver function.This usb_register_driver function actually registers the interface driver.The detailed explanation about this function was described in previous posting.How the USB sub-system matches the device to its associated driver?Whether the device is a real usb device or its interface,both should be registered on the usb-bus.Therefore, when the device and interfaces are added to the usb-bus,its probe function, usb_device_match function will be invoked after the device is registered.static int usb_device_match(struct device *dev, struct device_driver *drv){        /* devices and interfaces are handled separately */        if (is_usb_device(dev)) {                struct usb_device *udev;                struct usb_device_driver *udrv;                /* interface drivers never match devices */                if (!is_usb_device_driver(drv))                        return 0;                udev = to_usb_device(dev);                udrv = to_usb_device_driver(drv);                /* If the device driver under consideration does not have a                 * id_table or a match function, then let the driver's probe                 * function decide.                 */                if (!udrv-&gt;id_table &amp;&amp; !udrv-&gt;match)                        return 1;                return usb_driver_applicable(udev, udrv);        } else if (is_usb_interface(dev)) {                struct usb_interface *intf;                struct usb_driver *usb_drv;                const struct usb_device_id *id;                /* device drivers never match interfaces */                if (is_usb_device_driver(drv))                        return 0;                intf = to_usb_interface(dev);                usb_drv = to_usb_driver(drv);                id = usb_match_id(intf, usb_drv-&gt;id_table);                if (id)                        return 1;                id = usb_match_dynamic_id(intf, usb_drv);                if (id)                        return 1;        }        return 0;}The match function has different behavior depending on whether it is usb device or usb interface.Note that the device object passed to the probe functionis not a usb or interface specific device structure, buta generic device structure representing any device.Device type for USB systemstatic inline int is_usb_device(const struct device *dev){        return dev-&gt;type == &amp;usb_device_type;}static inline int is_usb_interface(const struct device *dev){        return dev-&gt;type == &amp;usb_if_device_type;}Because the device structure is not only used for representing the USB device and its interface device,but also for any other devices in the linux driver sub-system,it it not a good way to add another member field for distinguishing USB interface and driver.Instead of adding another field,usb sub-system utilize the type field of the device structure./* * The type of device, \"struct device\" is embedded in. A class * or bus can contain devices of different types * like \"partitions\" and \"disks\", \"mouse\" and \"event\". * This identifies the device type and carries type-specific * information, equivalent to the kobj_type of a kobject. * If \"name\" is specified, the uevent will contain it in * the DEVTYPE variable. */struct device_type {        const char *name;        const struct attribute_group **groups;        int (*uevent)(struct device *dev, struct kobj_uevent_env *env);        char *(*devnode)(struct device *dev, umode_t *mode,                         kuid_t *uid, kgid_t *gid);        void (*release)(struct device *dev);        const struct dev_pm_ops *pm;};struct device_type usb_device_type = {        .name =         \"usb_device\",        .release =      usb_release_dev,        .uevent =       usb_dev_uevent,        .devnode =      usb_devnode,#ifdef CONFIG_PM        .pm =           &amp;usb_device_pm_ops,#endif  };                                struct device_type usb_if_device_type = {        .name =         \"usb_interface\",        .release =      usb_release_interface,        .uevent =       usb_if_uevent,}; As shown in the above code,each USB subsystem utilize the two different types of device usb_device_type and usb_if_device_type.Actually there are two more types that are not discovered in this posting,type for USB endpoints and USB port. We will not cover them because they are not related to binding the driver to the hot-plugged USB device and its interface.Driver type for USB driversstatic inline int is_usb_device_driver(struct device_driver *drv){                               return container_of(drv, struct usbdrv_wrap, driver)-&gt;                        for_devices;}   You might remember the usb_register_device_driver(&amp;usb_generic_driver, THIS_MODULE) function invocationin the usb_init function in the previous blog posting.At the first glance, it is hard to understand how is that function is different form the usb_register_driver function used for registering the interface drivers.Most of the time you will utilize the usb_register_driverthrough the usb_register macrobecause registering interface driver for your USB interfaces is your first priority jobto use your USB device on the Linux system.When you compare the two function line by line,you can easily find that for_devices field is only setwhen the driver is registered through the usb_register_device_driver. And also you can find that different probe function is assigned to the driver attached to the usb-bus.Now the secret can be revealed.The is_usb_device_driver checks whether the for_device has been set,which means the current driver passed to this macro is designed for managing entire USB devices on the USB subsystem. From the name of the driver and its probe functionyou can also infer its purpose,usb_generic_driver and usb_generic_driver_match.The importance of this macro is you can always select the device driver for the USB device when the USB device wants to be registered to the USB bus.We will soon see how this macro is used for the USB matching function.USB interface device utilizes id_table        } else if (is_usb_interface(dev)) {                struct usb_interface *intf;                struct usb_driver *usb_drv;                const struct usb_device_id *id;                /* device drivers never match interfaces */                if (is_usb_device_driver(drv))                        return 0;                intf = to_usb_interface(dev);                usb_drv = to_usb_driver(drv);                id = usb_match_id(intf, usb_drv-&gt;id_table);                if (id)                        return 1;                id = usb_match_dynamic_id(intf, usb_drv);                if (id)                        return 1;        }Although the USB device should be matched and probed firstbefore the interface device can be populated,we will take a look at how the match function workswhen the interface device has been passed.This is because USB interface always utilize the id_table mechanism for matching the interface to its supporting driver,but the USB device may or may not contain a id_table and deferthe matching to the later probing.Anyway, first of all, to figure out whether the current device is USB interface device,it invokes is_usb_device macro.when the device is turned out to be interface device,then it checks current device driver passed to the match functionis an interface driver not the USB device driver with the help of isb_usb_device_driver macro.When those conditionals are passed,then the interface object can be retrieved from the device structure.The contain_of macro can do magic here;because the device structure is embedded in the interface object.When the target interface is retrieved,it invokes usb_match_id function./** * usb_match_id - find first usb_device_id matching device or interface * @interface: the interface of interest * @id: array of usb_device_id structures, terminated by zero entry * * usb_match_id searches an array of usb_device_id's and returns * the first one matching the device or interface, or null. * This is used when binding (or rebinding) a driver to an interface. * Most USB device drivers will use this indirectly, through the usb core, * but some layered driver frameworks use it directly.const struct usb_device_id *usb_match_id(struct usb_interface *interface,                                         const struct usb_device_id *id){        /* proc_connectinfo in devio.c may call us with id == NULL. */        if (id == NULL)                return NULL;        /* It is important to check that id-&gt;driver_info is nonzero,           since an entry that is all zeroes except for a nonzero           id-&gt;driver_info is the way to create an entry that           indicates that the driver want to examine every           device and interface. */        for (; id-&gt;idVendor || id-&gt;idProduct || id-&gt;bDeviceClass ||               id-&gt;bInterfaceClass || id-&gt;driver_info; id++) {                if (usb_match_one_id(interface, id))                        return id;        }        return NULL;}/* returns 0 if no match, 1 if match */int usb_match_one_id(struct usb_interface *interface,                     const struct usb_device_id *id){        struct usb_host_interface *intf;        struct usb_device *dev;        /* proc_connectinfo in devio.c may call us with id == NULL. */        if (id == NULL)                return 0;        intf = interface-&gt;cur_altsetting;        dev = interface_to_usbdev(interface);        if (!usb_match_device(dev, id))                return 0;        return usb_match_one_id_intf(dev, intf, id);}/* returns 0 if no match, 1 if match */int usb_match_device(struct usb_device *dev, const struct usb_device_id *id){        if ((id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_VENDOR) &amp;&amp;            id-&gt;idVendor != le16_to_cpu(dev-&gt;descriptor.idVendor))                return 0;        if ((id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_PRODUCT) &amp;&amp;            id-&gt;idProduct != le16_to_cpu(dev-&gt;descriptor.idProduct))                return 0;        /* No need to test id-&gt;bcdDevice_lo != 0, since 0 is never           greater than any unsigned number. */        if ((id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_DEV_LO) &amp;&amp;            (id-&gt;bcdDevice_lo &gt; le16_to_cpu(dev-&gt;descriptor.bcdDevice)))                return 0;        if ((id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_DEV_HI) &amp;&amp;            (id-&gt;bcdDevice_hi &lt; le16_to_cpu(dev-&gt;descriptor.bcdDevice)))                return 0;        if ((id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_DEV_CLASS) &amp;&amp;            (id-&gt;bDeviceClass != dev-&gt;descriptor.bDeviceClass))                return 0;        if ((id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_DEV_SUBCLASS) &amp;&amp;            (id-&gt;bDeviceSubClass != dev-&gt;descriptor.bDeviceSubClass))                return 0;        if ((id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_DEV_PROTOCOL) &amp;&amp;            (id-&gt;bDeviceProtocol != dev-&gt;descriptor.bDeviceProtocol))                return 0;        return 1;}/* returns 0 if no match, 1 if match */int usb_match_one_id_intf(struct usb_device *dev,                          struct usb_host_interface *intf,                          const struct usb_device_id *id){        /* The interface class, subclass, protocol and number should never be         * checked for a match if the device class is Vendor Specific,         * unless the match record specifies the Vendor ID. */        if (dev-&gt;descriptor.bDeviceClass == USB_CLASS_VENDOR_SPEC &amp;&amp;                        !(id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_VENDOR) &amp;&amp;                        (id-&gt;match_flags &amp; (USB_DEVICE_ID_MATCH_INT_CLASS |                                USB_DEVICE_ID_MATCH_INT_SUBCLASS |                                USB_DEVICE_ID_MATCH_INT_PROTOCOL |                                USB_DEVICE_ID_MATCH_INT_NUMBER)))                return 0;                if ((id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_INT_CLASS) &amp;&amp;            (id-&gt;bInterfaceClass != intf-&gt;desc.bInterfaceClass))                return 0;                        if ((id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_INT_SUBCLASS) &amp;&amp;            (id-&gt;bInterfaceSubClass != intf-&gt;desc.bInterfaceSubClass))                return 0;                if ((id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_INT_PROTOCOL) &amp;&amp;            (id-&gt;bInterfaceProtocol != intf-&gt;desc.bInterfaceProtocol))                return 0;        if ((id-&gt;match_flags &amp; USB_DEVICE_ID_MATCH_INT_NUMBER) &amp;&amp;            (id-&gt;bInterfaceNumber != intf-&gt;desc.bInterfaceNumber))                return 0;                return 1;}The first for loop in the usb_match_one_id function invokes usb_match_one_id with every USB device &amp; interface informationstored in the device_id table until the match happens.Each entry in the id_table not only contains information of the USB device itself associated with current interface, but also the information of each interface.invokes usb_match_one_id function with each interface information.Therefore, it need to check whether current interface has been populated by a USB device supported by the matching driver. The usb_match_device functioncheck USB device information stored in the usb_device structure associated with current interface driver.Because the interface driver has a reference of the USB device that populated current interface,it can retrieve the reference to the parent USB device.And because the USB device contains its USB device specific informationsuch as vendorID, productID,based on the match_flags of the id,it checks whether the match occurs.If the matching for the device information is done, then it invokes usb_match_one_id_intf function to further matchthe interface specific information.When all those conditional statmenets are passed in two functions,it means that the current USB information stored in the Nth location of the id_table,it returns 1 to the usb_match_id function.USB device needs to be handled by the USB device driver        if (is_usb_device(dev)) {                struct usb_device *udev;                struct usb_device_driver *udrv;                /* interface drivers never match devices */                if (!is_usb_device_driver(drv))                        return 0;                udev = to_usb_device(dev);                udrv = to_usb_device_driver(drv);                /* If the device driver under consideration does not have a                 * id_table or a match function, then let the driver's probe                 * function decide.                 */                if (!udrv-&gt;id_table &amp;&amp; !udrv-&gt;match)                        return 1;                return usb_driver_applicable(udev, udrv);When the current device used for matching is USB device not an interface,known by the is_usb_device macro,it first checks the current driver is the USB device driver.Because usb_init function registers only one USB device driver,usb_generic_driver,until that driver is found, it will keep returning 0.When the USB device driver is found,it first checks whether the driver has id_table and match callback function both(remind that current matching function isa callback match function of the usb_bus_type).When both of them doesn’t exist,it just returns value 1.And because usb_generic_driver doesn’t have match function and id_table fieldit will return 1 and defer all the further job to its probe function,usb_generic_driver_probe.After matching, invoke probe of the matching driverNote that based on the USB device type,USB device or USB interface,different probe function will be invoked.USB device is handled by the USB device driverFor a USB device,it always matches with a USB device driver, usb_generic_driver.Therefore, when the probe function is invoked as a result of successful matching,it always invokes the usb_generic_driver_probe function.int usb_generic_driver_probe(struct usb_device *udev){        int err, c;        /* Choose and set the configuration.  This registers the interfaces         * with the driver core and lets interface drivers bind to them.         */        if (udev-&gt;authorized == 0)                dev_err(&amp;udev-&gt;dev, \"Device is not authorized for usage\\n\");        else {                c = usb_choose_configuration(udev);                if (c &gt;= 0) {                        err = usb_set_configuration(udev, c);                        if (err &amp;&amp; err != -ENODEV) {                                dev_err(&amp;udev-&gt;dev, \"can't set config #%d, error %d\\n\",                                        c, err);                                /* This need not be fatal.  The user can try to                                 * set other configurations. */                        }                }        }        /* USB device state == configured ... usable */        usb_notify_add_device(udev);        return 0;}There are two important functions inside the probe function:usb_choose_configuration and usb_set_configuration.The first function chooses the best suitable configurationfrom the availables ones supported by the USB device.The details will not be convered in this posting.When the configuration is chosen, it returns the index of configuration.With the index, it invokes usb_set_configuration.Currently, we only have USB device provided configuration informationstored in the descriptor.Therefore, we should set the selected configurationto allow Linux USB subsystem can manage it for further operations.usb_set_configuration, populating the interface devicesBecause the usb_set_configuration is too complex to take a look at its entire operation in this posting,we will study only fractions of it related to the interface device allocationand binding to the driver.int usb_set_configuration(struct usb_device *dev, int configuration){        int i, ret;        struct usb_host_config *cp = NULL;        struct usb_interface **new_interfaces = NULL;        struct usb_hcd *hcd = bus_to_hcd(dev-&gt;bus);        int n, nintf;        if (dev-&gt;authorized == 0 || configuration == -1)                configuration = 0;        else {                  for (i = 0; i &lt; dev-&gt;descriptor.bNumConfigurations; i++) {                        if (dev-&gt;config[i].desc.bConfigurationValue ==                                        configuration) {                                cp = &amp;dev-&gt;config[i];                                break;                        }                }        }        if ((!cp &amp;&amp; configuration != 0))                return -EINVAL;        /* The USB spec says configuration 0 means unconfigured.         * But if a device includes a configuration numbered 0,         * we will accept it as a correctly configured state.         * Use -1 if you really want to unconfigure the device.         */        if (cp &amp;&amp; configuration == 0)                dev_warn(&amp;dev-&gt;dev, \"config 0 descriptor??\\n\");        /* Allocate memory for new interfaces before doing anything else,         * so that if we run out then nothing will have changed. */        n = nintf = 0;        if (cp) {                nintf = cp-&gt;desc.bNumInterfaces;                new_interfaces = kmalloc_array(nintf, sizeof(*new_interfaces),                                               GFP_NOIO);                if (!new_interfaces)                        return -ENOMEM;                                for (; n &lt; nintf; ++n) {                        new_interfaces[n] = kzalloc(                                        sizeof(struct usb_interface),                                        GFP_NOIO);                        if (!new_interfaces[n]) {                                ret = -ENOMEM;free_interfaces:                                                while (--n &gt;= 0)                                        kfree(new_interfaces[n]);                                kfree(new_interfaces);                                return ret;                        }                }                                i = dev-&gt;bus_mA - usb_get_max_power(dev, cp);                if (i &lt; 0)                        dev_warn(&amp;dev-&gt;dev, \"new config #%d exceeds power \"                                        \"limit by %dmA\\n\",                                        configuration, -i);        }Because the configuration number has been chosen by the usb_choose_configuration function,we can understand which configuration of the USB device should be enabled.Because usb_device can maintain all configurations provided by the USB device,it first choose the selected configuration based on the configuration index parameter.And then it allocates the interfaces supported by the selected configuration.Each configuration can support different number of interfaces,so based on the bNumInterfaces filed of the descriptor of the selected configuration,it allocates memory for the interfaces.After the memory space for interfaces are allocated,it needs to be initialized.        /*         * Initialize the new interface structures and the         * hc/hcd/usbcore interface/endpoint state.         */        for (i = 0; i &lt; nintf; ++i) {                struct usb_interface_cache *intfc;                struct usb_interface *intf;                struct usb_host_interface *alt;                u8 ifnum;                cp-&gt;interface[i] = intf = new_interfaces[i];                intfc = cp-&gt;intf_cache[i];                intf-&gt;altsetting = intfc-&gt;altsetting;                intf-&gt;num_altsetting = intfc-&gt;num_altsetting;                intf-&gt;authorized = !!HCD_INTF_AUTHORIZED(hcd);                kref_get(&amp;intfc-&gt;ref);                alt = usb_altnum_to_altsetting(intf, 0);                /* No altsetting 0?  We'll assume the first altsetting.                 * We could use a GetInterface call, but if a device is                 * so non-compliant that it doesn't have altsetting 0                 * then I wouldn't trust its reply anyway.                 */                if (!alt)                        alt = &amp;intf-&gt;altsetting[0];                ifnum = alt-&gt;desc.bInterfaceNumber;                intf-&gt;intf_assoc = find_iad(dev, cp, ifnum);                intf-&gt;cur_altsetting = alt;                usb_enable_interface(dev, intf, true);                intf-&gt;dev.parent = &amp;dev-&gt;dev;                if (usb_of_has_combined_node(dev)) {                        device_set_of_node_from_dev(&amp;intf-&gt;dev, &amp;dev-&gt;dev);                } else {                        intf-&gt;dev.of_node = usb_of_get_interface_node(dev,                                        configuration, ifnum);                }                ACPI_COMPANION_SET(&amp;intf-&gt;dev, ACPI_COMPANION(&amp;dev-&gt;dev));                intf-&gt;dev.driver = NULL;                intf-&gt;dev.bus = &amp;usb_bus_type;                intf-&gt;dev.type = &amp;usb_if_device_type;                intf-&gt;dev.groups = usb_interface_groups;                INIT_WORK(&amp;intf-&gt;reset_ws, __usb_queue_reset_device);                intf-&gt;minor = -1;                device_initialize(&amp;intf-&gt;dev);                pm_runtime_no_callbacks(&amp;intf-&gt;dev);                dev_set_name(&amp;intf-&gt;dev, \"%d-%s:%d.%d\", dev-&gt;bus-&gt;busnum,                                dev-&gt;devpath, configuration, ifnum);                usb_get_dev(dev);        }        kfree(new_interfaces);Here, the nintf means the number of interfaces should be supported for the selected configuration.Each interface is set to beattached to the usb_bus_type bus,and has usb_if_device_type as its type(seen in the match function before).        /* Now that all the interfaces are set up, register them         * to trigger binding of drivers to interfaces.  probe()         * routines may install different altsettings and may         * claim() any interfaces not yet bound.  Many class drivers         * need that: CDC, audio, video, etc.         */        for (i = 0; i &lt; nintf; ++i) {                struct usb_interface *intf = cp-&gt;interface[i];                if (intf-&gt;dev.of_node &amp;&amp;                    !of_device_is_available(intf-&gt;dev.of_node)) {                        dev_info(&amp;dev-&gt;dev, \"skipping disabled interface %d\\n\",                                 intf-&gt;cur_altsetting-&gt;desc.bInterfaceNumber);                        continue;                }                dev_dbg(&amp;dev-&gt;dev,                        \"adding %s (config #%d, interface %d)\\n\",                        dev_name(&amp;intf-&gt;dev), configuration,                        intf-&gt;cur_altsetting-&gt;desc.bInterfaceNumber);                device_enable_async_suspend(&amp;intf-&gt;dev);                ret = device_add(&amp;intf-&gt;dev);                if (ret != 0) {                        dev_err(&amp;dev-&gt;dev, \"device_add(%s) --&gt; %d\\n\",                                dev_name(&amp;intf-&gt;dev), ret);                        continue;                }                create_intf_ep_devs(intf);        }After the interface has been initialized,it should be registered to the usb bus to be utilized.Note that driver field of the interface device is passed to the device_add function.Because this device is set to be attached to the usb_bus_type bus,the same match function we’ve analyzed before will be invoked once again,usb_device_match.This time, the device passed to the usb_device_match is the interface,it will traverse entire registered interface drivers and tries to find the driver supporting current interface device.Note that the id_table will be used for matching.After the interface device matches with a specific interface driver,it will end up calling the probe function of the driver.Note that the probe function called at this time is not aprobe function of the USB device driver,which led us to populate the interface devices.This time the probe function registered in the matched device driver will be invoked instead!Yeah~! now your USB device interface is bound to a driver and can be managed by the Linux from now on."
  },
  
  {
    "title": "Clks",
    "url": "/posts/clks/",
    "categories": "linux,, embedded-linux",
    "tags": "",
    "date": "2021-05-07 00:00:00 -0400",
    





    
    "snippet": "Dwc3 probingstatic int dwc3_apple_m1_probe(struct platform_device *pdev){               struct dwc3_apple       *da;         struct device           *dev = &amp;pdev-&gt;dev;        struct device_n...",
    "content": "Dwc3 probingstatic int dwc3_apple_m1_probe(struct platform_device *pdev){               struct dwc3_apple       *da;         struct device           *dev = &amp;pdev-&gt;dev;        struct device_node      *np = dev-&gt;of_node;                int                     ret;                da = devm_kzalloc(dev, sizeof(*da), GFP_KERNEL);        if (!da)                return -ENOMEM;                platform_set_drvdata(pdev, da);        da-&gt;dev = dev;                da-&gt;atcphy = of_iomap(np, 0);        da-&gt;usbcore = of_iomap(np, 1);        if (!da-&gt;atcphy || !da-&gt;usbcore) {                pr_err(\"%pOFn: %s: failed to map MMIO ranges.\", np, __func__);                return -EINVAL;        }                ret = clk_bulk_get_all(da-&gt;dev, &amp;da-&gt;clks);        if (ret &lt; 0) {                pr_err(\"%pOFn: %s: clk_bulk_get_all failed.\", np, __func__);                return ret;        }                da-&gt;num_clocks = ret;        ret = clk_bulk_prepare_enable(da-&gt;num_clocks, da-&gt;clks);        if (ret) {                pr_err(\"%pOFn: %s: clk_bulk_prepare_enable failed.\", np, __func__);                return ret;        }                ret = dwc3_apple_m1_start(da);        if(ret) {                pr_err(\"%pOFn: %s: failed to prepare hardware: %d.\", np, __func__, ret);                return ret;        }                ret = of_platform_populate(np, NULL, NULL, dev);        if (ret)                goto err_clk_put;                pm_runtime_set_active(dev);        pm_runtime_enable(dev);        pm_runtime_get_sync(dev);                return 0;err_clk_put:        clk_bulk_disable_unprepare(da-&gt;num_clocks, da-&gt;clks);        clk_bulk_put_all(da-&gt;num_clocks, da-&gt;clks);                return ret;}TODO: more…Adding clock to the system in Linux Kernel with Device Tree    refclk24mhz: refclk24mhz {        compatible = \"fixed-clock\";        #clock-cells = &lt;0&gt;;        clock-frequency = &lt;24000000&gt;;        clock-output-names = \"refclk24mhz\";    };        atc0_common: atc0_common@23b700420 {            compatible = \"apple,pmgr-clk-gate\";            #clock-cells = &lt;0&gt;;            reg = &lt;0x2 0x3b700420 0x0 0x8&gt;;            clocks = &lt;&amp;refclk24mhz&gt;;            clock-output-names = \"atc0_common\";        };        atc0_usb_aon: atc0_usb_aon@23d280088 {            compatible = \"apple,pmgr-clk-gate\";            #clock-cells = &lt;0&gt;;            reg = &lt;0x2 0x3d280088 0x0 0x8&gt;;            clocks = &lt;&amp;atc0_common&gt;;            clock-output-names = \"atc0_usb_aon\";        };        atc0_usb: atc0_usb@23d280098 {            compatible = \"apple,pmgr-clk-gate\";            #clock-cells = &lt;0&gt;;            reg = &lt;0x2 0x3d280098 0x0 0x8&gt;;            clocks = &lt;&amp;atc0_usb_aon&gt;;            clock-output-names = \"atc0_usb\";        };When we take a look at the atc0_usb clock device,we can find that it has another clock phandle as its clocks property.When we go all the way up to the root clock device node,we can see that 24mhz fixed clock device is theroot device of multiple atc0 related clocks.Because fixed-clock is not in the interest of this posting,we will take a look at how the device driver associated with apple clock device handles those clock nodes in the device tree.static int clk_apple_pmgr_driver_probe(struct platform_device *pdev){        struct device_node *node = pdev-&gt;dev.of_node;        const char **parent_names = NULL;        unsigned num_parents, type;        struct clk *clk;        void __iomem *bases[MAX_BASES] = { NULL };        int i, n, err;        unsigned seqn[4] = { 0 }, *seq[4] = { NULL };        static const char * const seqname[4] = { \"pre-down\", \"pre-up\", \"post-down\", \"post-up\" };        struct clk_apple_pmgr *clk_apple_pmgr;        struct clk_init_data init = {};        struct clk_hw *hw;        num_parents = of_clk_get_parent_count(node);        if(num_parents) {                parent_names = devm_kcalloc(&amp;pdev-&gt;dev, num_parents, sizeof(char *), GFP_KERNEL);                if(!parent_names)                        return -ENOMEM;                of_clk_parent_fill(node, parent_names, num_parents);        }        n = of_property_count_elems_of_size(node, \"reg\", sizeof(uint32_t) * (of_n_addr_cells(node) + of_n_size_cells(node)));        if(n &lt; 0) {                pr_err(\"%pOFn: %s: not enough MMIO ranges.\\n\", node, __func__);                return -EINVAL;        }        if(n &gt; MAX_BASES) {                pr_err(\"%pOFn: %s: too many MMIO ranges.\\n\", node, __func__);                return -EINVAL;        }        for(i=0; i&lt;n; i++) {                bases[i] = of_iomap(node, i);                if(!bases[i]) {                        pr_err(\"%pOFn: %s: unable to map MMIO range %d.\\n\", node, __func__, i);                        return -EINVAL;                }        }        for(i=0; i&lt;4; i++) {                n = of_property_count_elems_of_size(node, seqname[i], sizeof(uint32_t));                if(n &gt; 0) {                        seq[i] = devm_kcalloc(&amp;pdev-&gt;dev, n, sizeof(unsigned), GFP_KERNEL);                        if(!seq[i])                                return -ENOMEM;                        seqn[i] = of_property_read_variable_u32_array(node, seqname[i], seq[i], 0, n) / 4;                }        }The first part of the probe function retrieves the reg values of the clock.of_property_count_elems_of_size function searches device tree nodesand returns how many reg property has values on its list.After the number has been figured out,it maps the value of reg properties to the kernel memoryusing the of_iomap function.Because reg property contains the physical addresses that maps the device,of_iomap function traverse the reg property list and maps the addresses. All the mapped virtual addresses are stored in the bases array.In addition to the reg property,there can be clk related properties specified in the seqname array.When those properties exist and has some values,it should also be parsed and its string value should be translated to unsigned 32bit integer values(of_property_read_variable_u32_array).struct clk_apple_pmgr {        struct clk_hw hw;        unsigned type;        const char *name;        void __iomem *bases[MAX_BASES];        const unsigned *seq[4];        unsigned seqn[4];        u32 freq_target;};          type = 0;        if(of_device_is_compatible(node, \"apple,pmgr-clk-gate\"))                type = PMGR_GATE;        clk_apple_pmgr = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(*clk_apple_pmgr), GFP_KERNEL);        if(!clk_apple_pmgr)                return -ENOMEM;        clk_apple_pmgr-&gt;type = type;        for(i=0; i&lt;MAX_BASES; i++)                clk_apple_pmgr-&gt;bases[i] = bases[i];        for(i=0; i&lt;4; i++) {                clk_apple_pmgr-&gt;seq[i] = seq[i];                clk_apple_pmgr-&gt;seqn[i] = seqn[i];        }        clk_apple_pmgr-&gt;name = node-&gt;name;        init.name = node-&gt;name;        init.parent_names = parent_names;        init.num_parents = num_parents;The next part is not that interesting, just allocating instance of clk_apple_pmgrand assigns values read from the device tree to the allocated object.        switch(type) {        case PMGR_GATE:                err = clk_prepare_apple_pmgr_gate(clk_apple_pmgr, &amp;init, &amp;pdev-&gt;dev, node, parent_names, num_parents, bases);                break;        default:                pr_err(\"%pOFn: %s: unsupported device type\\n\", node, __func__);                return -EINVAL;        }        if(err)                return err;        clk_apple_pmgr-&gt;hw.init = &amp;init;        hw = &amp;clk_apple_pmgr-&gt;hw;        err = devm_clk_hw_register(&amp;pdev-&gt;dev, hw);        if(err)                return err;        clk = hw-&gt;clk;        return of_clk_add_provider(node, of_clk_src_simple_get, clk);}The last part is the most important part, initializing the clock and registering to the linux kernel.First, take a look at how the clock can be initiliazed./** * struct clk_init_data - holds init data that's common to all clocks and is * shared between the clock provider and the common clock framework. * * @name: clock name * @ops: operations this clock supports * @parent_names: array of string names for all possible parents * @parent_data: array of parent data for all possible parents (when some *               parents are external to the clk controller) * @parent_hws: array of pointers to all possible parents (when all parents *              are internal to the clk controller) * @num_parents: number of possible parents * @flags: framework-level hints and quirks */struct clk_init_data {        const char              *name;        const struct clk_ops    *ops;        /* Only one of the following three should be assigned */        const char              * const *parent_names;        const struct clk_parent_data    *parent_data;        const struct clk_hw             **parent_hws;        u8                      num_parents;        unsigned long           flags;};static int clk_prepare_apple_pmgr_gate(struct clk_apple_pmgr *clk_apple_pmgr, struct clk_init_data *init, struct device *dev, struct device_node *node, const char * const *parent_names, u8 num_parents, void __iomem **bases){        init-&gt;ops = &amp;clk_apple_pmgr_gate_ops;        init-&gt;flags = CLK_SET_RATE_PARENT;        return 0;}Actually, initilization function doesn’t do much for pmgr_gate clock.for gate type clock, it must implement at least three functionsspeicified in the clk_apple_pmgr_gate_ops.It just assigns the clock operations to the init structure and set flags. And the filled-out init structure object is assigned to the clk_hw structure’s memeber field init.Now we are going to check the most important clock api, clk_hw_register generating clock and of_clk_add_provider registering generated clock to the linux system.First take a look at how the clk_hw_register function can populate the clock and register it to clock tree./**      * devm_clk_hw_register - resource managed clk_hw_register() * @dev: device that is registering this clock * @hw: link to hardware-specific clock data *       * Managed clk_hw_register(). Clocks registered by this function are * automatically clk_hw_unregister()ed on driver detach. See clk_hw_register() * for more information. */     int devm_clk_hw_register(struct device *dev, struct clk_hw *hw){               struct clk_hw **hwp;        int ret;                        hwp = devres_alloc(devm_clk_hw_unregister_cb, sizeof(*hwp), GFP_KERNEL);        if (!hwp)                return -ENOMEM;                ret = clk_hw_register(dev, hw);        if (!ret) {                *hwp = hw;                devres_add(dev, hwp);        } else {                devres_free(hwp);        }                return ret;}EXPORT_SYMBOL_GPL(devm_clk_hw_register);/** * clk_hw_register - register a clk_hw and return an error code * @dev: device that is registering this clock * @hw: link to hardware-specific clock data * * clk_hw_register is the primary interface for populating the clock tree with * new clock nodes. It returns an integer equal to zero indicating success or * less than zero indicating failure. Drivers must test for an error code after * calling clk_hw_register(). */int clk_hw_register(struct device *dev, struct clk_hw *hw){        return PTR_ERR_OR_ZERO(__clk_register(dev, dev_or_parent_of_node(dev),                               hw));}static struct clk *__clk_register(struct device *dev, struct device_node *np, struct clk_hw *hw){        int ret;        struct clk_core *core;        const struct clk_init_data *init = hw-&gt;init;        /*         * The init data is not supposed to be used outside of registration path.         * Set it to NULL so that provider drivers can't use it either and so that         * we catch use of hw-&gt;init early on in the core.         */        hw-&gt;init = NULL;        core = kzalloc(sizeof(*core), GFP_KERNEL);        if (!core) {                ret = -ENOMEM;                goto fail_out;        }        core-&gt;name = kstrdup_const(init-&gt;name, GFP_KERNEL);        if (!core-&gt;name) {                ret = -ENOMEM;                goto fail_name;        }        if (WARN_ON(!init-&gt;ops)) {                ret = -EINVAL;                goto fail_ops;        }        core-&gt;ops = init-&gt;ops;        if (dev &amp;&amp; pm_runtime_enabled(dev))                core-&gt;rpm_enabled = true;        core-&gt;dev = dev;        core-&gt;of_node = np;        if (dev &amp;&amp; dev-&gt;driver)                core-&gt;owner = dev-&gt;driver-&gt;owner;        core-&gt;hw = hw;        core-&gt;flags = init-&gt;flags;        core-&gt;num_parents = init-&gt;num_parents;        core-&gt;min_rate = 0;        core-&gt;max_rate = ULONG_MAX;        hw-&gt;core = core;        ret = clk_core_populate_parent_map(core, init);        if (ret)                goto fail_parents;        INIT_HLIST_HEAD(&amp;core-&gt;clks);As you can see in the above function,the init structure we set in the clk_hw object is used to fill out the new core structure object.After finishing assigning information to core object, it should be assigned to the clk_hw object.        /*         * Don't call clk_hw_create_clk() here because that would pin the         * provider module to itself and prevent it from ever being removed.         */        hw-&gt;clk = alloc_clk(core, NULL, NULL);        if (IS_ERR(hw-&gt;clk)) {                ret = PTR_ERR(hw-&gt;clk);                goto fail_create_clk;        }        clk_core_link_consumer(hw-&gt;core, hw-&gt;clk);        ret = __clk_core_init(core);        if (!ret)                return hw-&gt;clk;        clk_prepare_lock();        clk_core_unlink_consumer(hw-&gt;clk);        clk_prepare_unlock();        free_clk(hw-&gt;clk);        hw-&gt;clk = NULL;fail_create_clk:        clk_core_free_parent_map(core);fail_parents:fail_ops:        kfree_const(core-&gt;name);fail_name:        kfree(core);fail_out:        return ERR_PTR(ret);}Now fianlly, this function allocates a clock object using alloc_clk function.By passing the core structure, it will assign the object to the generated clk object.At the time of clk object has been allocated,clk object, core object, and clk_hw objects will have a pointers connecting each other.Also __clk_core_inti function checkswhether the core operations has been properly set, and if they are set correctly, they initialize the left fieldssuch as frequency using the provided operations.Adding generated clock as providerAfter generating all required fields for clock structure such as core and clk_hw,we need to register the generated clock to the system. Let’s revisit how the of_clk_add_provider is invoked in the probe function of apple-pmgr device driverand check out how this function finish clock registration properly./**        clk = hw-&gt;clk;        return of_clk_add_provider(node, of_clk_src_simple_get, clk); * of_clk_add_provider() - Register a clock provider for a node * @np: Device node pointer associated with clock provider * @clk_src_get: callback for decoding clock * @data: context pointer for @clk_src_get callback. * * This function is *deprecated*. Use of_clk_add_hw_provider() instead. */int of_clk_add_provider(struct device_node *np,                        struct clk *(*clk_src_get)(struct of_phandle_args *clkspec,                                                   void *data),                        void *data){        struct of_clk_provider *cp;        int ret;        cp = kzalloc(sizeof(*cp), GFP_KERNEL);        if (!cp)                return -ENOMEM;        cp-&gt;node = of_node_get(np);        cp-&gt;data = data;        cp-&gt;get = clk_src_get;        mutex_lock(&amp;of_clk_mutex);        list_add(&amp;cp-&gt;link, &amp;of_clk_providers);        mutex_unlock(&amp;of_clk_mutex);        pr_debug(\"Added clock from %pOF\\n\", np);        clk_core_reparent_orphans();        ret = of_clk_set_defaults(np, true);        if (ret &lt; 0)                of_clk_del_provider(np);        return ret;}EXPORT_SYMBOL_GPL(of_clk_add_provider);Note that we passed clk object to the of_clk_add_provider function as its data argument.This data is actual clock object we created through previous steps. And also, it requires clk_src_get function which will be invoked later to retireve the registered clock object by the consumer. Actually registering our clock object to system is just assigning the of_clk_provider structure that embracesnot only the clock object we created, but also the method to retrieve the clock object to the global list called of_clk_providers.Later this list will be searched to find out the clock objectwhenever the other device node requiring the pre-configured clocks.Utilizing registered clock resource in other devicesdriver/clk/clk-bulk.cint __must_check clk_bulk_get_all(struct device *dev,                                  struct clk_bulk_data **clks){        struct device_node *np = dev_of_node(dev);        if (!np)                return 0;        return of_clk_bulk_get_all(np, clks);}EXPORT_SYMBOL(clk_bulk_get_all);static int __must_check of_clk_bulk_get_all(struct device_node *np,                                            struct clk_bulk_data **clks){        struct clk_bulk_data *clk_bulk;        int num_clks;        int ret;        num_clks = of_clk_get_parent_count(np);        if (!num_clks)                return 0;        clk_bulk = kmalloc_array(num_clks, sizeof(*clk_bulk), GFP_KERNEL);        if (!clk_bulk)                return -ENOMEM;        ret = of_clk_bulk_get(np, num_clks, clk_bulk);        if (ret) {                kfree(clk_bulk);                return ret;        }        *clks = clk_bulk;        return num_clks;}static int __must_check of_clk_bulk_get(struct device_node *np, int num_clks,                                        struct clk_bulk_data *clks){        int ret;        int i;        for (i = 0; i &lt; num_clks; i++) {                clks[i].id = NULL;                clks[i].clk = NULL;        }        for (i = 0; i &lt; num_clks; i++) {                of_property_read_string_index(np, \"clock-names\", i, &amp;clks[i].id);                clks[i].clk = of_clk_get(np, i);                if (IS_ERR(clks[i].clk)) {                        ret = PTR_ERR(clks[i].clk);                        pr_err(\"%pOF: Failed to get clk index: %d ret: %d\\n\",                               np, i, ret);                        clks[i].clk = NULL;                        goto err;                }        }        return 0;err:        clk_bulk_put(i, clks);        return ret;}The most important part of the above function is of_clk_get which retrieves the clk structureassociated with current device. Remember that the clk structure should be registered before the device make use of it.struct clk *of_clk_get(struct device_node *np, int index){        return __of_clk_get(np, index, np-&gt;full_name, NULL);}       EXPORT_SYMBOL(of_clk_get);static struct clk *__of_clk_get(struct device_node *np,                                int index, const char *dev_id,                                const char *con_id){        struct clk_hw *hw = of_clk_get_hw(np, index, con_id);                                                    return clk_hw_create_clk(NULL, hw, dev_id, con_id);}               To retrieve the clk structure,it first needs to have an access to the clk_hw structure which is embedded in the clk structure that we want.struct clk_hw *of_clk_get_hw(struct device_node *np, int index,                             const char *con_id){                       int ret;        struct clk_hw *hw;        struct of_phandle_args clkspec;                                ret = of_parse_clkspec(np, index, con_id, &amp;clkspec);        if (ret)                        return ERR_PTR(ret);                        hw = of_clk_get_hw_from_clkspec(&amp;clkspec);        of_node_put(clkspec.np);                return hw;}Locating the clock nodeThe below sequences of function tries to locate the clock property of your device node first.Because clock property contains one or more clocks as the format for phandles, first we need to find a phandle associated with the clock devicethat we want to retrieve. To do that, we can pass either index or con_id used to find the phandle that we want to retrieve. After the phandle is found,because phandle is also a device node for the clock,we can retrieve the associated device node of that clock source.This clock resource is going to be stored in the clkspec./** * of_parse_clkspec() - Parse a DT clock specifier for a given device node * @np: device node to parse clock specifier from * @index: index of phandle to parse clock out of. If index &lt; 0, @name is used * @name: clock name to find and parse. If name is NULL, the index is used * @out_args: Result of parsing the clock specifier * * Parses a device node's \"clocks\" and \"clock-names\" properties to find the * phandle and cells for the index or name that is desired. The resulting clock * specifier is placed into @out_args, or an errno is returned when there's a * parsing error. The @index argument is ignored if @name is non-NULL. * * Example: * * phandle1: clock-controller@1 { *      #clock-cells = &lt;2&gt;; * } * * phandle2: clock-controller@2 { *      #clock-cells = &lt;1&gt;; * } * * clock-consumer@3 { *      clocks = &lt;&amp;phandle1 1 2 &amp;phandle2 3&gt;; *      clock-names = \"name1\", \"name2\"; * } * * To get a device_node for `clock-controller@2' node you may call this * function a few different ways: * *   of_parse_clkspec(clock-consumer@3, -1, \"name2\", &amp;args); *   of_parse_clkspec(clock-consumer@3, 1, NULL, &amp;args); *   of_parse_clkspec(clock-consumer@3, 1, \"name2\", &amp;args); * * Return: 0 upon successfully parsing the clock specifier. Otherwise, -ENOENT * if @name is NULL or -EINVAL if @name is non-NULL and it can't be found in * the \"clock-names\" property of @np. */struct of_phandle_args {        struct device_node *np;        int args_count;        uint32_t args[MAX_PHANDLE_ARGS];};static int of_parse_clkspec(const struct device_node *np, int index,                            const char *name, struct of_phandle_args *out_args){        int ret = -ENOENT;        /* Walk up the tree of devices looking for a clock property that matches */        while (np) {                /*                 * For named clocks, first look up the name in the                 * \"clock-names\" property.  If it cannot be found, then index                 * will be an error code and of_parse_phandle_with_args() will                 * return -EINVAL.                 */                if (name)                        index = of_property_match_string(np, \"clock-names\", name);                ret = of_parse_phandle_with_args(np, \"clocks\", \"#clock-cells\",                                                 index, out_args);                if (!ret)                        break;                if (name &amp;&amp; index &gt;= 0)                        break;                /*                 * No matching clock found on this node.  If the parent node                 * has a \"clock-ranges\" property, then we can try one of its                 * clocks.                 */                np = np-&gt;parent;                if (np &amp;&amp; !of_get_property(np, \"clock-ranges\", NULL))                        break;                index = 0;        }        return ret;}When you look at the device file, you can easily find that your device has clock property,and it contains one or more phandles, which are specified with \\&amp; operator and the clk device name.Because clock of the device can be represented with the phandle,of_parse_clkspec needs to utilize proper device tree api that can parse the phandle, of_parse_phandle_with_args.This function parse the current device node and retrieves phandles assigned to specific property.Here, since this function pass the “clocks” as the property names,it will find the cloks and generates the of_phandle_args structureusing the assigned phandles.Note that index passed to every above function implies that clock properties can have multiple phandlesassociated with the current device. Therefore, by passing the index, we can skip the previous clock phandles and only pick the clock that we want to have access.Note that the function that we initially invoked isof_clk_bulk_get not of_clk_get (need to retrieve all clk phandles).Also, of_parse_clkspec function utilize index when the name parameter has been passed as NULL.If the name has been passed, instead of index,then the index will be re-calculated by searching the clock-names, which contains string names associated with each clock phandles.In our case, we have passed the index instead of its string alias, it will directly invoke the of_parse_phandle_with_args with the passed index.drivers/of/base.c/** * of_parse_phandle_with_args() - Find a node pointed by phandle in a list * @np:         pointer to a device tree node containing a list * @list_name:  property name that contains a list * @cells_name: property name that specifies phandles' arguments count * @index:      index of a phandle to parse out * @out_args:   optional pointer to output arguments structure (will be filled) * * This function is useful to parse lists of phandles and their arguments. * Returns 0 on success and fills out_args, on error returns appropriate * errno value. * * Caller is responsible to call of_node_put() on the returned out_args-&gt;np * pointer. *  * Example: *  * phandle1: node1 { *      #list-cells = &lt;2&gt;; * }     *  * phandle2: node2 { *      #list-cells = &lt;1&gt;; * }     *       * node3 { *      list = &lt;&amp;phandle1 1 2 &amp;phandle2 3&gt;; * } *  * To get a device_node of the `node2' node you may call this: * of_parse_phandle_with_args(node3, \"list\", \"#list-cells\", 1, &amp;args); */  int of_parse_phandle_with_args(const struct device_node *np, const char *list_name,                                const char *cells_name, int index,                                struct of_phandle_args *out_args){        int cell_count = -1;         if (index &lt; 0)                return -EINVAL;        /* If cells_name is NULL we assume a cell count of 0 */        if (!cells_name)                cell_count = 0;                return __of_parse_phandle_with_args(np, list_name, cells_name,                                            cell_count, index, out_args);}        static int __of_parse_phandle_with_args(const struct device_node *np,                                        const char *list_name,                                        const char *cells_name,                                        int cell_count, int index,                                        struct of_phandle_args *out_args){        struct of_phandle_iterator it;        int rc, cur_index = 0;        /* Loop over the phandles until all the requested entry is found */        of_for_each_phandle(&amp;it, rc, np, list_name, cells_name, cell_count) {                /*                 * All of the error cases bail out of the loop, so at                 * this point, the parsing is successful. If the requested                 * index matches, then fill the out_args structure and return,                 * or return -ENOENT for an empty entry.                 */                rc = -ENOENT;                if (cur_index == index) {                        if (!it.phandle)                                goto err;                        if (out_args) {                                int c;                                c = of_phandle_iterator_args(&amp;it,                                                             out_args-&gt;args,                                                             MAX_PHANDLE_ARGS);                                out_args-&gt;np = it.node;                                out_args-&gt;args_count = c;                        } else {                                of_node_put(it.node);                        }                        /* Found it! return success */                        return 0;                }                cur_index++;        }        /*         * Unlock node before returning result; will be one of:         * -ENOENT : index is for empty phandle         * -EINVAL : parsing error on data         */ err:        of_node_put(it.node);        return rc;}__of_parse_phandle_with_args actually traverse the phandle list and retrieve one phandle element from there based on the index fed to this function.Because we passed out_args parameter,when the match occurs, the found phandle should be stored to the out_args. Here, out_args has memeber fied np, which is a device nodeassociated with the found phandle. Because currently we are searching clocks, the found phandle’s node, it.node, should be a device node representing the clock source.Locating the clk structure associated with the found clkspecLet’s revisit the of_clk_get_hw function.struct clk_hw *of_clk_get_hw(struct device_node *np, int index,                             const char *con_id){        int ret;        struct clk_hw *hw;        struct of_phandle_args clkspec;        ret = of_parse_clkspec(np, index, con_id, &amp;clkspec);        if (ret)                return ERR_PTR(ret);        hw = of_clk_get_hw_from_clkspec(&amp;clkspec);        of_node_put(clkspec.np);        return hw;}After the of_parse_clkspec function parse the device tree and generate the clkspec structure associated withone clock resource specified in the clock property,it invokes of_clk_get_hw_from_clkspec to have an access on clk_hw structure associated with the found clock resource.Note that clk_hw is a structure embedded in the clk structure,which is used to access the clk structure and invokes associated clock operations.Also, because clkspec we found has information such as device node associated with the found clock resource,we can utilize it to find the pre-registered clocks./** * struct of_clk_provider - Clock provider registration structure * @link: Entry in global list of clock providers * @node: Pointer to device tree node of clock provider * @get: Get clock callback.  Returns NULL or a struct clk for the *       given clock specifier * @get_hw: Get clk_hw callback.  Returns NULL, ERR_PTR or a *       struct clk_hw for the given clock specifier * @data: context pointer to be passed into @get callback */struct of_clk_provider {        struct list_head link;        struct device_node *node;        struct clk *(*get)(struct of_phandle_args *clkspec, void *data);        struct clk_hw *(*get_hw)(struct of_phandle_args *clkspec, void *data);        void *data;};static struct clk_hw *of_clk_get_hw_from_clkspec(struct of_phandle_args *clkspec){        struct of_clk_provider *provider;        struct clk_hw *hw = ERR_PTR(-EPROBE_DEFER);        if (!clkspec)                return ERR_PTR(-EINVAL);        mutex_lock(&amp;of_clk_mutex);        list_for_each_entry(provider, &amp;of_clk_providers, link) {                if (provider-&gt;node == clkspec-&gt;np) {                        hw = __of_clk_get_hw_from_provider(provider, clkspec);                        if (!IS_ERR(hw))                                break;                }        }        mutex_unlock(&amp;of_clk_mutex);        return hw;}static struct clk_hw *__of_clk_get_hw_from_provider(struct of_clk_provider *provider,                              struct of_phandle_args *clkspec){        struct clk *clk;        if (provider-&gt;get_hw)                return provider-&gt;get_hw(clkspec, provider-&gt;data);        clk = provider-&gt;get(clkspec, provider-&gt;data);        if (IS_ERR(clk))                return ERR_CAST(clk);        return __clk_get_hw(clk);}Linux kernel maintains global list for clock provider, of_clk_providers.This structure contains all the clock resources present in this current system. Also, each of_clk_provider structure contains its associated device node which was used to register clock resource.Remind that we have an access to the clock device node that we want to utilize on our device.Therefore, of_clk_get_hw_from_clkspec function iterates the list of of_clk_provider and tries to find an entrythat matches its device node member field with device node we found in the previous step (stored in the clkspec).When the entry found, it invokes __of_clk_get_he_from_provider functionto actually retrieve the clk structure registered in the provider structure. Note that clk_hw structure is embedded in the clk structure.Check out that of_clk_provider structure contains two function pointers,get and get_hw function.When the clock resource has been registered with of_clk_add_provider function,the get function should have been successfully registered in the provider’s get member field.By invoking get function,we can easily retrieve the clk structuregenerated before in the clock source driver particularly at probe function.Finally, clk_hw can be easily retrieved with the __clk_get_hwwhich just returns the member field core-&gt;hw of the clock structure.static struct clk *__of_clk_get(struct device_node *np,                                int index, const char *dev_id,                                const char *con_id){        struct clk_hw *hw = of_clk_get_hw(np, index, con_id);        return clk_hw_create_clk(NULL, hw, dev_id, con_id);}Finally, we can get the access to the hw structure associated with one clock resource specified in the device file, particularly one clock resource specified as phandle of clock property of our device node./** * clk_hw_create_clk: Allocate and link a clk consumer to a clk_core given * a clk_hw * @dev: clk consumer device * @hw: clk_hw associated with the clk being consumed * @dev_id: string describing device name * @con_id: connection ID string on device * * This is the main function used to create a clk pointer for use by clk * consumers. It connects a consumer to the clk_core and clk_hw structures * used by the framework and clk provider respectively. */struct clk *clk_hw_create_clk(struct device *dev, struct clk_hw *hw,                              const char *dev_id, const char *con_id){        struct clk *clk;        struct clk_core *core;        /* This is to allow this function to be chained to others */        if (IS_ERR_OR_NULL(hw))                return ERR_CAST(hw);        core = hw-&gt;core;        clk = alloc_clk(core, dev_id, con_id);        if (IS_ERR(clk))                return clk;        clk-&gt;dev = dev;        if (!try_module_get(core-&gt;owner)) {                free_clk(clk);                return ERR_PTR(-ENOENT);        }        kref_get(&amp;core-&gt;ref);        clk_core_link_consumer(core, clk);        return clk;}/** * alloc_clk - Allocate a clk consumer, but leave it unlinked to the clk_core * @core: clk to allocate a consumer for * @dev_id: string describing device name * @con_id: connection ID string on device * * Returns: clk consumer left unlinked from the consumer list */static struct clk *alloc_clk(struct clk_core *core, const char *dev_id,                             const char *con_id){        struct clk *clk;        clk = kzalloc(sizeof(*clk), GFP_KERNEL);        if (!clk)                return ERR_PTR(-ENOMEM);        clk-&gt;core = core;        clk-&gt;dev_id = dev_id;        clk-&gt;con_id = kstrdup_const(con_id, GFP_KERNEL);        clk-&gt;max_rate = ULONG_MAX;        return clk;}Although we had an access to the clk structure that has been registered to the system as the provider.And instead of returning that clk structure,we only have an access to the clk_hw structure.Threfore, we need to create clk structure instance once againassociated with current clk_hw structure.Then how it is different from the clk structure stored in the provider structure?Preparing and enabling cloksstatic inline int __must_checkclk_bulk_prepare_enable(int num_clks, const struct clk_bulk_data *clks){               int ret;                        ret = clk_bulk_prepare(num_clks, clks);        if (ret)                return ret;        ret = clk_bulk_enable(num_clks, clks);        if (ret)                 clk_bulk_unprepare(num_clks, clks);                        return ret;}Before clock is enabled, it should be prepared with the provided apis. Note that clks embeds core structure that contains clock operations required to prepare and enable clocks./** * clk_bulk_prepare - prepare a set of clocks * @num_clks: the number of clk_bulk_data * @clks: the clk_bulk_data table being prepared * * clk_bulk_prepare may sleep, which differentiates it from clk_bulk_enable. * Returns 0 on success, -EERROR otherwise. */int __must_check clk_bulk_prepare(int num_clks,                                  const struct clk_bulk_data *clks){        int ret;        int i;        for (i = 0; i &lt; num_clks; i++) {                ret = clk_prepare(clks[i].clk);                if (ret) {                        pr_err(\"Failed to prepare clk '%s': %d\\n\",                                clks[i].id, ret);                        goto err;                }        }        return 0;err:        clk_bulk_unprepare(i, clks);        return  ret;}EXPORT_SYMBOL_GPL(clk_bulk_prepare);/** * clk_prepare - prepare a clock source * @clk: the clk being prepared *       * clk_prepare may sleep, which differentiates it from clk_enable.  In a simple * case, clk_prepare can be used instead of clk_enable to ungate a clk if the * operation may sleep.  One example is a clk which is accessed over I2c.  In * the complex case a clk ungate operation may require a fast and a slow part. * It is this reason that clk_prepare and clk_enable are not mutually * exclusive.  In fact clk_prepare must be called before clk_enable. * Returns 0 on success, -EERROR otherwise. */int clk_prepare(struct clk *clk){        if (!clk)                return 0;        return clk_core_prepare_lock(clk-&gt;core);}EXPORT_SYMBOL_GPL(clk_prepare);static int clk_core_prepare_lock(struct clk_core *core){        int ret;        clk_prepare_lock();        ret = clk_core_prepare(core);        clk_prepare_unlock();         return ret;}Because we have multiple clock instances,we should invoke clk_bulk_prepare functionthat traverses all clk structures we found beforeand invoke the associated prepare methodsembedded in that core structure of each clk structure.static int clk_core_prepare(struct clk_core *core){        int ret = 0;        lockdep_assert_held(&amp;prepare_lock);        if (!core)                return 0;        if (core-&gt;prepare_count == 0) {                ret = clk_pm_runtime_get(core);                if (ret)                        return ret;                ret = clk_core_prepare(core-&gt;parent);                if (ret)                        goto runtime_put;                trace_clk_prepare(core);                if (core-&gt;ops-&gt;prepare)                        ret = core-&gt;ops-&gt;prepare(core-&gt;hw);                trace_clk_prepare_complete(core);                if (ret)                        goto unprepare;        }        core-&gt;prepare_count++;        /*         * CLK_SET_RATE_GATE is a special case of clock protection         * Instead of a consumer claiming exclusive rate control, it is         * actually the provider which prevents any consumer from making any         * operation which could result in a rate change or rate glitch while         * the clock is prepared.         */        if (core-&gt;flags &amp; CLK_SET_RATE_GATE)                clk_core_rate_protect(core);        return 0;unprepare:        clk_core_unprepare(core-&gt;parent);runtime_put:        clk_pm_runtime_put(core);        return ret;}Based on the clock type such as gate, change rate, multiplexer, etc,some clocks may or may not implement the prepare method.Also, various clocks consists of hierarchies,one clock can has its associated parent clocks.Therefore, although target clock doesn’t need a prepare method,it might need to invoke its parent’s prepare method.Note that this prepare method is going to be invoked recursively.After that, target clock will invoke its prepare function if presents.In our case,dwc3 apple device utilizes the atc0_usb and atcphy0 clock,and they don’t have any prepare clocks because they are clock gates.To understand how the prepare function actually prepares the clock,you should take a look at each clock’s device driverthat initialize clk structure and its corresponding core and ops fields.managing those clocks. Also you might want to check their parent clocks to see how prepare functions are implemented in its parent clock.Anyway, in our case we can find that it doesn’t need to invoke prepare method directly.Therefore, let’s see how the clocks are going to be enabled. Most of the basic skeletons for the clk_bulk_enable is same as clk_bulk_prepare.It traverse found clk structure used by the current device nodeand invokes the enable function associated with them making use of the ops of the core structure./** * clk_bulk_enable - ungate a set of clocks * @num_clks: the number of clk_bulk_data * @clks: the clk_bulk_data table being ungated * * clk_bulk_enable must not sleep * Returns 0 on success, -EERROR otherwise. */int __must_check clk_bulk_enable(int num_clks, const struct clk_bulk_data *clks){        int ret;        int i;        for (i = 0; i &lt; num_clks; i++) {                ret = clk_enable(clks[i].clk);                if (ret) {                        pr_err(\"Failed to enable clk '%s': %d\\n\",                                clks[i].id, ret);                        goto err;                }        }        return 0;err:        clk_bulk_disable(i, clks);        return  ret;}EXPORT_SYMBOL_GPL(clk_bulk_enable);/** * clk_enable - ungate a clock * @clk: the clk being ungated * * clk_enable must not sleep, which differentiates it from clk_prepare.  In a * simple case, clk_enable can be used instead of clk_prepare to ungate a clk * if the operation will never sleep.  One example is a SoC-internal clk which * is controlled via simple register writes.  In the complex case a clk ungate * operation may require a fast and a slow part.  It is this reason that * clk_enable and clk_prepare are not mutually exclusive.  In fact clk_prepare * must be called before clk_enable.  Returns 0 on success, -EERROR * otherwise. */int clk_enable(struct clk *clk){        if (!clk)                return 0;        return clk_core_enable_lock(clk-&gt;core);}EXPORT_SYMBOL_GPL(clk_enable);static int clk_core_enable_lock(struct clk_core *core){        unsigned long flags;        int ret;        flags = clk_enable_lock();        ret = clk_core_enable(core);        clk_enable_unlock(flags);        return ret;}static int clk_core_enable(struct clk_core *core){        int ret = 0;        lockdep_assert_held(&amp;enable_lock);        if (!core)                return 0;        if (WARN(core-&gt;prepare_count == 0,            \"Enabling unprepared %s\\n\", core-&gt;name))                return -ESHUTDOWN;        if (core-&gt;enable_count == 0) {                ret = clk_core_enable(core-&gt;parent);                if (ret)                        return ret;                trace_clk_enable_rcuidle(core);                if (core-&gt;ops-&gt;enable)                        ret = core-&gt;ops-&gt;enable(core-&gt;hw);                trace_clk_enable_complete_rcuidle(core);                if (ret) {                        clk_core_disable(core-&gt;parent);                        return ret;                }        }        core-&gt;enable_count++;        return 0;}To see which function will be invokedwhen current clock’s enable function is invoked. Remember that the dwc3 node utilize the apple_pmgr_gate type clocks.const struct clk_ops clk_apple_pmgr_gate_ops = {        .enable = clk_apple_pmgr_gate_enable,        .disable = clk_apple_pmgr_gate_disable,        .is_enabled = clk_apple_pmgr_gate_is_enabled,};static int clk_apple_pmgr_gate_enable(struct clk_hw *hw){        struct clk_apple_pmgr *clk = to_clk_apple_pmgr(hw);        unsigned max = 10000;        uint32_t val;        clk_apple_pmgr_run_seq(clk, clk-&gt;seq[1], clk-&gt;seqn[1]);        val = readl(clk-&gt;bases[0]);        val |= 15;        val &amp;= ~0x300;        writel(val, clk-&gt;bases[0]);        while(max --) {                mb();                val = readl(clk-&gt;bases[0]);                if(((val &gt;&gt; 4) &amp; 15) == 15) {                        val &amp;= ~0x300;                        writel(val | 0x10000000, clk-&gt;bases[0]);                        clk_apple_pmgr_run_seq(clk, clk-&gt;seq[3], clk-&gt;seqn[3]);                        return 0;                }                if(max &lt; 5000)                        udelay(20);                cpu_relax();        }        pr_err(\"%s: failed to enable PMGR clock\\n\", clk-&gt;name);        return -ETIMEDOUT;}Because ops has clk_apple_pmgr_gate_enable as its enable function pointer,whenever the enable function is invoked through its core’s ops,the assigned function will be invoked and enable the clock. We are not going to cover the detail implementation of the clock enable function.More DWC3 on apple deviceAfter the clock provided to the dwc3 device has been prepared and enabled,it should start the dwc3 device itself.The dwc3_apple_m1_start function does this for you.#define USBCORE_DWC3                                    0x280000#define DWC3_GUSB2PHYCFG0_SUSPENDUSB20                  (1 « 6)#define DWC3_GUSB3PIPECTL0_SUSPENDENABLE                (1 « 17)static int dwc3_apple_m1_start(struct dwc3_apple *da){        int ret;    /* set core mode to host */    dwc3_apple_m1_rmwl(DWC3_GCTL_PRTCAPDIR(3), DWC3_GCTL_PRTCAPDIR(DWC3_GCTL_PRTCAP_HOST),            da-&gt;usbcore + USBCORE_DWC3 + DWC3_GCTL);    ret = dwc3_apple_m1_tunable(da, \"tunable\");    if(ret &lt; 0)            return ret;    /* set up power and clock parameters in DRD core */    dwc3_apple_m1_rmwl(DWC3_GCTL_PWRDNSCALE(0x1FFF), DWC3_GCTL_PWRDNSCALE(13),            da-&gt;usbcore + USBCORE_DWC3 + DWC3_GCTL);    dwc3_apple_m1_rmwl(0, DWC3_GCTL_GBLHIBERNATIONEN, da-&gt;usbcore + USBCORE_DWC3 + DWC3_GCTL);    /* unsuspend the PHYs */    dwc3_apple_m1_rmwl(DWC3_GUSB2PHYCFG0_SUSPENDUSB20, 0,            da-&gt;usbcore + USBCORE_DWC3 + DWC3_GUSB2PHYCFG(0));    dwc3_apple_m1_rmwl(DWC3_GUSB3PIPECTL0_SUSPENDENABLE, 0,            da-&gt;usbcore + USBCORE_DWC3 + DWC3_GUSB3PIPECTL(0));    return 0; }"
  },
  
  {
    "title": "Register Device Through Bus",
    "url": "/posts/register-device-through-bus/",
    "categories": "linux,, embedded-linux",
    "tags": "",
    "date": "2021-05-04 00:00:00 -0400",
    





    
    "snippet": "Initializing usb subsystem/* * Init */static int __init usb_init(void){        int retval;        if (usb_disabled()) {                pr_info(\"%s: USB support disabled\\n\", usbcore_name);          ...",
    "content": "Initializing usb subsystem/* * Init */static int __init usb_init(void){        int retval;        if (usb_disabled()) {                pr_info(\"%s: USB support disabled\\n\", usbcore_name);                return 0;        }        usb_init_pool_max();        usb_debugfs_init();        usb_acpi_register();        retval = bus_register(&amp;usb_bus_type);        if (retval)                goto bus_register_failed;        retval = bus_register_notifier(&amp;usb_bus_type, &amp;usb_bus_nb);        if (retval)                goto bus_notifier_failed;        retval = usb_major_init();        if (retval)                goto major_init_failed;        retval = usb_register(&amp;usbfs_driver);        if (retval)                goto driver_register_failed;        retval = usb_devio_init();        if (retval)                goto usb_devio_init_failed;        retval = usb_hub_init();        if (retval)                goto hub_init_failed;        retval = usb_register_device_driver(&amp;usb_generic_driver, THIS_MODULE);        if (!retval)                goto out;        usb_hub_cleanup();hub_init_failed:        usb_devio_cleanup();usb_devio_init_failed:        usb_deregister(&amp;usbfs_driver);driver_register_failed:        usb_major_cleanup();major_init_failed:        bus_unregister_notifier(&amp;usb_bus_type, &amp;usb_bus_nb);bus_notifier_failed:        bus_unregister(&amp;usb_bus_type);bus_register_failed:        usb_acpi_unregister();        usb_debugfs_cleanup();out:        return retval;}#define subsys_initcall(fn)             __define_initcall(fn, 4)subsys_initcall(usb_init);Usb bus registrationBased on the previous postings,we can know that a bus_type object representing a specific bus sub-systemshould be initialized and registeredby invoking bus_register function. Let’s take a look at which field of bus_type has been statically provided by the usb core first.struct bus_type usb_bus_type = {        .name =         \"usb\",        .match =        usb_device_match,        .uevent =       usb_uevent,        .need_parent_lock =     true,};Compared to the bus_type object used for the platform bus,usb bus_type object provides only handful of information.Note that even the probe function has not been provided.Registering notifier block for usb busWe doesn’t utilize the notifier block for the platform bus, but usb bus utilize the notifier block.The bus_register_notifier set the usb_bus_nb notifier block to the initialized usb bus.static struct notifier_block usb_bus_nb = {        .notifier_call = usb_bus_notify,};   /* * Notifications of device and interface registration */static int usb_bus_notify(struct notifier_block *nb, unsigned long action,                void *data){        struct device *dev = data;        switch (action) {        case BUS_NOTIFY_ADD_DEVICE:                if (dev-&gt;type == &amp;usb_device_type)                        (void) usb_create_sysfs_dev_files(to_usb_device(dev));                else if (dev-&gt;type == &amp;usb_if_device_type)                        usb_create_sysfs_intf_files(to_usb_interface(dev));                break;        case BUS_NOTIFY_DEL_DEVICE:                if (dev-&gt;type == &amp;usb_device_type)                        usb_remove_sysfs_dev_files(to_usb_device(dev));                else if (dev-&gt;type == &amp;usb_if_device_type)                        usb_remove_sysfs_intf_files(to_usb_interface(dev));                break;        }        return 0;}  When the notification is sent, the above usb_bus_notify function will be invoked with the action parameter.Based on the action, it hanldes usb device management on the sysfs.Register usb filesystem driver, usbfsstruct usb_driver usbfs_driver = {        .name =         \"usbfs\",        .probe =        driver_probe,        .disconnect =   driver_disconnect,        .suspend =      driver_suspend,        .resume =       driver_resume,        .supports_autosuspend = 1,};      #define usb_register(driver) \\        usb_register_driver(driver, THIS_MODULE, KBUILD_MODNAME)/** * usb_register_driver - register a USB interface driver * @new_driver: USB operations for the interface driver * @owner: module owner of this driver. * @mod_name: module name string * * Registers a USB interface driver with the USB core.  The list of * unattached interfaces will be rescanned whenever a new driver is * added, allowing the new driver to attach to any recognized interfaces. * * Return: A negative error code on failure and 0 on success. * * NOTE: if you want your driver to use the USB major number, you must call * usb_register_dev() to enable that functionality.  This function no longer * takes care of that. */int usb_register_driver(struct usb_driver *new_driver, struct module *owner,                        const char *mod_name){               int retval = 0;                if (usb_disabled())                return -ENODEV;        new_driver-&gt;drvwrap.for_devices = 0;        new_driver-&gt;drvwrap.driver.name = new_driver-&gt;name;        new_driver-&gt;drvwrap.driver.bus = &amp;usb_bus_type;        new_driver-&gt;drvwrap.driver.probe = usb_probe_interface;        new_driver-&gt;drvwrap.driver.remove = usb_unbind_interface;        new_driver-&gt;drvwrap.driver.owner = owner;        new_driver-&gt;drvwrap.driver.mod_name = mod_name;        new_driver-&gt;drvwrap.driver.dev_groups = new_driver-&gt;dev_groups;        spin_lock_init(&amp;new_driver-&gt;dynids.lock);        INIT_LIST_HEAD(&amp;new_driver-&gt;dynids.list);         retval = driver_register(&amp;new_driver-&gt;drvwrap.driver);        if (retval)                goto out;          retval = usb_create_newid_files(new_driver);        if (retval)                goto out_newid;                pr_info(\"%s: registered new interface driver %s\\n\",                        usbcore_name, new_driver-&gt;name);        out:            return retval;out_newid:        driver_unregister(&amp;new_driver-&gt;drvwrap.driver);         pr_err(\"%s: error %d registering interface driver %s\\n\",                usbcore_name, retval, new_driver-&gt;name);        goto out;}usb_register macro invokes usb_register_driver which register the usb related driver to the usb bus.Initializing usb deviostatic struct cdev usb_device_cdev;        int __init usb_devio_init(void){                       int retval;                retval = register_chrdev_region(USB_DEVICE_DEV, USB_DEVICE_MAX,                                        \"usb_device\");        if (retval) {                printk(KERN_ERR \"Unable to register minors for usb_device\\n\");                goto out;        }        cdev_init(&amp;usb_device_cdev, &amp;usbdev_file_operations);        retval = cdev_add(&amp;usb_device_cdev, USB_DEVICE_DEV, USB_DEVICE_MAX);        if (retval) {                printk(KERN_ERR \"Unable to get usb_device major %d\\n\",                       USB_DEVICE_MAJOR);                goto error_cdev;        }        usb_register_notify(&amp;usbdev_nb);out:            return retval;error_cdev:        unregister_chrdev_region(USB_DEVICE_DEV, USB_DEVICE_MAX);        goto out;}const struct file_operations usbdev_file_operations = {        .owner =          THIS_MODULE,        .llseek =         no_seek_end_llseek,        .read =           usbdev_read,        .poll =           usbdev_poll,        .unlocked_ioctl = usbdev_ioctl,        .compat_ioctl =   compat_ptr_ioctl,        .mmap =           usbdev_mmap,        .open =           usbdev_open,        .release =        usbdev_release,      };static int usbdev_notify(struct notifier_block *self,                               unsigned long action, void *dev){                       switch (action) {        case USB_DEVICE_ADD:                break;        case USB_DEVICE_REMOVE:                usbdev_remove(dev);                break;        }        return NOTIFY_OK;}                       static struct notifier_block usbdev_nb = {        .notifier_call =        usbdev_notify,};      The first thing done by the usb devio initialization is initializing and creating the character device for usb devices.Compared to platform devices,usb devices provides interfaces to the user programsso that the user can directly communicate with the device files of the usb devices. The usb_dev_file_operations provides file operations related callback functions for that purpose. XXX: how the other devices are accessed ? does it through the devio??????Registering usb hub device driverint usb_hub_init(void){               if (usb_register(&amp;hub_driver) &lt; 0) {                printk(KERN_ERR \"%s: can't register hub driver\\n\",                        usbcore_name);                return -1;        }                /*               * The workqueue needs to be freezable to avoid interfering with         * USB-PERSIST port handover. Otherwise it might see that a full-speed         * device was gone before the EHCI controller had handed its port         * over to the companion full-speed controller.         */        hub_wq = alloc_workqueue(\"usb_hub_wq\", WQ_FREEZABLE, 0);        if (hub_wq)                return 0;        /* Fall through if kernel_thread failed */        usb_deregister(&amp;hub_driver);        pr_err(\"%s: can't allocate workqueue for usb hub\\n\", usbcore_name);        return -1;}          MODULE_DEVICE_TABLE(usb, hub_id_table);static struct usb_driver hub_driver = {        .name =         \"hub\",        .probe =        hub_probe,        .disconnect =   hub_disconnect,        .suspend =      hub_suspend,        .resume =       hub_resume,        .reset_resume = hub_reset_resume,        .pre_reset =    hub_pre_reset,        .post_reset =   hub_post_reset,        .unlocked_ioctl = hub_ioctl,        .id_table =     hub_id_table,        .supports_autosuspend = 1,};                      static const struct usb_device_id hub_id_table[] = {    { .match_flags = USB_DEVICE_ID_MATCH_VENDOR                   | USB_DEVICE_ID_MATCH_PRODUCT                   | USB_DEVICE_ID_MATCH_INT_CLASS,      .idVendor = USB_VENDOR_SMSC,      .idProduct = USB_PRODUCT_USB5534B,      .bInterfaceClass = USB_CLASS_HUB,      .driver_info = HUB_QUIRK_DISABLE_AUTOSUSPEND},    { .match_flags = USB_DEVICE_ID_MATCH_VENDOR                        | USB_DEVICE_ID_MATCH_INT_CLASS,      .idVendor = USB_VENDOR_GENESYS_LOGIC,      .bInterfaceClass = USB_CLASS_HUB,      .driver_info = HUB_QUIRK_CHECK_PORT_AUTOSUSPEND},    { .match_flags = USB_DEVICE_ID_MATCH_DEV_CLASS,      .bDeviceClass = USB_CLASS_HUB},    { .match_flags = USB_DEVICE_ID_MATCH_INT_CLASS,      .bInterfaceClass = USB_CLASS_HUB},     { }                                         /* Terminating entry */};      Registering hub_event threadstatic int hub_probe(struct usb_interface *intf, const struct usb_device_id *id){\tstruct usb_host_interface *desc;\tstruct usb_device *hdev;\tstruct usb_hub *hub;\tdesc = intf-&gt;cur_altsetting;\thdev = interface_to_usbdev(intf);\t/*\t * Set default autosuspend delay as 0 to speedup bus suspend,\t * based on the below considerations:\t *\t * - Unlike other drivers, the hub driver does not rely on the\t *   autosuspend delay to provide enough time to handle a wakeup\t *   event, and the submitted status URB is just to check future\t *   change on hub downstream ports, so it is safe to do it.\t *\t * - The patch might cause one or more auto supend/resume for\t *   below very rare devices when they are plugged into hub\t *   first time:\t *\t *   \tdevices having trouble initializing, and disconnect\t *   \tthemselves from the bus and then reconnect a second\t *   \tor so later\t *\t *   \tdevices just for downloading firmware, and disconnects\t *   \tthemselves after completing it\t *\t *   For these quite rare devices, their drivers may change the\t *   autosuspend delay of their parent hub in the probe() to one\t *   appropriate value to avoid the subtle problem if someone\t *   does care it.\t *\t * - The patch may cause one or more auto suspend/resume on\t *   hub during running 'lsusb', but it is probably too\t *   infrequent to worry about.\t *\t * - Change autosuspend delay of hub can avoid unnecessary auto\t *   suspend timer for hub, also may decrease power consumption\t *   of USB bus.\t *\t * - If user has indicated to prevent autosuspend by passing\t *   usbcore.autosuspend = -1 then keep autosuspend disabled.\t */#ifdef CONFIG_PM\tif (hdev-&gt;dev.power.autosuspend_delay &gt;= 0)\t\tpm_runtime_set_autosuspend_delay(&amp;hdev-&gt;dev, 0);#endif\t/*\t * Hubs have proper suspend/resume support, except for root hubs\t * where the controller driver doesn't have bus_suspend and\t * bus_resume methods.\t */\tif (hdev-&gt;parent) {\t\t/* normal device */\t\tusb_enable_autosuspend(hdev);\t} else {\t\t\t/* root hub */\t\tconst struct hc_driver *drv = bus_to_hcd(hdev-&gt;bus)-&gt;driver;\t\tif (drv-&gt;bus_suspend &amp;&amp; drv-&gt;bus_resume)\t\t\tusb_enable_autosuspend(hdev);\t}\tif (hdev-&gt;level == MAX_TOPO_LEVEL) {\t\tdev_err(&amp;intf-&gt;dev,\t\t\t\"Unsupported bus topology: hub nested too deep\\n\");\t\treturn -E2BIG;\t}#ifdef\tCONFIG_USB_OTG_DISABLE_EXTERNAL_HUB\tif (hdev-&gt;parent) {\t\tdev_warn(&amp;intf-&gt;dev, \"ignoring external hub\\n\");\t\treturn -ENODEV;\t}#endif\tif (!hub_descriptor_is_sane(desc)) {\t\tdev_err(&amp;intf-&gt;dev, \"bad descriptor, ignoring hub\\n\");\t\treturn -EIO;\t}\t/* We found a hub */\tdev_info(&amp;intf-&gt;dev, \"USB hub found\\n\");\thub = kzalloc(sizeof(*hub), GFP_KERNEL);\tif (!hub)\t\treturn -ENOMEM;\tkref_init(&amp;hub-&gt;kref);\thub-&gt;intfdev = &amp;intf-&gt;dev;\thub-&gt;hdev = hdev;\tINIT_DELAYED_WORK(&amp;hub-&gt;leds, led_work);\tINIT_DELAYED_WORK(&amp;hub-&gt;init_work, NULL);\tINIT_WORK(&amp;hub-&gt;events, hub_event);\tspin_lock_init(&amp;hub-&gt;irq_urb_lock);\ttimer_setup(&amp;hub-&gt;irq_urb_retry, hub_retry_irq_urb, 0);\tusb_get_intf(intf);\tusb_get_dev(hdev);\tusb_set_intfdata(intf, hub);\tintf-&gt;needs_remote_wakeup = 1;\tpm_suspend_ignore_children(&amp;intf-&gt;dev, true);\tif (hdev-&gt;speed == USB_SPEED_HIGH)\t\thighspeed_hubs++;\tif (id-&gt;driver_info &amp; HUB_QUIRK_CHECK_PORT_AUTOSUSPEND)\t\thub-&gt;quirk_check_port_auto_suspend = 1;\tif (id-&gt;driver_info &amp; HUB_QUIRK_DISABLE_AUTOSUSPEND) {\t\thub-&gt;quirk_disable_autosuspend = 1;\t\tusb_autopm_get_interface_no_resume(intf);\t}\tif (hub_configure(hub, &amp;desc-&gt;endpoint[0].desc) &gt;= 0)\t\treturn 0;\thub_disconnect(intf);\treturn -ENODEV;}5362 static void hub_event(struct work_struct *work)5363 {5364         struct usb_device *hdev;5365         struct usb_interface *intf;5366         struct usb_hub *hub;5367         struct device *hub_dev;5368         u16 hubstatus;5369         u16 hubchange;5370         int i, ret;5371 5372         hub = container_of(work, struct usb_hub, events);5373         hdev = hub-&gt;hdev;5374         hub_dev = hub-&gt;intfdev;5375         intf = to_usb_interface(hub_dev);5376 5377         dev_dbg(hub_dev, \"state %d ports %d chg %04x evt %04x\\n\",5378                         hdev-&gt;state, hdev-&gt;maxchild,5379                         /* NOTE: expects max 15 ports... */5380                         (u16) hub-&gt;change_bits[0],5381                         (u16) hub-&gt;event_bits[0]);5382 5383         /* Lock the device, then check to see if we were5384          * disconnected while waiting for the lock to succeed. */5385         usb_lock_device(hdev);5386         if (unlikely(hub-&gt;disconnected))5387                 goto out_hdev_lock;5388 5389         /* If the hub has died, clean up after it */5390         if (hdev-&gt;state == USB_STATE_NOTATTACHED) {5391                 hub-&gt;error = -ENODEV;5392                 hub_quiesce(hub, HUB_DISCONNECT);5393                 goto out_hdev_lock;5394         }5395 5396         /* Autoresume */5397         ret = usb_autopm_get_interface(intf);5398         if (ret) {5399                 dev_dbg(hub_dev, \"Can't autoresume: %d\\n\", ret);5400                 goto out_hdev_lock;5401         }5402 5403         /* If this is an inactive hub, do nothing */5404         if (hub-&gt;quiescing)5405                 goto out_autopm;5406 5407         if (hub-&gt;error) {5408                 dev_dbg(hub_dev, \"resetting for error %d\\n\", hub-&gt;error);5409 5410                 ret = usb_reset_device(hdev);5411                 if (ret) {5412                         dev_dbg(hub_dev, \"error resetting hub: %d\\n\", ret);5413                         goto out_autopm;5414                 }5415 5416                 hub-&gt;nerrors = 0;5417                 hub-&gt;error = 0;5418         }5419 5420         /* deal with port status changes */5421         for (i = 1; i &lt;= hdev-&gt;maxchild; i++) {5422                 struct usb_port *port_dev = hub-&gt;ports[i - 1];5423 5424                 if (test_bit(i, hub-&gt;event_bits)5425                                 || test_bit(i, hub-&gt;change_bits)5426                                 || test_bit(i, hub-&gt;wakeup_bits)) {5427                         /*5428                          * The get_noresume and barrier ensure that if5429                          * the port was in the process of resuming, we5430                          * flush that work and keep the port active for5431                          * the duration of the port_event().  However,5432                          * if the port is runtime pm suspended5433                          * (powered-off), we leave it in that state, run5434                          * an abbreviated port_event(), and move on.5435                          */5436                         pm_runtime_get_noresume(&amp;port_dev-&gt;dev);5437                         pm_runtime_barrier(&amp;port_dev-&gt;dev);5438                         usb_lock_port(port_dev);5439                         port_event(hub, i);5440                         usb_unlock_port(port_dev);5441                         pm_runtime_put_sync(&amp;port_dev-&gt;dev);5442                 }5443         }5444 5445         /* deal with hub status changes */5446         if (test_and_clear_bit(0, hub-&gt;event_bits) == 0)5447                 ;       /* do nothing */5448         else if (hub_hub_status(hub, &amp;hubstatus, &amp;hubchange) &lt; 0)5449                 dev_err(hub_dev, \"get_hub_status failed\\n\");5450         else {5451                 if (hubchange &amp; HUB_CHANGE_LOCAL_POWER) {5452                         dev_dbg(hub_dev, \"power change\\n\");5453                         clear_hub_feature(hdev, C_HUB_LOCAL_POWER);5454                         if (hubstatus &amp; HUB_STATUS_LOCAL_POWER)5455                                 /* FIXME: Is this always true? */5456                                 hub-&gt;limited_power = 1;5457                         else5458                                 hub-&gt;limited_power = 0;5459                 }5460                 if (hubchange &amp; HUB_CHANGE_OVERCURRENT) {5461                         u16 status = 0;5462                         u16 unused;5463 5464                         dev_dbg(hub_dev, \"over-current change\\n\");5465                         clear_hub_feature(hdev, C_HUB_OVER_CURRENT);5466                         msleep(500);    /* Cool down */5467                         hub_power_on(hub, true);5468                         hub_hub_status(hub, &amp;status, &amp;unused);5469                         if (status &amp; HUB_STATUS_OVERCURRENT)5470                                 dev_err(hub_dev, \"over-current condition\\n\");5471                 }5472         }5473 5474 out_autopm:5475         /* Balance the usb_autopm_get_interface() above */5476         usb_autopm_put_interface_no_suspend(intf);5477 out_hdev_lock:5478         usb_unlock_device(hdev);5479 5480         /* Balance the stuff in kick_hub_wq() and allow autosuspend */5481         usb_autopm_put_interface(intf);5482         kref_put(&amp;hub-&gt;kref, hub_release);5483 }5254 static void port_event(struct usb_hub *hub, int port1)5255                 __must_hold(&amp;port_dev-&gt;status_lock)5256 {5257         int connect_change;5258         struct usb_port *port_dev = hub-&gt;ports[port1 - 1];5259         struct usb_device *udev = port_dev-&gt;child;5260         struct usb_device *hdev = hub-&gt;hdev;5261         u16 portstatus, portchange;5262 5263         connect_change = test_bit(port1, hub-&gt;change_bits);5264         clear_bit(port1, hub-&gt;event_bits);5265         clear_bit(port1, hub-&gt;wakeup_bits);5266 5267         if (hub_port_status(hub, port1, &amp;portstatus, &amp;portchange) &lt; 0)5268                 return;5269 5270         if (portchange &amp; USB_PORT_STAT_C_CONNECTION) {5271                 usb_clear_port_feature(hdev, port1, USB_PORT_FEAT_C_CONNECTION);5272                 connect_change = 1;5273         }5274 5275         if (portchange &amp; USB_PORT_STAT_C_ENABLE) {5276                 if (!connect_change)5277                         dev_dbg(&amp;port_dev-&gt;dev, \"enable change, status %08x\\n\",5278                                         portstatus);5279                 usb_clear_port_feature(hdev, port1, USB_PORT_FEAT_C_ENABLE);5280 5281                 /*5282                  * EM interference sometimes causes badly shielded USB devices5283                  * to be shutdown by the hub, this hack enables them again.5284                  * Works at least with mouse driver.5285                  */5286                 if (!(portstatus &amp; USB_PORT_STAT_ENABLE)5287                     &amp;&amp; !connect_change &amp;&amp; udev) {5288                         dev_err(&amp;port_dev-&gt;dev, \"disabled by hub (EMI?), re-enabling...\\n\");5289                         connect_change = 1;5290                 }5291         }5292 5293         if (portchange &amp; USB_PORT_STAT_C_OVERCURRENT) {5294                 u16 status = 0, unused;5295                 port_dev-&gt;over_current_count++;5296                 port_over_current_notify(port_dev);5297 5298                 dev_dbg(&amp;port_dev-&gt;dev, \"over-current change #%u\\n\",5299                         port_dev-&gt;over_current_count);5300                 usb_clear_port_feature(hdev, port1,5301                                 USB_PORT_FEAT_C_OVER_CURRENT);5302                 msleep(100);    /* Cool down */5303                 hub_power_on(hub, true);5304                 hub_port_status(hub, port1, &amp;status, &amp;unused);5305                 if (status &amp; USB_PORT_STAT_OVERCURRENT)5306                         dev_err(&amp;port_dev-&gt;dev, \"over-current condition\\n\");5307         }5308 5309         if (portchange &amp; USB_PORT_STAT_C_RESET) {5310                 dev_dbg(&amp;port_dev-&gt;dev, \"reset change\\n\");5311                 usb_clear_port_feature(hdev, port1, USB_PORT_FEAT_C_RESET);5312         }5313         if ((portchange &amp; USB_PORT_STAT_C_BH_RESET)5314             &amp;&amp; hub_is_superspeed(hdev)) {5315                 dev_dbg(&amp;port_dev-&gt;dev, \"warm reset change\\n\");5316                 usb_clear_port_feature(hdev, port1,5317                                 USB_PORT_FEAT_C_BH_PORT_RESET);5318         }5319         if (portchange &amp; USB_PORT_STAT_C_LINK_STATE) {5320                 dev_dbg(&amp;port_dev-&gt;dev, \"link state change\\n\");5321                 usb_clear_port_feature(hdev, port1,5322                                 USB_PORT_FEAT_C_PORT_LINK_STATE);5323         }5324         if (portchange &amp; USB_PORT_STAT_C_CONFIG_ERROR) {5325                 dev_warn(&amp;port_dev-&gt;dev, \"config error\\n\");5326                 usb_clear_port_feature(hdev, port1,5327                                 USB_PORT_FEAT_C_PORT_CONFIG_ERROR);5328         }5329 5330         /* skip port actions that require the port to be powered on */5331         if (!pm_runtime_active(&amp;port_dev-&gt;dev))5332                 return;5333 5334         if (hub_handle_remote_wakeup(hub, port1, portstatus, portchange))5335                 connect_change = 1;5336 5337         /*5338          * Warm reset a USB3 protocol port if it's in5339          * SS.Inactive state.5340          */5341         if (hub_port_warm_reset_required(hub, port1, portstatus)) {5342                 dev_dbg(&amp;port_dev-&gt;dev, \"do warm reset\\n\");5343                 if (!udev || !(portstatus &amp; USB_PORT_STAT_CONNECTION)5344                                 || udev-&gt;state == USB_STATE_NOTATTACHED) {5345                         if (hub_port_reset(hub, port1, NULL,5346                                         HUB_BH_RESET_TIME, true) &lt; 0)5347                                 hub_port_disable(hub, port1, 1);5348                 } else {5349                         usb_unlock_port(port_dev);5350                         usb_lock_device(udev);5351                         usb_reset_device(udev);5352                         usb_unlock_device(udev);5353                         usb_lock_port(port_dev);5354                         connect_change = 0;5355                 }5356         }5357 5358         if (connect_change)5359                 hub_port_connect_change(hub, port1, portstatus, portchange);5360 }5156 /* Handle physical or logical connection change events.5157  * This routine is called when:5158  *      a port connection-change occurs;5159  *      a port enable-change occurs (often caused by EMI);5160  *      usb_reset_and_verify_device() encounters changed descriptors (as from5161  *              a firmware download)5162  * caller already locked the hub5163  */5164 static void hub_port_connect_change(struct usb_hub *hub, int port1,5165                                         u16 portstatus, u16 portchange)5166                 __must_hold(&amp;port_dev-&gt;status_lock)5167 {5168         struct usb_port *port_dev = hub-&gt;ports[port1 - 1];5169         struct usb_device *udev = port_dev-&gt;child;5170         int status = -ENODEV;5171 5172         dev_dbg(&amp;port_dev-&gt;dev, \"status %04x, change %04x, %s\\n\", portstatus,5173                         portchange, portspeed(hub, portstatus));5174 5175         if (hub-&gt;has_indicators) {5176                 set_port_led(hub, port1, HUB_LED_AUTO);5177                 hub-&gt;indicator[port1-1] = INDICATOR_AUTO;5178         }5179 5180 #ifdef  CONFIG_USB_OTG5181         /* during HNP, don't repeat the debounce */5182         if (hub-&gt;hdev-&gt;bus-&gt;is_b_host)5183                 portchange &amp;= ~(USB_PORT_STAT_C_CONNECTION |5184                                 USB_PORT_STAT_C_ENABLE);5185 #endif5186 5187         /* Try to resuscitate an existing device */5188         if ((portstatus &amp; USB_PORT_STAT_CONNECTION) &amp;&amp; udev &amp;&amp;5189                         udev-&gt;state != USB_STATE_NOTATTACHED) {5190                 if (portstatus &amp; USB_PORT_STAT_ENABLE) {5191                         status = 0;             /* Nothing to do */5192 #ifdef CONFIG_PM5193                 } else if (udev-&gt;state == USB_STATE_SUSPENDED &amp;&amp;5194                                 udev-&gt;persist_enabled) {5195                         /* For a suspended device, treat this as a5196                          * remote wakeup event.5197                          */5198                         usb_unlock_port(port_dev);5199                         status = usb_remote_wakeup(udev);5200                         usb_lock_port(port_dev);5201 #endif5202                 } else {5203                         /* Don't resuscitate */;5204                 }5205         }5206         clear_bit(port1, hub-&gt;change_bits);5207 5208         /* successfully revalidated the connection */5209         if (status == 0)5210                 return;5211 5212         usb_unlock_port(port_dev);5213         hub_port_connect(hub, port1, portstatus, portchange);5214         usb_lock_port(port_dev);5215 }4933 static void hub_port_connect(struct usb_hub *hub, int port1, u16 portstatus,4934                 u16 portchange)4935 {4936         int status = -ENODEV;4937         int i;4938         unsigned unit_load;4939         struct usb_device *hdev = hub-&gt;hdev;4940         struct usb_hcd *hcd = bus_to_hcd(hdev-&gt;bus);4941         struct usb_port *port_dev = hub-&gt;ports[port1 - 1];4942         struct usb_device *udev = port_dev-&gt;child;4943         static int unreliable_port = -1;4944 4945         /* Disconnect any existing devices under this port */4946         if (udev) {4947                 if (hcd-&gt;usb_phy &amp;&amp; !hdev-&gt;parent)4948                         usb_phy_notify_disconnect(hcd-&gt;usb_phy, udev-&gt;speed);4949                 usb_disconnect(&amp;port_dev-&gt;child);4950         }4951 4952         /* We can forget about a \"removed\" device when there's a physical4953          * disconnect or the connect status changes.4954          */4955         if (!(portstatus &amp; USB_PORT_STAT_CONNECTION) ||4956                         (portchange &amp; USB_PORT_STAT_C_CONNECTION))4957                 clear_bit(port1, hub-&gt;removed_bits);4958 4959         if (portchange &amp; (USB_PORT_STAT_C_CONNECTION |4960                                 USB_PORT_STAT_C_ENABLE)) {4961                 status = hub_port_debounce_be_stable(hub, port1);4962                 if (status &lt; 0) {4963                         if (status != -ENODEV &amp;&amp;4964                                 port1 != unreliable_port &amp;&amp;4965                                 printk_ratelimit())4966                                 dev_err(&amp;port_dev-&gt;dev, \"connect-debounce failed\\n\");4967                         portstatus &amp;= ~USB_PORT_STAT_CONNECTION;4968                         unreliable_port = port1;4969                 } else {4970                         portstatus = status;4971                 }4972         }4973 4974         /* Return now if debouncing failed or nothing is connected or4975          * the device was \"removed\".4976          */4977         if (!(portstatus &amp; USB_PORT_STAT_CONNECTION) ||4978                         test_bit(port1, hub-&gt;removed_bits)) {4979 4980                 /*4981                  * maybe switch power back on (e.g. root hub was reset)4982                  * but only if the port isn't owned by someone else.4983                  */4984                 if (hub_is_port_power_switchable(hub)4985                                 &amp;&amp; !port_is_power_on(hub, portstatus)4986                                 &amp;&amp; !port_dev-&gt;port_owner)4987                         set_port_feature(hdev, port1, USB_PORT_FEAT_POWER);4988 4989                 if (portstatus &amp; USB_PORT_STAT_ENABLE)4990                         goto done;4991                 return;4992         }4993         if (hub_is_superspeed(hub-&gt;hdev))4994                 unit_load = 150;4995         else4996                 unit_load = 100;4997 4998         status = 0;4999         for (i = 0; i &lt; SET_CONFIG_TRIES; i++) {5000 5001                 /* reallocate for each attempt, since references5002                  * to the previous one can escape in various ways5003                  */5004                 udev = usb_alloc_dev(hdev, hdev-&gt;bus, port1);5005                 if (!udev) {5006                         dev_err(&amp;port_dev-&gt;dev,5007                                         \"couldn't allocate usb_device\\n\");5008                         goto done;5009                 }5010 5011                 usb_set_device_state(udev, USB_STATE_POWERED);5012                 udev-&gt;bus_mA = hub-&gt;mA_per_port;5013                 udev-&gt;level = hdev-&gt;level + 1;5014                 udev-&gt;wusb = hub_is_wusb(hub);5015 5016                 /* Devices connected to SuperSpeed hubs are USB 3.0 or later */5017                 if (hub_is_superspeed(hub-&gt;hdev))5018                         udev-&gt;speed = USB_SPEED_SUPER;5019                 else5020                         udev-&gt;speed = USB_SPEED_UNKNOWN;5021 5022                 choose_devnum(udev);5023                 if (udev-&gt;devnum &lt;= 0) {5024                         status = -ENOTCONN;     /* Don't retry */5025                         goto loop;5026                 }5027 5028                 /* reset (non-USB 3.0 devices) and get descriptor */5029                 usb_lock_port(port_dev);5030                 status = hub_port_init(hub, udev, port1, i);5031                 usb_unlock_port(port_dev);5032                 if (status &lt; 0)5033                         goto loop;5034 5035                 if (udev-&gt;quirks &amp; USB_QUIRK_DELAY_INIT)5036                         msleep(2000);5037 5038                 /* consecutive bus-powered hubs aren't reliable; they can5039                  * violate the voltage drop budget.  if the new child has5040                  * a \"powered\" LED, users should notice we didn't enable it5041                  * (without reading syslog), even without per-port LEDs5042                  * on the parent.5043                  */5044                 if (udev-&gt;descriptor.bDeviceClass == USB_CLASS_HUB5045                                 &amp;&amp; udev-&gt;bus_mA &lt;= unit_load) {5046                         u16     devstat;5047 5048                         status = usb_get_std_status(udev, USB_RECIP_DEVICE, 0,5049                                         &amp;devstat);5050                         if (status) {5051                                 dev_dbg(&amp;udev-&gt;dev, \"get status %d ?\\n\", status);5052                                 goto loop_disable;5053                         }5054                         if ((devstat &amp; (1 &lt;&lt; USB_DEVICE_SELF_POWERED)) == 0) {5055                                 dev_err(&amp;udev-&gt;dev,5056                                         \"can't connect bus-powered hub \"5057                                         \"to this port\\n\");5058                                 if (hub-&gt;has_indicators) {5059                                         hub-&gt;indicator[port1-1] =5060                                                 INDICATOR_AMBER_BLINK;5061                                         queue_delayed_work(5062                                                 system_power_efficient_wq,5063                                                 &amp;hub-&gt;leds, 0);5064                                 }5065                                 status = -ENOTCONN;     /* Don't retry */5066                                 goto loop_disable;5067                         }5068                 }5069 5070                 /* check for devices running slower than they could */5071                 if (le16_to_cpu(udev-&gt;descriptor.bcdUSB) &gt;= 0x02005072                                 &amp;&amp; udev-&gt;speed == USB_SPEED_FULL5073                                 &amp;&amp; highspeed_hubs != 0)5074                         check_highspeed(hub, udev, port1);5075 5076                 /* Store the parent's children[] pointer.  At this point5077                  * udev becomes globally accessible, although presumably5078                  * no one will look at it until hdev is unlocked.5079                  */5080                 status = 0;5081 5082                 mutex_lock(&amp;usb_port_peer_mutex);5083 5084                 /* We mustn't add new devices if the parent hub has5085                  * been disconnected; we would race with the5086                  * recursively_mark_NOTATTACHED() routine.5087                  */5088                 spin_lock_irq(&amp;device_state_lock);5089                 if (hdev-&gt;state == USB_STATE_NOTATTACHED)5090                         status = -ENOTCONN;5091                 else5092                         port_dev-&gt;child = udev;5093                 spin_unlock_irq(&amp;device_state_lock);5094                 mutex_unlock(&amp;usb_port_peer_mutex);5095 5096                 /* Run it through the hoops (find a driver, etc) */5097                 if (!status) {5098                         status = usb_new_device(udev);5099                         if (status) {5100                                 mutex_lock(&amp;usb_port_peer_mutex);5101                                 spin_lock_irq(&amp;device_state_lock);5102                                 port_dev-&gt;child = NULL;5103                                 spin_unlock_irq(&amp;device_state_lock);5104                                 mutex_unlock(&amp;usb_port_peer_mutex);5105                         } else {5106                                 if (hcd-&gt;usb_phy &amp;&amp; !hdev-&gt;parent)5107                                         usb_phy_notify_connect(hcd-&gt;usb_phy,5108                                                         udev-&gt;speed);5109                         }5110                 }5111 5112                 if (status)5113                         goto loop_disable;5114 5115                 status = hub_power_remaining(hub);5116                 if (status)5117                         dev_dbg(hub-&gt;intfdev, \"%dmA power budget left\\n\", status);5118 5119                 return;5120 5121 loop_disable:5122                 hub_port_disable(hub, port1, 1);5123 loop:5124                 usb_ep0_reinit(udev);5125                 release_devnum(udev);5126                 hub_free_dev(udev);5127                 usb_put_dev(udev);5128                 if ((status == -ENOTCONN) || (status == -ENOTSUPP))5129                         break;5130 5131                 /* When halfway through our retry count, power-cycle the port */5132                 if (i == (SET_CONFIG_TRIES / 2) - 1) {5133                         dev_info(&amp;port_dev-&gt;dev, \"attempt power cycle\\n\");5134                         usb_hub_set_port_power(hdev, hub, port1, false);5135                         msleep(2 * hub_power_on_good_delay(hub));5136                         usb_hub_set_port_power(hdev, hub, port1, true);5137                         msleep(hub_power_on_good_delay(hub));5138                 }5139         }5140         if (hub-&gt;hdev-&gt;parent ||5141                         !hcd-&gt;driver-&gt;port_handed_over ||5142                         !(hcd-&gt;driver-&gt;port_handed_over)(hcd, port1)) {5143                 if (status != -ENOTCONN &amp;&amp; status != -ENODEV)5144                         dev_err(&amp;port_dev-&gt;dev,5145                                         \"unable to enumerate USB device\\n\");5146         }5147 5148 done:5149         hub_port_disable(hub, port1, 1);5150         if (hcd-&gt;driver-&gt;relinquish_port &amp;&amp; !hub-&gt;hdev-&gt;parent) {5151                 if (status != -ENOTCONN &amp;&amp; status != -ENODEV)5152                         hcd-&gt;driver-&gt;relinquish_port(hcd, port1);5153         }5154 }Add new device2482 int usb_new_device(struct usb_device *udev)2483 {2484     int err;2485 2486     if (udev-&gt;parent) {2487         /* Initialize non-root-hub device wakeup to disabled;2488          * device (un)configuration controls wakeup capable2489          * sysfs power/wakeup controls wakeup enabled/disabled2490          */2491         device_init_wakeup(&amp;udev-&gt;dev, 0);2492     }2493 2494     /* Tell the runtime-PM framework the device is active */2495     pm_runtime_set_active(&amp;udev-&gt;dev);2496     pm_runtime_get_noresume(&amp;udev-&gt;dev);2497     pm_runtime_use_autosuspend(&amp;udev-&gt;dev);2498     pm_runtime_enable(&amp;udev-&gt;dev);2499 2500     /* By default, forbid autosuspend for all devices.  It will be2501      * allowed for hubs during binding.2502      */2503     usb_disable_autosuspend(udev);2504 2505     err = usb_enumerate_device(udev);   /* Read descriptors */2506     if (err &lt; 0)2507         goto fail;2508     dev_dbg(&amp;udev-&gt;dev, \"udev %d, busnum %d, minor = %d\\n\",2509             udev-&gt;devnum, udev-&gt;bus-&gt;busnum,2510             (((udev-&gt;bus-&gt;busnum-1) * 128) + (udev-&gt;devnum-1)));2511     /* export the usbdev device-node for libusb */2512     udev-&gt;dev.devt = MKDEV(USB_DEVICE_MAJOR,2513             (((udev-&gt;bus-&gt;busnum-1) * 128) + (udev-&gt;devnum-1)));2514 2515     /* Tell the world! */2516     announce_device(udev);2517 2518     if (udev-&gt;serial)2519         add_device_randomness(udev-&gt;serial, strlen(udev-&gt;serial));2520     if (udev-&gt;product)2521         add_device_randomness(udev-&gt;product, strlen(udev-&gt;product));2522     if (udev-&gt;manufacturer)2523         add_device_randomness(udev-&gt;manufacturer,2524                       strlen(udev-&gt;manufacturer));2525 2526     device_enable_async_suspend(&amp;udev-&gt;dev);2527 2528     /* check whether the hub or firmware marks this port as non-removable */2529     if (udev-&gt;parent)2530         set_usb_port_removable(udev);2531 2532     /* Register the device.  The device driver is responsible2533      * for configuring the device and invoking the add-device2534      * notifier chain (used by usbfs and possibly others).2535      */2536     err = device_add(&amp;udev-&gt;dev);2537     if (err) {2538         dev_err(&amp;udev-&gt;dev, \"can't device_add, error %d\\n\", err);2539         goto fail;2540     }2541 2542     /* Create link files between child device and usb port device. */2543     if (udev-&gt;parent) {2544         struct usb_hub *hub = usb_hub_to_struct_hub(udev-&gt;parent);2545         int port1 = udev-&gt;portnum;2546         struct usb_port *port_dev = hub-&gt;ports[port1 - 1];2547 2548         err = sysfs_create_link(&amp;udev-&gt;dev.kobj,2549                 &amp;port_dev-&gt;dev.kobj, \"port\");2550         if (err)2551             goto fail;2552 2553         err = sysfs_create_link(&amp;port_dev-&gt;dev.kobj,2554                 &amp;udev-&gt;dev.kobj, \"device\");2555         if (err) {2556             sysfs_remove_link(&amp;udev-&gt;dev.kobj, \"port\");2557             goto fail;2558         }2559 2560         if (!test_and_set_bit(port1, hub-&gt;child_usage_bits))2561             pm_runtime_get_sync(&amp;port_dev-&gt;dev);2562     }2563 2564     (void) usb_create_ep_devs(&amp;udev-&gt;dev, &amp;udev-&gt;ep0, udev);2565     usb_mark_last_busy(udev);2566     pm_runtime_put_sync_autosuspend(&amp;udev-&gt;dev);2567     return err;2568 2569 fail:2570     usb_set_device_state(udev, USB_STATE_NOTATTACHED);2571     pm_runtime_disable(&amp;udev-&gt;dev);2572     pm_runtime_set_suspended(&amp;udev-&gt;dev);2573     return err;2574 }Register usb device driversstruct usb_device_driver usb_generic_driver = {        .name = \"usb\",        .match = usb_generic_driver_match,        .probe = usb_generic_driver_probe,        .disconnect = usb_generic_driver_disconnect,#ifdef  CONFIG_PM        .suspend = usb_generic_driver_suspend,        .resume = usb_generic_driver_resume,#endif        .supports_autosuspend = 1,};/** * usb_register_device_driver - register a USB device (not interface) driver * @new_udriver: USB operations for the device driver * @owner: module owner of this driver. * * Registers a USB device driver with the USB core.  The list of * unattached devices will be rescanned whenever a new driver is * added, allowing the new driver to attach to any recognized devices. * * Return: A negative error code on failure and 0 on success. */int usb_register_device_driver(struct usb_device_driver *new_udriver,                struct module *owner){        int retval = 0;        if (usb_disabled())                return -ENODEV;        new_udriver-&gt;drvwrap.for_devices = 1;        new_udriver-&gt;drvwrap.driver.name = new_udriver-&gt;name;        new_udriver-&gt;drvwrap.driver.bus = &amp;usb_bus_type;        new_udriver-&gt;drvwrap.driver.probe = usb_probe_device;        new_udriver-&gt;drvwrap.driver.remove = usb_unbind_device;        new_udriver-&gt;drvwrap.driver.owner = owner;        new_udriver-&gt;drvwrap.driver.dev_groups = new_udriver-&gt;dev_groups;        retval = driver_register(&amp;new_udriver-&gt;drvwrap.driver);        if (!retval) {                pr_info(\"%s: registered new device driver %s\\n\",                        usbcore_name, new_udriver-&gt;name);                /*                 * Check whether any device could be better served with                 * this new driver                 */                bus_for_each_dev(&amp;usb_bus_type, NULL, new_udriver,                                 __usb_bus_reprobe_drivers);        } else {                pr_err(\"%s: error %d registering device driver %s\\n\",                        usbcore_name, retval, new_udriver-&gt;name);        }        return retval;}EXPORT_SYMBOL_GPL(usb_register_device_driver);Note that this function is slightly different from usb_register_driver functionwhich was used for registering usb core interface driverssuch as hub_driver, usbfs_driver.One of the notable difference is for_device field is only set for the usb_generic_driver, and other interface driver doesn’t set the flag.Also differnet call back functions are set for this driverXXXThe other biggest difference is it invokes __usb_bus_reprobe_drivers function for devices registered on the usb bus.Long journey to understand how the USB device can be hot-pluggedAlthough most basic usb core part and usb bushave been initialized at the boot-up,the internal usb controllers and another layers of device driver should be boundto fully manage the usb subsystem.Previous initializations are mostly focused on the usb core parts that provides generic software layers for usb management regardless of the hardware specification of the internal usb controller.However, the first layer that actually encountersusb attachment and detachment is the usb controller.As usb specification develops, its controller implementing the specifications also evolved,and linux supports various usb controllers.First to understand the linux-supporting usb controllers,you have to clearly distinguish usb host controller interface from actual usb controller.There are four typical host controller interfaces supported by Linux:*OHCI (Open Host Controller Interface(Compaq)) supporting only USB1.1 (Full and Low speeds),*UHCI (Universal Host Controller Interface (Intel)) supporting 1.x(Full and Low speeds). The hardware composition of UHCI is simple which makes its driver more complex burdening your processor.*EHCI (Extended Host Controller Interface) supporting USB 2.0.*XHCI (Extended Host Controller Interface) supporting USB 3.x and belows for compatibility (including 2.0, 1.X)XXXAlthough there is only one fixed specification for a particular USB version, there can be various versions of USB controller that implements particular specifications. Therefore, to support those controllers,device driver should be required.For example,the linux provides device driver supports for DWC3 which is SuperSpeed (SS) USB 3.0 Dual-Role-Device (DRD) from Synopsys.Also, it has support for CDNS3which is a SuperSpeed (SS) USB 3.0 Dual-Role-Device (DRD) controller from Cadence.Furthermore, note that there can be more device specific USB micro controllerthat is not supported by the linux officially. For further information about Linux supported USB controller,take a look at usb directory.The USB Host controller driversThere are various usb controllers implemented by the different vendors even though they support same USB specification.Therefore, to reduce the boilerplate inmultiple host controller driver,linux implementes generic host controller code that can support multiple versions of it.struct usb_hcd {        /*         * housekeeping         */        struct usb_bus          self;           /* hcd is-a bus */        struct kref             kref;           /* reference counter */        const char              *product_desc;  /* product/vendor string */        int                     speed;          /* Speed for this roothub.                                                 * May be different from                                                 * hcd-&gt;driver-&gt;flags &amp; HCD_MASK                                                 */        char                    irq_descr[24];  /* driver + bus # */        struct timer_list       rh_timer;       /* drives root-hub polling */        struct urb              *status_urb;    /* the current status urb */#ifdef CONFIG_PM        struct work_struct      wakeup_work;    /* for remote wakeup */#endif        struct work_struct      died_work;      /* for when the device dies */        /*         * hardware info/state         */        const struct hc_driver  *driver;        /* hw-specific hooks */\t...}Usb_hcd structure maintains general informationrequired for managing USB controllers regradless of its specification versions and vendors. Therefore, to utilize the benefit of Linux USB subsystem,each host controller driver should provideall information required by generic usb_hcd structure.xHCI usb specificationLet’s switch gears and take a look at USB specification, particularly xHCI. At the time of writing this posting,the xHCI usb specification is the up-to-date version of USBsupporting usb3.x and belows such as usb1.x and usb 2.0.The USB specification is a essential informationto represent a particular USB host controller device.You can see that usb_hcd structure contains the hc_driver structure pointerin the above code block.This structure contains USB specification specific callback functions utilized by the USB host controller driver to support specific USB protocol such as USB 3.0, USB 2.0, etc.drivers/usb/host/xhci.c filestatic const struct hc_driver xhci_hc_driver = {        .description =          \"xhci-hcd\",        .product_desc =         \"xHCI Host Controller\",        .hcd_priv_size =        sizeof(struct xhci_hcd),        /*         * generic hardware linkage         */        .irq =                  xhci_irq,        .flags =                HCD_MEMORY | HCD_DMA | HCD_USB3 | HCD_SHARED |                                HCD_BH,        /*         * basic lifecycle operations         */        .reset =                NULL, /* set in xhci_init_driver() */        .start =                xhci_run,        .stop =                 xhci_stop,        .shutdown =             xhci_shutdown,        /*         * managing i/o requests and associated device resources         */        .map_urb_for_dma =      xhci_map_urb_for_dma,        .unmap_urb_for_dma =    xhci_unmap_urb_for_dma,        .urb_enqueue =          xhci_urb_enqueue,        .urb_dequeue =          xhci_urb_dequeue,        .alloc_dev =            xhci_alloc_dev,        .free_dev =             xhci_free_dev,        .alloc_streams =        xhci_alloc_streams,        .free_streams =         xhci_free_streams,        .add_endpoint =         xhci_add_endpoint,        .drop_endpoint =        xhci_drop_endpoint,        .endpoint_disable =     xhci_endpoint_disable,        .endpoint_reset =       xhci_endpoint_reset,        .check_bandwidth =      xhci_check_bandwidth,        .reset_bandwidth =      xhci_reset_bandwidth,        .address_device =       xhci_address_device,        .enable_device =        xhci_enable_device,        .update_hub_device =    xhci_update_hub_device,        .reset_device =         xhci_discover_or_reset_device,        /*         * scheduling support         */        .get_frame_number =     xhci_get_frame,        /*         * root hub support         */        .hub_control =          xhci_hub_control,        .hub_status_data =      xhci_hub_status_data,        .bus_suspend =          xhci_bus_suspend,        .bus_resume =           xhci_bus_resume,        .get_resuming_ports =   xhci_get_resuming_ports,        /*         * call back when device connected and addressed         */        .update_device =        xhci_update_device,        .set_usb2_hw_lpm =      xhci_set_usb2_hardware_lpm,        .enable_usb3_lpm_timeout =      xhci_enable_usb3_lpm_timeout,        .disable_usb3_lpm_timeout =     xhci_disable_usb3_lpm_timeout,        .find_raw_port_number = xhci_find_raw_port_number,        .clear_tt_buffer_complete = xhci_clear_tt_buffer_complete,};Because various USB controllers can adopt the xHCI specification,to reduce boiler plate code,Linux developers already implemented the generic operationsrequired to support xHCI spec.When one has reference to the xhci_hc_driverit can utilize all xHCI provided functionalities and doesn’t need to implement xHCI protocol on its driver implementation once again.void xhci_init_driver(struct hc_driver *drv,                      const struct xhci_driver_overrides *over){        BUG_ON(!over);        /* Copy the generic table to drv then apply the overrides */        *drv = xhci_hc_driver;        if (over) {                drv-&gt;hcd_priv_size += over-&gt;extra_priv_size;                if (over-&gt;reset)                        drv-&gt;reset = over-&gt;reset;                if (over-&gt;start)                        drv-&gt;start = over-&gt;start;        }}EXPORT_SYMBOL_GPL(xhci_init_driver);The xHCI driver implementing the xhci_hc_driver doesn’t consume this structure,but provide it to other drivers who want to utilize the xHCI specification.In other words, xHCI driver is not designed to be bound to specific hardware module,but a just kernel level driver designed to supports other usb host controllers.To acheive that, it exports function xhci_init_driver.When other device driver invokes this function,the xhci_hc_driver’s reference is returned.xHCI platform driverstatic struct platform_driver usb_xhci_driver = {        .probe  = xhci_plat_probe,        .remove = xhci_plat_remove,        .shutdown = usb_hcd_platform_shutdown,        .driver = {                .name = \"xhci-hcd\",                .pm = &amp;xhci_plat_pm_ops,                .of_match_table = of_match_ptr(usb_xhci_of_match),                .acpi_match_table = ACPI_PTR(usb_xhci_acpi_match),        },};MODULE_ALIAS(\"platform:xhci-hcd\");static int __init xhci_plat_init(void){        xhci_init_driver(&amp;xhci_plat_hc_driver, &amp;xhci_plat_overrides);        return platform_driver_register(&amp;usb_xhci_driver);}void xhci_init_driver(struct hc_driver *drv,                      const struct xhci_driver_overrides *over){        BUG_ON(!over);        /* Copy the generic table to drv then apply the overrides */        *drv = xhci_hc_driver;        if (over) {                drv-&gt;hcd_priv_size += over-&gt;extra_priv_size;                if (over-&gt;reset)                        drv-&gt;reset = over-&gt;reset;                if (over-&gt;start)                        drv-&gt;start = over-&gt;start;        }}EXPORT_SYMBOL_GPL(xhci_init_driver);The paltform driver for xHCI host controller interfaceinvokes xhci_init_driver in its driver init function.This allows the xhci-hcd driver to get reference of the xHCI core object for hc_driver, and also register the driver itself as platform driver.Note that usb_xhci_driver is a driver for platform xHCI controller.DWC3DWC3 is a SuperSpeed USB 3.0 controller developed by the Synopsys DesignWare.In this posting this USB controller is used in our SoCand probed by the device tree.#ifdef CONFIG_OFstatic const struct of_device_id of_dwc3_match[] = {        {                .compatible = \"snps,dwc3\"        },        {                .compatible = \"synopsys,dwc3\"        },        { },};MODULE_DEVICE_TABLE(of, of_dwc3_match);static struct platform_driver dwc3_driver = {        .probe          = dwc3_probe,        .remove         = dwc3_remove,        .driver         = {                .name   = \"dwc3\",                .of_match_table = of_match_ptr(of_dwc3_match),                .acpi_match_table = ACPI_PTR(dwc3_acpi_match),                .pm     = &amp;dwc3_dev_pm_ops,        },};module_platform_driver(dwc3_driver);When a device node in the device treehas compatilbe string one of “snps,dwc3” or “synopsys,dwc3”the pre-designated probe function, dwc3_probe will be invoked.DWC3 probe functionLet’s take a look at what happens when the DWC3 controller is found.static int dwc3_probe(struct platform_device *pdev){        struct device           *dev = &amp;pdev-&gt;dev;        struct resource         *res, dwc_res;        struct dwc3             *dwc;        int                     ret;        void __iomem            *regs;        dwc = devm_kzalloc(dev, sizeof(*dwc), GFP_KERNEL);        if (!dwc)                return -ENOMEM;        dwc-&gt;dev = dev;        res = platform_get_resource(pdev, IORESOURCE_MEM, 0);        if (!res) {                dev_err(dev, \"missing memory resource\\n\");                return -ENODEV;        }        dwc-&gt;xhci_resources[0].start = res-&gt;start;        dwc-&gt;xhci_resources[0].end = dwc-&gt;xhci_resources[0].start +                                        DWC3_XHCI_REGS_END;        dwc-&gt;xhci_resources[0].flags = res-&gt;flags;        dwc-&gt;xhci_resources[0].name = res-&gt;name;        /*         * Request memory region but exclude xHCI regs,         * since it will be requested by the xhci-plat driver.         */        dwc_res = *res;        dwc_res.start += DWC3_GLOBALS_REGS_START;        regs = devm_ioremap_resource(dev, &amp;dwc_res);        if (IS_ERR(regs))                return PTR_ERR(regs);        dwc-&gt;regs       = regs;        dwc-&gt;regs_size  = resource_size(&amp;dwc_res);        dwc3_get_properties(dwc);        dma_set_mask_and_coherent(dev, DMA_BIT_MASK(dwc-&gt;dma_mask_bits));        dwc-&gt;reset = devm_reset_control_array_get(dev, true, true);        if (IS_ERR(dwc-&gt;reset))                return PTR_ERR(dwc-&gt;reset);        if (dev-&gt;of_node) {                ret = devm_clk_bulk_get_all(dev, &amp;dwc-&gt;clks);                if (ret == -EPROBE_DEFER)                        return ret;                /*                 * Clocks are optional, but new DT platforms should support all                 * clocks as required by the DT-binding.                 */                if (ret &lt; 0)                        dwc-&gt;num_clks = 0;                else                        dwc-&gt;num_clks = ret;        }        ret = reset_control_deassert(dwc-&gt;reset);        if (ret)                return ret;        ret = clk_bulk_prepare_enable(dwc-&gt;num_clks, dwc-&gt;clks);        if (ret)                goto assert_reset;        if (!dwc3_core_is_valid(dwc)) {                 dev_err(dwc-&gt;dev, \"this is not a DesignWare USB3 DRD Core\\n\");                ret = -ENODEV;                goto disable_clks;        }        platform_set_drvdata(pdev, dwc);        dwc3_cache_hwparams(dwc);        spin_lock_init(&amp;dwc-&gt;lock);        pm_runtime_set_active(dev);        pm_runtime_use_autosuspend(dev);        pm_runtime_set_autosuspend_delay(dev, DWC3_DEFAULT_AUTOSUSPEND_DELAY);        pm_runtime_enable(dev);        ret = pm_runtime_get_sync(dev);        if (ret &lt; 0)                goto err1;        pm_runtime_forbid(dev);        ret = dwc3_alloc_event_buffers(dwc, DWC3_EVENT_BUFFERS_SIZE);        if (ret) {                dev_err(dwc-&gt;dev, \"failed to allocate event buffers\\n\");                ret = -ENOMEM;                goto err2;        }        ret = dwc3_get_dr_mode(dwc);        if (ret)                goto err3;        ret = dwc3_alloc_scratch_buffers(dwc);        if (ret)                goto err3;        ret = dwc3_core_init(dwc);        if (ret) {                if (ret != -EPROBE_DEFER)                        dev_err(dev, \"failed to initialize core: %d\\n\", ret);                goto err4;        }        dwc3_check_params(dwc);        ret = dwc3_core_init_mode(dwc);        if (ret)                goto err5;        dwc3_debugfs_init(dwc);        pm_runtime_put(dev);        return 0;\t...}The first priority of the dwc3_probe function is retrieving the memory mapped address of the dwc3 USB controller. This address should be specified in the device node of the DWC3 controller. When you look at the binding of the DWC3,you can easily find that the first reg value of the DWC3 binding is a memory address of the DWC3 controller mapped on that system.Therefore, by invoking res = platform_get_resource(pdev, IORESOURCE_MEM, 0)you can retrieve the memory mapped address of the DWC3 controller.This address region not only contains xHCI information, but also DWC3 specific registers. Because we will defer to the xHCI driver on discovering its registers and configuring xHCI specific settings,we will skip the memory region containing the xHCI registersby adding predefined DWC3 offset(dwc_res.start += DWC3_GLOBALS_REGS_START).Because currently accessible address is physically mapped DWC3 register address,we need to let the kernel translate this address and generate kernel virtual address. To achieve it,it invoke sdevm_ioremap_resource function. After this function is invokes, we can access the DWC3 register as ifit resides on the virtual memory of the kernel.After the successful ioremap,dwc3_cache_hwparams function reads the DWC3 configuration registers and stores them in the dwc3 structure objectas a cache.The reason of having cache is reading those information from the actual memoryis much faster than reading them from the memory mapped DWC3’s actual registers.static void dwc3_cache_hwparams(struct dwc3 *dwc){        struct dwc3_hwparams    *parms = &amp;dwc-&gt;hwparams;        parms-&gt;hwparams0 = dwc3_readl(dwc-&gt;regs, DWC3_GHWPARAMS0);        parms-&gt;hwparams1 = dwc3_readl(dwc-&gt;regs, DWC3_GHWPARAMS1);        parms-&gt;hwparams2 = dwc3_readl(dwc-&gt;regs, DWC3_GHWPARAMS2);        parms-&gt;hwparams3 = dwc3_readl(dwc-&gt;regs, DWC3_GHWPARAMS3);        parms-&gt;hwparams4 = dwc3_readl(dwc-&gt;regs, DWC3_GHWPARAMS4);        parms-&gt;hwparams5 = dwc3_readl(dwc-&gt;regs, DWC3_GHWPARAMS5);        parms-&gt;hwparams6 = dwc3_readl(dwc-&gt;regs, DWC3_GHWPARAMS6);        parms-&gt;hwparams7 = dwc3_readl(dwc-&gt;regs, DWC3_GHWPARAMS7);        parms-&gt;hwparams8 = dwc3_readl(dwc-&gt;regs, DWC3_GHWPARAMS8);}The registers read by the above function are GHWPARAMS0 to GHWPARAMS7 which are Global Hardware Parameters registers.These registers contain all the informationrequired to initialize the DWC3 device driver.The detailed information about those registers are described in the DWC3 specification.The GHWPARAMS0 register read from dwc3_cache_hwparams functionis used to determine the mode of the DWC3 controller.There are three different types of mode:Device-only, Host-only, and  Dual-role device (DRD).static int dwc3_get_dr_mode(struct dwc3 *dwc){        enum usb_dr_mode mode;        struct device *dev = dwc-&gt;dev;        unsigned int hw_mode;        if (dwc-&gt;dr_mode == USB_DR_MODE_UNKNOWN)                dwc-&gt;dr_mode = USB_DR_MODE_OTG;        mode = dwc-&gt;dr_mode;        hw_mode = DWC3_GHWPARAMS0_MODE(dwc-&gt;hwparams.hwparams0);        switch (hw_mode) {        case DWC3_GHWPARAMS0_MODE_GADGET:                if (IS_ENABLED(CONFIG_USB_DWC3_HOST)) {                        dev_err(dev,                                \"Controller does not support host mode.\\n\");                        return -EINVAL;                }                mode = USB_DR_MODE_PERIPHERAL;                break;        case DWC3_GHWPARAMS0_MODE_HOST:                if (IS_ENABLED(CONFIG_USB_DWC3_GADGET)) {                        dev_err(dev,                                \"Controller does not support device mode.\\n\");                        return -EINVAL;                }                mode = USB_DR_MODE_HOST;                break;        default:                if (IS_ENABLED(CONFIG_USB_DWC3_HOST))                        mode = USB_DR_MODE_HOST;                else if (IS_ENABLED(CONFIG_USB_DWC3_GADGET))                        mode = USB_DR_MODE_PERIPHERAL;                /*                 * DWC_usb31 and DWC_usb3 v3.30a and higher do not support OTG                 * mode. If the controller supports DRD but the dr_mode is not                 * specified or set to OTG, then set the mode to peripheral.                 */                if (mode == USB_DR_MODE_OTG &amp;&amp;                    (!IS_ENABLED(CONFIG_USB_ROLE_SWITCH) ||                     !device_property_read_bool(dwc-&gt;dev, \"usb-role-switch\")) &amp;&amp;                    !DWC3_VER_IS_PRIOR(DWC3, 330A))                        mode = USB_DR_MODE_PERIPHERAL;        }        if (mode != dwc-&gt;dr_mode) {                dev_warn(dev,                         \"Configuration mismatch. dr_mode forced to %s\\n\",                         mode == USB_DR_MODE_HOST ? \"host\" : \"gadget\");                dwc-&gt;dr_mode = mode;        }        return 0;}The above function determines the operation mode of the DWC3Based on this mode, dwc3 core can be initialized in different way.tatic int dwc3_core_init_mode(struct dwc3 *dwc){        struct device *dev = dwc-&gt;dev;        int ret;        switch (dwc-&gt;dr_mode) {        case USB_DR_MODE_PERIPHERAL:                dwc3_set_prtcap(dwc, DWC3_GCTL_PRTCAP_DEVICE);                if (dwc-&gt;usb2_phy)                        otg_set_vbus(dwc-&gt;usb2_phy-&gt;otg, false);                phy_set_mode(dwc-&gt;usb2_generic_phy, PHY_MODE_USB_DEVICE);                phy_set_mode(dwc-&gt;usb3_generic_phy, PHY_MODE_USB_DEVICE);                ret = dwc3_gadget_init(dwc);                if (ret) {                        if (ret != -EPROBE_DEFER)                                dev_err(dev, \"failed to initialize gadget\\n\");                        eeturn ret;                }                break;        case USB_DR_MODE_HOST:                dwc3_set_prtcap(dwc, DWC3_GCTL_PRTCAP_HOST);                if (dwc-&gt;usb2_phy)                        otg_set_vbus(dwc-&gt;usb2_phy-&gt;otg, true);                phy_set_mode(dwc-&gt;usb2_generic_phy, PHY_MODE_USB_HOST);                phy_set_mode(dwc-&gt;usb3_generic_phy, PHY_MODE_USB_HOST);                ret = dwc3_host_init(dwc);                if (ret) {                        if (ret != -EPROBE_DEFER)                                dev_err(dev, \"failed to initialize host\\n\");                        return ret;                }                break;        case USB_DR_MODE_OTG:                INIT_WORK(&amp;dwc-&gt;drd_work, __dwc3_set_mode);                ret = dwc3_drd_init(dwc);                if (ret) {                        if (ret != -EPROBE_DEFER)                                dev_err(dev, \"failed to initialize dual-role\\n\");                        return ret;                }                break;        default:                dev_err(dev, \"Unsupported mode of operation %d\\n\", dwc-&gt;dr_mode);                return -EINVAL;        }        return 0;}When the dr_mode is set as USB_DR_MODE_HOST,it invokes dwc_host_init functionwhich register xHCI device!dwc3 host init-allocate xhci-hcd platform deviceint dwc3_host_init(struct dwc3 *dwc){        struct property_entry   props[4];        struct platform_device  *xhci;        int                     ret, irq;        struct resource         *res;        struct platform_device  *dwc3_pdev = to_platform_device(dwc-&gt;dev);        int                     prop_idx = 0;        irq = dwc3_host_get_irq(dwc);        if (irq &lt; 0)                return irq;        res = platform_get_resource_byname(dwc3_pdev, IORESOURCE_IRQ, \"host\");        if (!res)                res = platform_get_resource_byname(dwc3_pdev, IORESOURCE_IRQ,                                \"dwc_usb3\");        if (!res)                res = platform_get_resource(dwc3_pdev, IORESOURCE_IRQ, 0);        if (!res)                return -ENOMEM;        dwc-&gt;xhci_resources[1].start = irq;        dwc-&gt;xhci_resources[1].end = irq;        dwc-&gt;xhci_resources[1].flags = res-&gt;flags;        dwc-&gt;xhci_resources[1].name = res-&gt;name;        xhci = platform_device_alloc(\"xhci-hcd\", PLATFORM_DEVID_AUTO);        if (!xhci) {                dev_err(dwc-&gt;dev, \"couldn't allocate xHCI device\\n\");                return -ENOMEM;        }        xhci-&gt;dev.parent        = dwc-&gt;dev;        ACPI_COMPANION_SET(&amp;xhci-&gt;dev, ACPI_COMPANION(dwc-&gt;dev));        dwc-&gt;xhci = xhci;        ret = platform_device_add_resources(xhci, dwc-&gt;xhci_resources,                                                DWC3_XHCI_RESOURCES_NUM);        if (ret) {                dev_err(dwc-&gt;dev, \"couldn't add resources to xHCI device\\n\");                goto err;        }        memset(props, 0, sizeof(struct property_entry) * ARRAY_SIZE(props));        if (dwc-&gt;usb3_lpm_capable)                props[prop_idx++] = PROPERTY_ENTRY_BOOL(\"usb3-lpm-capable\");        if (dwc-&gt;usb2_lpm_disable)                props[prop_idx++] = PROPERTY_ENTRY_BOOL(\"usb2-lpm-disable\");        /**         * WORKAROUND: dwc3 revisions &lt;=3.00a have a limitation         * where Port Disable command doesn't work.         *         * The suggested workaround is that we avoid Port Disable         * completely.         *         * This following flag tells XHCI to do just that.         */        if (DWC3_VER_IS_WITHIN(DWC3, ANY, 300A))                props[prop_idx++] = PROPERTY_ENTRY_BOOL(\"quirk-broken-port-ped\");        if (prop_idx) {                ret = platform_device_add_properties(xhci, props);                if (ret) {                        dev_err(dwc-&gt;dev, \"failed to add properties to xHCI\\n\");                        goto err;                }        }        ret = platform_device_add(xhci);        if (ret) {                dev_err(dwc-&gt;dev, \"failed to register xHCI device\\n\");                goto err;        }        return 0;err:        platform_device_put(xhci);        return ret;}It invokes platform_device_alloc(“xhci-hcd”, PLATFORM_DEVID_AUTO) functionwhich allocates and register the platform device.Note that the device name “xhci-hcd” is the name of the device driver that we’ve explored before.Yes this is the name of usb_xhci_driverwhich will be used to bind the allocated device to the driver. After the xHCI device is allocated,it registers the generated device to the platform busby invoking platform_device_add function.This function invokes device_add function, andbecause the autoprobe flag is enabled for the platform bus,its corresponding driver’s bind function will be invoked.Let’s go back to usb_xhi_driver again!Because the generated platform device doesn’t have of_match table,it will utilize the name of the device “xhci-hcd” andwill bound to the usb_xhci_driver.static int xhci_plat_probe(struct platform_device *pdev){        const struct xhci_plat_priv *priv_match;        const struct hc_driver  *driver;        struct device           *sysdev, *tmpdev;        struct xhci_hcd         *xhci;        struct resource         *res;        struct usb_hcd          *hcd;        int                     ret;        int                     irq;        struct xhci_plat_priv   *priv = NULL;        if (usb_disabled())                return -ENODEV;        driver = &amp;xhci_plat_hc_driver;        irq = platform_get_irq(pdev, 0);        if (irq &lt; 0)                return irq;        /*         * sysdev must point to a device that is known to the system firmware         * or PCI hardware. We handle these three cases here:         * 1. xhci_plat comes from firmware         * 2. xhci_plat is child of a device from firmware (dwc3-plat)         * 3. xhci_plat is grandchild of a pci device (dwc3-pci)         */        for (sysdev = &amp;pdev-&gt;dev; sysdev; sysdev = sysdev-&gt;parent) {                if (is_of_node(sysdev-&gt;fwnode) ||                        is_acpi_device_node(sysdev-&gt;fwnode))                        break;#ifdef CONFIG_PCI                else if (sysdev-&gt;bus == &amp;pci_bus_type)                        break;#endif        }        if (!sysdev)                sysdev = &amp;pdev-&gt;dev;        /* Try to set 64-bit DMA first */        if (WARN_ON(!sysdev-&gt;dma_mask))                /* Platform did not initialize dma_mask */                ret = dma_coerce_mask_and_coherent(sysdev,                                                   DMA_BIT_MASK(64));        else                ret = dma_set_mask_and_coherent(sysdev, DMA_BIT_MASK(64));        /* If seting 64-bit DMA mask fails, fall back to 32-bit DMA mask */        if (ret) {                ret = dma_set_mask_and_coherent(sysdev, DMA_BIT_MASK(32));                if (ret)                        return ret;        }        pm_runtime_set_active(&amp;pdev-&gt;dev);        pm_runtime_enable(&amp;pdev-&gt;dev);        pm_runtime_get_noresume(&amp;pdev-&gt;dev);        hcd = __usb_create_hcd(driver, sysdev, &amp;pdev-&gt;dev,                               dev_name(&amp;pdev-&gt;dev), NULL);        if (!hcd) {                ret = -ENOMEM;                goto disable_runtime;        }        hcd-&gt;regs = devm_platform_get_and_ioremap_resource(pdev, 0, &amp;res);        if (IS_ERR(hcd-&gt;regs)) {                ret = PTR_ERR(hcd-&gt;regs);                goto put_hcd;        }        hcd-&gt;rsrc_start = res-&gt;start;        hcd-&gt;rsrc_len = resource_size(res);        xhci = hcd_to_xhci(hcd);        /*         * Not all platforms have clks so it is not an error if the         * clock do not exist.         */        xhci-&gt;reg_clk = devm_clk_get_optional(&amp;pdev-&gt;dev, \"reg\");        if (IS_ERR(xhci-&gt;reg_clk)) {                ret = PTR_ERR(xhci-&gt;reg_clk);                goto put_hcd;        }        ret = clk_prepare_enable(xhci-&gt;reg_clk);        if (ret)                goto put_hcd;        xhci-&gt;clk = devm_clk_get_optional(&amp;pdev-&gt;dev, NULL);        if (IS_ERR(xhci-&gt;clk)) {                ret = PTR_ERR(xhci-&gt;clk);                goto disable_reg_clk;        }        ret = clk_prepare_enable(xhci-&gt;clk);        if (ret)                goto disable_reg_clk;        if (pdev-&gt;dev.of_node)                priv_match = of_device_get_match_data(&amp;pdev-&gt;dev);        else                priv_match = dev_get_platdata(&amp;pdev-&gt;dev);        if (priv_match) {                priv = hcd_to_xhci_priv(hcd);                /* Just copy data for now */                *priv = *priv_match;        }        device_set_wakeup_capable(&amp;pdev-&gt;dev, true);        xhci-&gt;main_hcd = hcd;        xhci-&gt;shared_hcd = __usb_create_hcd(driver, sysdev, &amp;pdev-&gt;dev,                        dev_name(&amp;pdev-&gt;dev), hcd);        if (!xhci-&gt;shared_hcd) {                ret = -ENOMEM;                goto disable_clk;        }        /* imod_interval is the interrupt moderation value in nanoseconds. */        xhci-&gt;imod_interval = 40000;        /* Iterate over all parent nodes for finding quirks */        for (tmpdev = &amp;pdev-&gt;dev; tmpdev; tmpdev = tmpdev-&gt;parent) {                if (device_property_read_bool(tmpdev, \"usb2-lpm-disable\"))                        xhci-&gt;quirks |= XHCI_HW_LPM_DISABLE;                if (device_property_read_bool(tmpdev, \"usb3-lpm-capable\"))                        xhci-&gt;quirks |= XHCI_LPM_SUPPORT;                if (device_property_read_bool(tmpdev, \"quirk-broken-port-ped\"))                        xhci-&gt;quirks |= XHCI_BROKEN_PORT_PED;                device_property_read_u32(tmpdev, \"imod-interval-ns\",                                         &amp;xhci-&gt;imod_interval);        }        hcd-&gt;usb_phy = devm_usb_get_phy_by_phandle(sysdev, \"usb-phy\", 0);        if (IS_ERR(hcd-&gt;usb_phy)) {                ret = PTR_ERR(hcd-&gt;usb_phy);                if (ret == -EPROBE_DEFER)                        goto put_usb3_hcd;                hcd-&gt;usb_phy = NULL;        } else {                ret = usb_phy_init(hcd-&gt;usb_phy);                if (ret)                        goto put_usb3_hcd;        }        hcd-&gt;tpl_support = of_usb_host_tpl_support(sysdev-&gt;of_node);        xhci-&gt;shared_hcd-&gt;tpl_support = hcd-&gt;tpl_support;        if (priv &amp;&amp; (priv-&gt;quirks &amp; XHCI_SKIP_PHY_INIT))                hcd-&gt;skip_phy_initialization = 1;        if (priv &amp;&amp; (priv-&gt;quirks &amp; XHCI_SG_TRB_CACHE_SIZE_QUIRK))                xhci-&gt;quirks |= XHCI_SG_TRB_CACHE_SIZE_QUIRK;        ret = usb_add_hcd(hcd, irq, IRQF_SHARED);        if (ret)                goto disable_usb_phy;        if (HCC_MAX_PSA(xhci-&gt;hcc_params) &gt;= 4)                xhci-&gt;shared_hcd-&gt;can_do_streams = 1;        ret = usb_add_hcd(xhci-&gt;shared_hcd, irq, IRQF_SHARED);        if (ret)                goto dealloc_usb2_hcd;        device_enable_async_suspend(&amp;pdev-&gt;dev);        pm_runtime_put_noidle(&amp;pdev-&gt;dev);        /*         * Prevent runtime pm from being on as default, users should enable         * runtime pm using power/control in sysfs.         */        pm_runtime_forbid(&amp;pdev-&gt;dev);        return 0;dealloc_usb2_hcd:        usb_remove_hcd(hcd);disable_usb_phy:        usb_phy_shutdown(hcd-&gt;usb_phy);put_usb3_hcd:        usb_put_hcd(xhci-&gt;shared_hcd);disable_clk:        clk_disable_unprepare(xhci-&gt;clk);disable_reg_clk:        clk_disable_unprepare(xhci-&gt;reg_clk);put_hcd:        usb_put_hcd(hcd);disable_runtime:        pm_runtime_put_noidle(&amp;pdev-&gt;dev);        pm_runtime_disable(&amp;pdev-&gt;dev);        return ret;}The probe function firstly assigns the xhci_plat_hc_driver object to the driver local variable. Remember that xhci_plat_hc_driver object is intialized to contain the reference of xhci core hc_driverat the module loading time.xHCI platform probe - registering root hubXXXTODO!!###Registering root hub from the controller/** * usb_add_hcd - finish generic HCD structure initialization and register * @hcd: the usb_hcd structure to initialize * @irqnum: Interrupt line to allocate * @irqflags: Interrupt type flags * * Finish the remaining parts of generic HCD initialization: allocate the * buffers of consistent memory, register the bus, request the IRQ line, * and call the driver's reset() and start() routines. */int usb_add_hcd(struct usb_hcd *hcd,                unsigned int irqnum, unsigned long irqflags){        int retval;        struct usb_device *rhdev;        if (!hcd-&gt;skip_phy_initialization &amp;&amp; usb_hcd_is_primary_hcd(hcd)) {                hcd-&gt;phy_roothub = usb_phy_roothub_alloc(hcd-&gt;self.sysdev);                if (IS_ERR(hcd-&gt;phy_roothub))                        return PTR_ERR(hcd-&gt;phy_roothub);                retval = usb_phy_roothub_init(hcd-&gt;phy_roothub);                if (retval)                        return retval;                retval = usb_phy_roothub_set_mode(hcd-&gt;phy_roothub,                                                  PHY_MODE_USB_HOST_SS);                if (retval)                        retval = usb_phy_roothub_set_mode(hcd-&gt;phy_roothub,                                                          PHY_MODE_USB_HOST);                if (retval)                        goto err_usb_phy_roothub_power_on;                retval = usb_phy_roothub_power_on(hcd-&gt;phy_roothub);                if (retval)                        goto err_usb_phy_roothub_power_on;        }        dev_info(hcd-&gt;self.controller, \"%s\\n\", hcd-&gt;product_desc);        switch (authorized_default) {        case USB_AUTHORIZE_NONE:                hcd-&gt;dev_policy = USB_DEVICE_AUTHORIZE_NONE;                break;        case USB_AUTHORIZE_ALL:                hcd-&gt;dev_policy = USB_DEVICE_AUTHORIZE_ALL;                break;        case USB_AUTHORIZE_INTERNAL:                hcd-&gt;dev_policy = USB_DEVICE_AUTHORIZE_INTERNAL;                break;        case USB_AUTHORIZE_WIRED:        default:                hcd-&gt;dev_policy = hcd-&gt;wireless ?                        USB_DEVICE_AUTHORIZE_NONE : USB_DEVICE_AUTHORIZE_ALL;                break;        }        set_bit(HCD_FLAG_HW_ACCESSIBLE, &amp;hcd-&gt;flags);        /* per default all interfaces are authorized */        set_bit(HCD_FLAG_INTF_AUTHORIZED, &amp;hcd-&gt;flags);        /* HC is in reset state, but accessible.  Now do the one-time init,         * bottom up so that hcds can customize the root hubs before hub_wq         * starts talking to them.  (Note, bus id is assigned early too.)         */        retval = hcd_buffer_create(hcd);        if (retval != 0) {                dev_dbg(hcd-&gt;self.sysdev, \"pool alloc failed\\n\");                goto err_create_buf;        }        retval = usb_register_bus(&amp;hcd-&gt;self);        if (retval &lt; 0)                goto err_register_bus;        rhdev = usb_alloc_dev(NULL, &amp;hcd-&gt;self, 0);        if (rhdev == NULL) {                dev_err(hcd-&gt;self.sysdev, \"unable to allocate root hub\\n\");                retval = -ENOMEM;                goto err_allocate_root_hub;        }        mutex_lock(&amp;usb_port_peer_mutex);        hcd-&gt;self.root_hub = rhdev;        mutex_unlock(&amp;usb_port_peer_mutex);        rhdev-&gt;rx_lanes = 1;        rhdev-&gt;tx_lanes = 1;        switch (hcd-&gt;speed) {        case HCD_USB11:                rhdev-&gt;speed = USB_SPEED_FULL;                break;        case HCD_USB2:                rhdev-&gt;speed = USB_SPEED_HIGH;                break;        case HCD_USB25:                rhdev-&gt;speed = USB_SPEED_WIRELESS;                break;        case HCD_USB3:                rhdev-&gt;speed = USB_SPEED_SUPER;                break;        case HCD_USB32:                rhdev-&gt;rx_lanes = 2;                rhdev-&gt;tx_lanes = 2;                fallthrough;        case HCD_USB31:                rhdev-&gt;speed = USB_SPEED_SUPER_PLUS;                break;        default:                retval = -EINVAL;                goto err_set_rh_speed;        }        /* wakeup flag init defaults to \"everything works\" for root hubs,         * but drivers can override it in reset() if needed, along with         * recording the overall controller's system wakeup capability.         */        device_set_wakeup_capable(&amp;rhdev-&gt;dev, 1);        /* HCD_FLAG_RH_RUNNING doesn't matter until the root hub is         * registered.  But since the controller can die at any time,         * let's initialize the flag before touching the hardware.         */        set_bit(HCD_FLAG_RH_RUNNING, &amp;hcd-&gt;flags);        /* \"reset\" is misnamed; its role is now one-time init. the controller         * should already have been reset (and boot firmware kicked off etc).         */        if (hcd-&gt;driver-&gt;reset) {                retval = hcd-&gt;driver-&gt;reset(hcd);                if (retval &lt; 0) {                        dev_err(hcd-&gt;self.controller, \"can't setup: %d\\n\",                                        retval);                        goto err_hcd_driver_setup;                }        }        hcd-&gt;rh_pollable = 1;        retval = usb_phy_roothub_calibrate(hcd-&gt;phy_roothub);        if (retval)                goto err_hcd_driver_setup;        /* NOTE: root hub and controller capabilities may not be the same */        if (device_can_wakeup(hcd-&gt;self.controller)                        &amp;&amp; device_can_wakeup(&amp;hcd-&gt;self.root_hub-&gt;dev))                dev_dbg(hcd-&gt;self.controller, \"supports USB remote wakeup\\n\");        /* initialize tasklets */        init_giveback_urb_bh(&amp;hcd-&gt;high_prio_bh);        init_giveback_urb_bh(&amp;hcd-&gt;low_prio_bh);        /* enable irqs just before we start the controller,         * if the BIOS provides legacy PCI irqs.         */        if (usb_hcd_is_primary_hcd(hcd) &amp;&amp; irqnum) {                retval = usb_hcd_request_irqs(hcd, irqnum, irqflags);                if (retval)                        goto err_request_irq;        }        hcd-&gt;state = HC_STATE_RUNNING;        retval = hcd-&gt;driver-&gt;start(hcd);        if (retval &lt; 0) {                dev_err(hcd-&gt;self.controller, \"startup error %d\\n\", retval);                goto err_hcd_driver_start;        }        /* starting here, usbcore will pay attention to this root hub */        retval = register_root_hub(hcd);        if (retval != 0)                goto err_register_root_hub;        if (hcd-&gt;uses_new_polling &amp;&amp; HCD_POLL_RH(hcd))                usb_hcd_poll_rh_status(hcd);        return retval;err_register_root_hub:        hcd-&gt;rh_pollable = 0;        clear_bit(HCD_FLAG_POLL_RH, &amp;hcd-&gt;flags);        del_timer_sync(&amp;hcd-&gt;rh_timer);        hcd-&gt;driver-&gt;stop(hcd);        hcd-&gt;state = HC_STATE_HALT;        clear_bit(HCD_FLAG_POLL_RH, &amp;hcd-&gt;flags);        del_timer_sync(&amp;hcd-&gt;rh_timer);err_hcd_driver_start:        if (usb_hcd_is_primary_hcd(hcd) &amp;&amp; hcd-&gt;irq &gt; 0)                free_irq(irqnum, hcd);err_request_irq:err_hcd_driver_setup:err_set_rh_speed:        usb_put_invalidate_rhdev(hcd);err_allocate_root_hub:        usb_deregister_bus(&amp;hcd-&gt;self);err_register_bus:        hcd_buffer_destroy(hcd);err_create_buf:        usb_phy_roothub_power_off(hcd-&gt;phy_roothub);err_usb_phy_roothub_power_on:        usb_phy_roothub_exit(hcd-&gt;phy_roothub);        return retval;}EXPORT_SYMBOL_GPL(usb_add_hcd);/** * register_root_hub - called by usb_add_hcd() to register a root hub * @hcd: host controller for this root hub * * This function registers the root hub with the USB subsystem.  It sets up * the device properly in the device tree and then calls usb_new_device() * to register the usb device.  It also assigns the root hub's USB address * (always 1). * * Return: 0 if successful. A negative error code otherwise. */static int register_root_hub(struct usb_hcd *hcd){        struct device *parent_dev = hcd-&gt;self.controller;        struct usb_device *usb_dev = hcd-&gt;self.root_hub;        const int devnum = 1;        int retval;        usb_dev-&gt;devnum = devnum;        usb_dev-&gt;bus-&gt;devnum_next = devnum + 1;        set_bit (devnum, usb_dev-&gt;bus-&gt;devmap.devicemap);        usb_set_device_state(usb_dev, USB_STATE_ADDRESS);        mutex_lock(&amp;usb_bus_idr_lock);        usb_dev-&gt;ep0.desc.wMaxPacketSize = cpu_to_le16(64);        retval = usb_get_device_descriptor(usb_dev, USB_DT_DEVICE_SIZE);        if (retval != sizeof usb_dev-&gt;descriptor) {                mutex_unlock(&amp;usb_bus_idr_lock);                dev_dbg (parent_dev, \"can't read %s device descriptor %d\\n\",                                dev_name(&amp;usb_dev-&gt;dev), retval);                return (retval &lt; 0) ? retval : -EMSGSIZE;        }        if (le16_to_cpu(usb_dev-&gt;descriptor.bcdUSB) &gt;= 0x0201) {                retval = usb_get_bos_descriptor(usb_dev);                if (!retval) {                        usb_dev-&gt;lpm_capable = usb_device_supports_lpm(usb_dev);                } else if (usb_dev-&gt;speed &gt;= USB_SPEED_SUPER) {                        mutex_unlock(&amp;usb_bus_idr_lock);                        dev_dbg(parent_dev, \"can't read %s bos descriptor %d\\n\",                                        dev_name(&amp;usb_dev-&gt;dev), retval);                        return retval;                }        }        retval = usb_new_device (usb_dev);        if (retval) {                dev_err (parent_dev, \"can't register root hub for %s, %d\\n\",                                dev_name(&amp;usb_dev-&gt;dev), retval);        } else {                spin_lock_irq (&amp;hcd_root_hub_lock);                hcd-&gt;rh_registered = 1;                spin_unlock_irq (&amp;hcd_root_hub_lock);                /* Did the HC die before the root hub was registered? */                if (HCD_DEAD(hcd))                        usb_hc_died (hcd);      /* This time clean up */        }        mutex_unlock(&amp;usb_bus_idr_lock);        return retval;}##When xHCI platform driver can be probed???"
  },
  
  {
    "title": "Register Platform Device Driver",
    "url": "/posts/register-platform-device-driver/",
    "categories": "linux,, embedded-linux",
    "tags": "",
    "date": "2021-05-02 00:00:00 -0400",
    





    
    "snippet": "We will cover how the platform device drivers can be registered and managed by the platform device bus subsystem.struct platform_driver {        int (*probe)(struct platform_device *);        int (...",
    "content": "We will cover how the platform device drivers can be registered and managed by the platform device bus subsystem.struct platform_driver {        int (*probe)(struct platform_device *);        int (*remove)(struct platform_device *);        void (*shutdown)(struct platform_device *);        int (*suspend)(struct platform_device *, pm_message_t state);        int (*resume)(struct platform_device *);        struct device_driver driver;        const struct platform_device_id *id_table;        bool prevent_deferred_probe;}/* module_platform_driver() - Helper macro for drivers that don't do * anything special in module init/exit.  This eliminates a lot of * boilerplate.  Each module may only use this macro once, and * calling it replaces module_init() and module_exit() */#define module_platform_driver(__platform_driver) \\        module_driver(__platform_driver, platform_driver_register, \\                        platform_driver_unregister)/* * use a macro to avoid include chaining to get THIS_MODULE */#define platform_driver_register(drv) \\        __platform_driver_register(drv, THIS_MODULE)extern int __platform_driver_register(struct platform_driver *,                                        struct module *);extern void platform_driver_unregister(struct platform_driver *);Every device driver who want to register their driver as platform deviceshould utilize pre-defined macro or invoke proper platform driver registration APIsat their driver initialization code.%Most kernel device driver utilizes the macro compared to have their own APIs to register the device driverbecause they don’t require any other complicated operations except registering the driver.Therefore, utilizing the macro will remove boilerplate code required for registering your driver to platform driverand make code looks simple.Above macro automatically generates initialization and de-initialization function foryour platform device driver.It just invokes platform_driver_register and platform_driver_unregister function on module init and exit respectively. Both macro requires a platform_driver structure object, andpopulating proper platform_driver structure representing current device drivershould be done by the device driver itself.We will take a look at platform device driver registeration process in detail./** * __platform_driver_register - register a driver for platform-level devices * @drv: platform driver structure * @owner: owning module/driver */int __platform_driver_register(struct platform_driver *drv,                                struct module *owner){        drv-&gt;driver.owner = owner;        drv-&gt;driver.bus = &amp;platform_bus_type;         return driver_register(&amp;drv-&gt;driver);}     Even though the device driver should provide the platform_driver and its associated call-back functions,it is not driver’s duty to allocate a driver structure used for actual device driver registration process.Therefore, the above function and further apis for platform devicewill populate the driver structure.Also, because driver registration process utilize the common driver core apis that can be used for every driver registration regardless of the bus type,it should set the generic driver object and pass it to the registration api.First of all,the bus_type object for platform device, platform_bus_type should be set as its busbecause this driver is supposed to support platform devices,After the member field of the generic driver object has been filled out,it invoked driver_register api./** * driver_register - register driver with bus * @drv: driver to register * * We pass off most of the work to the bus_add_driver() call, * since most of the things we have to do deal with the bus * structures. */int driver_register(struct device_driver *drv){        int ret;        struct device_driver *other;         if (!drv-&gt;bus-&gt;p) {                pr_err(\"Driver '%s' was unable to register with bus_type '%s' because the bus was not initialized.\\n\",                           drv-&gt;name, drv-&gt;bus-&gt;name);                return -EINVAL;        }         if ((drv-&gt;bus-&gt;probe &amp;&amp; drv-&gt;probe) ||            (drv-&gt;bus-&gt;remove &amp;&amp; drv-&gt;remove) ||            (drv-&gt;bus-&gt;shutdown &amp;&amp; drv-&gt;shutdown))                pr_warn(\"Driver '%s' needs updating - please use \"                        \"bus_type methods\\n\", drv-&gt;name);                other = driver_find(drv-&gt;name, drv-&gt;bus);        if (other) {                pr_err(\"Error: Driver '%s' is already registered, \"                        \"aborting...\\n\", drv-&gt;name);                return -EBUSY;        }                ret = bus_add_driver(drv);        if (ret)                return ret;        ret = driver_add_groups(drv, drv-&gt;groups);        if (ret) {                bus_remove_driver(drv);                return ret;        }        kobject_uevent(&amp;drv-&gt;p-&gt;kobj, KOBJ_ADD);                return ret;}EXPORT_SYMBOL_GPL(driver_register);Note that driver_register is not bus-specific api to register the driver.It is a generic api to register device driver structure to the bus.This function firstly checks whether the bus_type has been properly initialized and registered to the system by checking the presence of private sub-system of the bus. To understand how the private subsystem has been allocated for a bus,you might want to check previous posting.And then not to register same named device driver more than once,it checks if the bus attached to the device_driver structure already has the same named device driver. If there is nothing, then it delegatesmost of the driver registering processto bus_add_driver function.struct driver_private {        struct kobject kobj;        struct klist klist_devices;        struct klist_node knode_bus;        struct module_kobject *mkobj;        struct device_driver *driver;};/** * bus_add_driver - Add a driver to the bus. * @drv: driver. */int bus_add_driver(struct device_driver *drv){        struct bus_type *bus;        struct driver_private *priv;        int error = 0;        bus = bus_get(drv-&gt;bus);        if (!bus)                return -EINVAL;        pr_debug(\"bus: '%s': add driver %s\\n\", bus-&gt;name, drv-&gt;name);        priv = kzalloc(sizeof(*priv), GFP_KERNEL);        if (!priv) {                error = -ENOMEM;                goto out_put_bus;        }        klist_init(&amp;priv-&gt;klist_devices, NULL, NULL);        priv-&gt;driver = drv;        drv-&gt;p = priv;        priv-&gt;kobj.kset = bus-&gt;p-&gt;drivers_kset;        error = kobject_init_and_add(&amp;priv-&gt;kobj, &amp;driver_ktype, NULL,                                     \"%s\", drv-&gt;name);        if (error)                goto out_unregister;        klist_add_tail(&amp;priv-&gt;knode_bus, &amp;bus-&gt;p-&gt;klist_drivers);        if (drv-&gt;bus-&gt;p-&gt;drivers_autoprobe) {                error = driver_attach(drv);                if (error)                        goto out_unregister;        }        module_add_driver(drv-&gt;owner, drv);        error = driver_create_file(drv, &amp;driver_attr_uevent);        if (error) {                printk(KERN_ERR \"%s: uevent attr (%s) failed\\n\",                        __func__, drv-&gt;name);        }        error = driver_add_groups(drv, bus-&gt;drv_groups);        if (error) {                /* How the hell do we get out of this pickle? Give up */                printk(KERN_ERR \"%s: driver_create_groups(%s) failed\\n\",                        __func__, drv-&gt;name);        }        if (!drv-&gt;suppress_bind_attrs) {                error = add_bind_files(drv);                if (error) {                        /* Ditto */                        printk(KERN_ERR \"%s: add_bind_files(%s) failed\\n\",                                __func__, drv-&gt;name);                }        }        return 0;out_unregister:        kobject_put(&amp;priv-&gt;kobj);        /* drv-&gt;p is freed in driver_release()  */        drv-&gt;p = NULL;out_put_bus:        bus_put(bus);        return error;}The first thing done by the bus_add_driver function is allocating driver’s private data used to memorize the driver specific informationIn detail,driver_private is used to manage those private information related to current device driver.For example,it has klist_devices klist tellingwhich devices has been bound to current driver and knode_bus which is a knode object of the busthat we are trying to register our driver toIt resets the klist_devices list first using klist_init functionbecause there should be no devices attached to current deviceAfter the driver_private object has been initialized, it should be added to the driver.And then we need to register our driver to the bus.To do that klist_add_tail function willadd our driver_private’s knodw to the klist_driver list of the subsystem of the target bus.%When the bus has been initialized if the bus has been configured to probe the deviceat every driver registration (drivers_autoprobe flag)it invokes the driver_attach functionto check if there exists device hat can be bound to the newly registered driver./** * driver_attach - try to bind driver to devices. * @drv: driver. * * Walk the list of devices that the bus has on it and try to * match the driver with each one.  If driver_probe_device() * returns 0 and the @dev-&gt;driver is set, we've found a * compatible pair. */int driver_attach(struct device_driver *drv){        return bus_for_each_dev(drv-&gt;bus, NULL, drv, __driver_attach);}EXPORT_SYMBOL_GPL(driver_attach);static int __driver_attach(struct device *dev, void *data){        struct device_driver *drv = data;        int ret;        /*         * Lock device and try to bind to it. We drop the error         * here and always return 0, because we need to keep trying         * to bind to devices and some drivers will return an error         * simply if it didn't support the device.         *         * driver_probe_device() will spit a warning if there         * is an error.         */        ret = driver_match_device(drv, dev);        if (ret == 0) {                /* no match */                return 0;        } else if (ret == -EPROBE_DEFER) {                dev_dbg(dev, \"Device match requests probe deferral\\n\");                driver_deferred_probe_add(dev);        } else if (ret &lt; 0) {                dev_dbg(dev, \"Bus failed to match device: %d\\n\", ret);                return ret;        } /* ret &gt; 0 means positive match */        if (driver_allows_async_probing(drv)) {                /*                 * Instead of probing the device synchronously we will                 * probe it asynchronously to allow for more parallelism.                 *                 * We only take the device lock here in order to guarantee                 * that the dev-&gt;driver and async_driver fields are protected                 */                dev_dbg(dev, \"probing driver %s asynchronously\\n\", drv-&gt;name);                device_lock(dev);                if (!dev-&gt;driver) {                        get_device(dev);                        dev-&gt;p-&gt;async_driver = drv;                        async_schedule_dev(__driver_attach_async_helper, dev);                }                device_unlock(dev);                return 0;        }        device_driver_attach(drv, dev);        return 0;}driver_attach function invokes __driver_attach function against all devices registered to the bus our device driver attached.The driver_match_device matches the driver with the tarversed device(we covered the details about driver_match_device in previous posting).If the matching device found,it invokes device_driver_attach to manually bind device to the driver./** * device_driver_attach - attach a specific driver to a specific device * @drv: Driver to attach * @dev: Device to attach it to * * Manually attach driver to a device. Will acquire both @dev lock and * @dev-&gt;parent lock if needed. */int device_driver_attach(struct device_driver *drv, struct device *dev){        int ret = 0;        __device_driver_lock(dev, dev-&gt;parent);        /*         * If device has been removed or someone has already successfully         * bound a driver before us just skip the driver probe call.         */        if (!dev-&gt;p-&gt;dead &amp;&amp; !dev-&gt;driver)                ret = driver_probe_device(drv, dev);        __device_driver_unlock(dev, dev-&gt;parent);        return ret;}If the device is not dead and has not been bound to any device driver,then it invokes driver_probe_device to actually bind the device to driver.All the details are already covered in the previous posting.That’s it! We registered our platform device driver to the platform busXXX:move to other posting. 399 /** 400  * bus_for_each_drv - driver iterator 401  * @bus: bus we're dealing with. 402  * @start: driver to start iterating on. 403  * @data: data to pass to the callback. 404  * @fn: function to call for each driver. 405  * 406  * This is nearly identical to the device iterator above. 407  * We iterate over each driver that belongs to @bus, and call 408  * @fn for each. If @fn returns anything but 0, we break out 409  * and return it. If @start is not NULL, we use it as the head 410  * of the list. 411  * 412  * NOTE: we don't return the driver that returns a non-zero 413  * value, nor do we leave the reference count incremented for that 414  * driver. If the caller needs to know that info, it must set it 415  * in the callback. It must also be sure to increment the refcount 416  * so it doesn't disappear before returning to the caller. 417  */ 418 int bus_for_each_drv(struct bus_type *bus, struct device_driver *start, 419                      void *data, int (*fn)(struct device_driver *, void *)) 420 { 421         struct klist_iter i; 422         struct device_driver *drv; 423         int error = 0; 424  425         if (!bus) 426                 return -EINVAL; 427  428         klist_iter_init_node(&amp;bus-&gt;p-&gt;klist_drivers, &amp;i, 429                              start ? &amp;start-&gt;p-&gt;knode_bus : NULL); 430         while ((drv = next_driver(&amp;i)) &amp;&amp; !error) 431                 error = fn(drv, data); 432         klist_iter_exit(&amp;i); 433         return error; 434 } 387 static struct device_driver *next_driver(struct klist_iter *i) 388 { 389         struct klist_node *n = klist_next(i); 390         struct driver_private *drv_priv; 391  392         if (n) { 393                 drv_priv = container_of(n, struct driver_private, knode_bus); 394                 return drv_priv-&gt;driver; 395         } 396         return NULL; 397 }For example, bus core provides api called bus_for_each_drvto run a function against every device driver registered for a bus.It internally invokes next_driver function,and this function can retrieve the driver by making use of container_of macro.Here, klist_iter is used to traverse klist_node of each device driver associated with current bus.Also note that the traversed klist is klist_drivers,which is the member field of subsys_private structure of the bus_type structure.Therefore, whenever klist_next function is invoked,it returns one klist_node associated with one device driverwe’ve registered to the bus before.Note that this klist_node is the memeber field of driver_privatewe’ve set in the bus_add_driver function.As I told before, when we have a reference to klist_node of the driver,we can retrieve the driver_private structure,and using this reference, we can return the devive driver object itself. Note that this device driver structure is the wrapper device driver, drvwrapthat we generated in the usb_register_driver.When drivers_autoprobe has been set, it tries to bind the devices sitting on the bus with the new registered driver at the driver register time. When you go back to the bus_register function, you can find that the drivers_autoprobe flag has been set by default.Therefore, whenever the new device driver is trying to be registered to the bus,it will try to bind the driver to the already found devices. We didn’t cover the detail implementation of the driver_attach function,but it invokes binding function against the every registered devices on the busthat are managed by the klist."
  },
  
  {
    "title": "Platform Device",
    "url": "/posts/platform-device/",
    "categories": "linux,, embedded-linux",
    "tags": "",
    "date": "2021-05-01 00:00:00 -0400",
    





    
    "snippet": "Kernel initialization before DeviceTreeAlthough we are not going to cover the details of the initialization procedure,this post will take a look at what happens before the device tree is initialize...",
    "content": "Kernel initialization before DeviceTreeAlthough we are not going to cover the details of the initialization procedure,this post will take a look at what happens before the device tree is initialized.        __HEAD        /*         * DO NOT MODIFY. Image header expected by Linux boot-loaders.         */        efi_signature_nop                       // special NOP to identity as PE/COFF executable        b       primary_entry                   // branch to kernel start, magic        .quad   0                               // Image load offset from start of RAM, little-endian        le64sym _kernel_size_le                 // Effective size of kernel image, little-endian        le64sym _kernel_flags_le                // Informative flags, little-endian        .quad   0                               // reserved        .quad   0                               // reserved        .quad   0                               // reserved        .ascii  ARM64_IMAGE_MAGIC               // Magic number        .long   .Lpe_header_offset              // Offset to the PE header.        __EFI_PE_HEADER        __INIT        /*         * The following callee saved general purpose registers are used on the         * primary lowlevel boot path:         *         *  Register   Scope                      Purpose         *  x21        primary_entry() .. start_kernel()        FDT pointer passed at boot in x0         *  x23        primary_entry() .. start_kernel()        physical misalignment/KASLR offset         *  x28        __create_page_tables()                   callee preserved temp register         *  x19/x20    __primary_switch()                       callee preserved temp registers         *  x24        __primary_switch() .. relocate_kernel()  current RELR displacement         */SYM_CODE_START(primary_entry)        bl      preserve_boot_args        bl      init_kernel_el                  // w0=cpu_boot_mode        adrp    x23, __PHYS_OFFSET        and     x23, x23, MIN_KIMG_ALIGN - 1    // KASLR offset, defaults to 0        bl      set_cpu_boot_mode_flag        bl      __create_page_tables        /*         * The following calls CPU setup code, see arch/arm64/mm/proc.S for         * details.         * On return, the CPU will be ready for the MMU to be turned on and         * the TCR will have been set.         */        bl      __cpu_setup                     // initialise processor        b       __primary_switchSYM_CODE_END(primary_entry)SYM_FUNC_START_LOCAL(__primary_switch)#ifdef CONFIG_RANDOMIZE_BASE        mov     x19, x0                         // preserve new SCTLR_EL1 value        mrs     x20, sctlr_el1                  // preserve old SCTLR_EL1 value#endif        adrp    x1, init_pg_dir        bl      __enable_mmu#ifdef CONFIG_RELOCATABLE#ifdef CONFIG_RELR        mov     x24, #0                         // no RELR displacement yet#endif        bl      __relocate_kernel#ifdef CONFIG_RANDOMIZE_BASE        ldr     x8, =__primary_switched        adrp    x0, __PHYS_OFFSET        blr     x8        /*         * If we return here, we have a KASLR displacement in x23 which we need         * to take into account by discarding the current kernel mapping and         * creating a new one.         */        pre_disable_mmu_workaround        msr     sctlr_el1, x20                  // disable the MMU        isb        bl      __create_page_tables            // recreate kernel mapping        tlbi    vmalle1                         // Remove any stale TLB entries        dsb     nsh        msr     sctlr_el1, x19                  // re-enable the MMU        isb        ic      iallu                           // flush instructions fetched        dsb     nsh                             // via old mapping        isb        bl      __relocate_kernel#endif#endif        ldr     x8, =__primary_switched        adrp    x0, __PHYS_OFFSET        br      x8SYM_FUNC_END(__primary_switch)/* * The following fragment of code is executed with the MMU enabled. * *   x0 = __PHYS_OFFSET */SYM_FUNC_START_LOCAL(__primary_switched)        adrp    x4, init_thread_union        add     sp, x4, #THREAD_SIZE        adr_l   x5, init_task        msr     sp_el0, x5                      // Save thread_info#ifdef CONFIG_ARM64_PTR_AUTH        __ptrauth_keys_init_cpu x5, x6, x7, x8#endif        adr_l   x8, vectors                     // load VBAR_EL1 with virtual        msr     vbar_el1, x8                    // vector table address        isb        stp     xzr, x30, [sp, #-16]!        mov     x29, sp#ifdef CONFIG_SHADOW_CALL_STACK        adr_l   scs_sp, init_shadow_call_stack  // Set shadow call stack#endif        str_l   x21, __fdt_pointer, x5          // Save FDT pointer        ldr_l   x4, kimage_vaddr                // Save the offset between        sub     x4, x4, x0                      // the kernel virtual and        str_l   x4, kimage_voffset, x5          // physical mappings        // Clear BSS        adr_l   x0, __bss_start        mov     x1, xzr        adr_l   x2, __bss_stop        sub     x2, x2, x0        bl      __pi_memset        dsb     ishst                           // Make zero page visible to PTW#if defined(CONFIG_KASAN_GENERIC) || defined(CONFIG_KASAN_SW_TAGS)        bl      kasan_early_init#endif#ifdef CONFIG_RANDOMIZE_BASE        tst     x23, ~(MIN_KIMG_ALIGN - 1)      // already running randomized?        b.ne    0f        mov     x0, x21                         // pass FDT address in x0        bl      kaslr_early_init                // parse FDT for KASLR options        cbz     x0, 0f                          // KASLR disabled? just proceed        orr     x23, x23, x0                    // record KASLR offset        ldp     x29, x30, [sp], #16             // we must enable KASLR, return        ret                                     // to __primary_switch()0:#endif        add     sp, sp, #16        mov     x29, #0        mov     x30, #0        b       start_kernelSYM_FUNC_END(__primary_switched)After the processor has been initialized andsets up the execution environment,it jumps to the first C programmed function, start_kernel.asmlinkage __visible void __init __no_sanitize_address start_kernel(void){        char *command_line;        char *after_dashes;        set_task_stack_end_magic(&amp;init_task);        smp_setup_processor_id();        debug_objects_early_init();        cgroup_init_early();        local_irq_disable();        early_boot_irqs_disabled = true;        /*         * Interrupts are still disabled. Do necessary setups, then         * enable them.         */        boot_cpu_init();        page_address_init();        pr_notice(\"%s\", linux_banner);        early_security_init();        setup_arch(&amp;command_line);        setup_boot_config(command_line);        setup_command_line(command_line);        setup_nr_cpu_ids();        setup_per_cpu_areas();        smp_prepare_boot_cpu(); /* arch-specific boot-cpu hooks */        boot_cpu_hotplug_init();        build_all_zonelists(NULL);        page_alloc_init();        pr_notice(\"Kernel command line: %s\\n\", saved_command_line);        /* parameters may set static keys */        jump_label_init();        parse_early_param();        after_dashes = parse_args(\"Booting kernel\",                                  static_command_line, __start___param,                                  __stop___param - __start___param,                                  -1, -1, NULL, &amp;unknown_bootoption);        if (!IS_ERR_OR_NULL(after_dashes))                parse_args(\"Setting init args\", after_dashes, NULL, 0, -1, -1,                           NULL, set_init_arg);        if (extra_init_args)                parse_args(\"Setting extra init args\", extra_init_args,                           NULL, 0, -1, -1, NULL, set_init_arg);        /*         * These use large bootmem allocations and must precede         * kmem_cache_init()         */        setup_log_buf(0);        vfs_caches_init_early();        sort_main_extable();        trap_init();        mm_init();        ftrace_init();        /* trace_printk can be enabled here */        early_trace_init();        /*         * Set up the scheduler prior starting any interrupts (such as the         * timer interrupt). Full topology setup happens at smp_init()         * time - but meanwhile we still have a functioning scheduler.         */        sched_init();        /*         * Disable preemption - early bootup scheduling is extremely         * fragile until we cpu_idle() for the first time.         */        preempt_disable();        if (WARN(!irqs_disabled(),                 \"Interrupts were enabled *very* early, fixing it\\n\"))                local_irq_disable();        radix_tree_init();        /*         * Set up housekeeping before setting up workqueues to allow the unbound         * workqueue to take non-housekeeping into account.         */        housekeeping_init();        /*         * Allow workqueue creation and work item queueing/cancelling         * early.  Work item execution depends on kthreads and starts after         * workqueue_init().         */        workqueue_init_early();        rcu_init();        /* Trace events are available after this */        trace_init();        if (initcall_debug)                initcall_debug_enable();        context_tracking_init();        /* init some links before init_ISA_irqs() */        early_irq_init();        init_IRQ();        tick_init();        rcu_init_nohz();        init_timers();        hrtimers_init();        softirq_init();        timekeeping_init();        /*         * For best initial stack canary entropy, prepare it after:         * - setup_arch() for any UEFI RNG entropy and boot cmdline access         * - timekeeping_init() for ktime entropy used in rand_initialize()         * - rand_initialize() to get any arch-specific entropy like RDRAND         * - add_latent_entropy() to get any latent entropy         * - adding command line entropy         */        rand_initialize();        add_latent_entropy();        add_device_randomness(command_line, strlen(command_line));        boot_init_stack_canary();        time_init();        perf_event_init();        profile_init();        call_function_init();        WARN(!irqs_disabled(), \"Interrupts were enabled early\\n\");        early_boot_irqs_disabled = false;        local_irq_enable();        kmem_cache_init_late();        /*         * HACK ALERT! This is early. We're enabling the console before         * we've done PCI setups etc, and console_init() must be aware of         * this. But we do want output early, in case something goes wrong.         */        console_init();        if (panic_later)                panic(\"Too many boot %s vars at `%s'\", panic_later,                      panic_param);        lockdep_init();        /*         * Need to run this when irqs are enabled, because it wants         * to self-test [hard/soft]-irqs on/off lock inversion bugs         * too:         */        locking_selftest();        /*         * This needs to be called before any devices perform DMA         * operations that might use the SWIOTLB bounce buffers. It will         * mark the bounce buffers as decrypted so that their usage will         * not cause \"plain-text\" data to be decrypted when accessed.         */        mem_encrypt_init();#ifdef CONFIG_BLK_DEV_INITRD        if (initrd_start &amp;&amp; !initrd_below_start_ok &amp;&amp;            page_to_pfn(virt_to_page((void *)initrd_start)) &lt; min_low_pfn) {                pr_crit(\"initrd overwritten (0x%08lx &lt; 0x%08lx) - disabling it.\\n\",                    page_to_pfn(virt_to_page((void *)initrd_start)),                    min_low_pfn);                initrd_start = 0;        }#endif        setup_per_cpu_pageset();        numa_policy_init();        acpi_early_init();        if (late_time_init)                late_time_init();        sched_clock_init();        calibrate_delay();        pid_idr_init();        anon_vma_init();#ifdef CONFIG_X86        if (efi_enabled(EFI_RUNTIME_SERVICES))                efi_enter_virtual_mode();#endif        thread_stack_cache_init();        cred_init();        fork_init();        proc_caches_init();        uts_ns_init();        key_init();        security_init();        dbg_late_init();        vfs_caches_init();        pagecache_init();        signals_init();        seq_file_init();        proc_root_init();        nsfs_init();        cpuset_init();        cgroup_init();        taskstats_init_early();        delayacct_init();        poking_init();        check_bugs();        acpi_subsystem_init();        arch_post_acpi_subsys_init();        sfi_init_late();        kcsan_init();        /* Do the rest non-__init'ed, we're now alive */        arch_call_rest_init();        prevent_tail_call_optimization();}Although there are so many initialization function for kernel,what we have interest in now is the last init function,arch_call_rest_init which invokes rest_init function.void __init __weak arch_call_rest_init(void){        rest_init();}/* * We need to finalize in a non-__init function or else race conditions * between the root thread and the init thread may cause start_kernel to * be reaped by free_initmem before the root thread has proceeded to * cpu_idle. * * gcc-3.4 accidentally inlines this function, so use noinline. */static __initdata DECLARE_COMPLETION(kthreadd_done);noinline void __ref rest_init(void){        struct task_struct *tsk;        int pid;        rcu_scheduler_starting();        /*         * We need to spawn init first so that it obtains pid 1, however         * the init task will end up wanting to create kthreads, which, if         * we schedule it before we create kthreadd, will OOPS.         */        pid = kernel_thread(kernel_init, NULL, CLONE_FS);        /*         * Pin init on the boot CPU. Task migration is not properly working         * until sched_init_smp() has been run. It will set the allowed         * CPUs for init to the non isolated CPUs.         */        rcu_read_lock();        tsk = find_task_by_pid_ns(pid, &amp;init_pid_ns);        set_cpus_allowed_ptr(tsk, cpumask_of(smp_processor_id()));        rcu_read_unlock();        numa_default_policy();        pid = kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES);        rcu_read_lock();        kthreadd_task = find_task_by_pid_ns(pid, &amp;init_pid_ns);        rcu_read_unlock();        /*         * Enable might_sleep() and smp_processor_id() checks.         * They cannot be enabled earlier because with CONFIG_PREEMPTION=y         * kernel_thread() would trigger might_sleep() splats. With         * CONFIG_PREEMPT_VOLUNTARY=y the init task might have scheduled         * already, but it's stuck on the kthreadd_done completion.         */        system_state = SYSTEM_SCHEDULING;        complete(&amp;kthreadd_done);        /*         * The boot idle thread must execute schedule()         * at least once to get things moving:         */        schedule_preempt_disabled();        /* Call into cpu_idle with preempt disabled */        cpu_startup_entry(CPUHP_ONLINE);}The most important thing of the rest_init function isspawning kernel_init thread as the first kernel thread. The spawned kernel thread is dedicated to runthe first process as the kernel privilege,which is usally named */init.static int __ref kernel_init(void *unused){        int ret;        kernel_init_freeable();        /* need to finish all async __init code before freeing the memory */        async_synchronize_full();        kprobe_free_init_mem();        ftrace_free_init_mem();        free_initmem();        mark_readonly();        /*         * Kernel mappings are now finalized - update the userspace page-table         * to finalize PTI.         */        pti_finalize();        system_state = SYSTEM_RUNNING;        numa_default_policy();        rcu_end_inkernel_boot();        do_sysctl_args();        if (ramdisk_execute_command) {                ret = run_init_process(ramdisk_execute_command);                if (!ret)                        return 0;                pr_err(\"Failed to execute %s (error %d)\\n\",                       ramdisk_execute_command, ret);        }        /*         * We try each of these until one succeeds.         *         * The Bourne shell can be used instead of init if we are         * trying to recover a really broken machine.         */        if (execute_command) {                ret = run_init_process(execute_command);                if (!ret)                        return 0;                panic(\"Requested init %s failed (error %d).\",                      execute_command, ret);        }        if (CONFIG_DEFAULT_INIT[0] != '\\0') {                ret = run_init_process(CONFIG_DEFAULT_INIT);                if (ret)                        pr_err(\"Default init %s failed (error %d)\\n\",                               CONFIG_DEFAULT_INIT, ret);                else                        return 0;        }        if (!try_to_run_init_process(\"/sbin/init\") ||            !try_to_run_init_process(\"/etc/init\") ||            !try_to_run_init_process(\"/bin/init\") ||            !try_to_run_init_process(\"/bin/sh\"))                return 0;        panic(\"No working init found.  Try passing init= option to kernel. \"              \"See Linux Documentation/admin-guide/init.rst for guidance.\");}Although the most important goal of kernel_init thread is executing the init process,but we will see only the parts that allow us to handle registration of the devices specified in the device tree. Before the init_thread actuall invokes the init process,it calls kernel_init_freeable functionwhich actually handles the device registration.static noinline void __init kernel_init_freeable(void){        /*         * Wait until kthreadd is all set-up.         */        wait_for_completion(&amp;kthreadd_done);        /* Now the scheduler is fully set up and can do blocking allocations */        gfp_allowed_mask = __GFP_BITS_MASK;        /*         * init can allocate pages on any node         */        set_mems_allowed(node_states[N_MEMORY]);        cad_pid = task_pid(current);        smp_prepare_cpus(setup_max_cpus);        workqueue_init();        init_mm_internals();        rcu_init_tasks_generic();        do_pre_smp_initcalls();        lockup_detector_init();        smp_init();        sched_init_smp();        padata_init();        page_alloc_init_late();        /* Initialize page ext after all struct pages are initialized. */        page_ext_init();        do_basic_setup();        kunit_run_all_tests();        console_on_rootfs();        /*         * check if there is an early userspace init.  If yes, let it do all         * the work         */        if (init_eaccess(ramdisk_execute_command) != 0) {                ramdisk_execute_command = NULL;                prepare_namespace();        }        /*         * Ok, we have completed the initial bootup, and         * we're essentially up and running. Get rid of the         * initmem segments and start the user-mode stuff..         *         * rootfs is available now, try loading the public keys         * and default modules         */        integrity_load_keys();}/* * Ok, the machine is now initialized. None of the devices * have been touched yet, but the CPU subsystem is up and * running, and memory and process management works. * * Now we can finally start doing some real work.. */static void __init do_basic_setup(void){        cpuset_init_smp();        driver_init();        init_irq_proc();        do_ctors();        usermodehelper_enable();        do_initcalls();}Among the multiple initializations done by the do_basic_setup,driver_init and do_initcalls are strongly related to platform_device and device tree parsing.Here device tree parsing means that register the devicesspecified in the device tree to the kernel driver system.Initialization of driver subsystemBefore we assign the devices and bind the driver associated with,we have to initialize the driver subsystem of the kernel./** * driver_init - initialize driver model. * * Call the driver model init functions to initialize their * subsystems. Called early from init/main.c. */void __init driver_init(void){        /* These are the core pieces */        devtmpfs_init();        devices_init();        buses_init();        classes_init();        firmware_init();        hypervisor_init();        /* These are also core pieces, but must come after the         * core core pieces.         */        of_core_init();        platform_bus_init();        cpu_dev_init();        memory_dev_init();        container_dev_init();}int __init devices_init(void){        devices_kset = kset_create_and_add(\"devices\", &amp;device_uevent_ops, NULL);        if (!devices_kset)                return -ENOMEM;        dev_kobj = kobject_create_and_add(\"dev\", NULL);        if (!dev_kobj)                goto dev_kobj_err;        sysfs_dev_block_kobj = kobject_create_and_add(\"block\", dev_kobj);        if (!sysfs_dev_block_kobj)                goto block_kobj_err;        sysfs_dev_char_kobj = kobject_create_and_add(\"char\", dev_kobj);        if (!sysfs_dev_char_kobj)                goto char_kobj_err;        return 0; char_kobj_err:        kobject_put(sysfs_dev_block_kobj); block_kobj_err:        kobject_put(dev_kobj); dev_kobj_err:        kset_unregister(devices_kset);        return -ENOMEM;}int __init buses_init(void){        bus_kset = kset_create_and_add(\"bus\", &amp;bus_uevent_ops, NULL);        if (!bus_kset)                return -ENOMEM;        system_kset = kset_create_and_add(\"system\", NULL, &amp;devices_kset-&gt;kobj);        if (!system_kset)                return -ENOMEM;        return 0;}struct kobject *firmware_kobj;EXPORT_SYMBOL_GPL(firmware_kobj); int __init firmware_init(void){               firmware_kobj = kobject_create_and_add(\"firmware\", NULL);        if (!firmware_kobj)                return -ENOMEM;        return 0;}  Most of the init functions are about initializing driver related structures to manage device, driver, bus, etc which represent resources that can be registered on the driver sub-system. Those resources are managed with the kset and kobjects.For example, devices_init function allocateskset for managing all device resourceregistered to the system. It has root kobject, dev, and block and char devices aremanages ad its children resource. Other init functions are mostly same,allocating kset and kobjects associated with specific resources used in driver sub-system.void __init of_core_init(void){        struct device_node *np;        /* Create the kset, and register existing nodes */        mutex_lock(&amp;of_mutex);        of_kset = kset_create_and_add(\"devicetree\", NULL, firmware_kobj);        if (!of_kset) {                mutex_unlock(&amp;of_mutex);                pr_err(\"failed to register existing nodes\\n\");                return;        }        for_each_of_allnodes(np) {                __of_attach_node_sysfs(np);                if (np-&gt;phandle &amp;&amp; !phandle_cache[of_phandle_cache_hash(np-&gt;phandle)])                        phandle_cache[of_phandle_cache_hash(np-&gt;phandle)] = np;        }        mutex_unlock(&amp;of_mutex);        /* Symlink in /proc as required by userspace ABI */        if (of_root)                proc_symlink(\"device-tree\", NULL, \"/sys/firmware/devicetree/base\");}Similar to other init functions, of_core_init function also generates kset for devicetree.In addition to this,it traverse entire device_node of the device tree and generate hash of the phandle of each device node.struct bus_type platform_bus_type = {        .name           = \"platform\",        .dev_groups     = platform_dev_groups,        .match          = platform_match,        .uevent         = platform_uevent,        .probe          = platform_probe,        .remove         = platform_remove,        .shutdown       = platform_shutdown,        .dma_configure  = platform_dma_configure,        .pm             = &amp;platform_dev_pm_ops,};EXPORT_SYMBOL_GPL(platform_bus_type);int __init platform_bus_init(void){        int error;        early_platform_cleanup();        error = device_register(&amp;platform_bus);        if (error) {                put_device(&amp;platform_bus);                return error;        }        error =  bus_register(&amp;platform_bus_type);        if (error)                device_unregister(&amp;platform_bus);        of_platform_register_reconfig_notifier();        return error;}Now finally we have some more complex routines that XXXRegistering bus devicedrivers/base/core.c/**      * device_register - register a device with the system. * @dev: pointer to the device structure * * This happens in two clean steps - initialize the device * and add it to the system. The two steps can be called * separately, but this is the easiest and most common. * I.e. you should only call the two helpers separately if * have a clearly defined need to use and refcount the device * before it is added to the hierarchy. * * For more information, see the kerneldoc for device_initialize() * and device_add().     *       * NOTE: _Never_ directly free @dev after calling this function, even * if it returned an error! Always use put_device() to give up the * reference initialized in this function instead. */     int device_register(struct device *dev){               device_initialize(dev);        return device_add(dev);}EXPORT_SYMBOL_GPL(device_register);/** * device_initialize - init device structure. * @dev: device. * * This prepares the device for use by other layers by initializing * its fields. * It is the first half of device_register(), if called by * that function, though it can also be called separately, so one * may use @dev's fields. In particular, get_device()/put_device() * may be used for reference counting of @dev after calling this * function. *  * All fields in @dev must be initialized by the caller to 0, except * for those explicitly set to some other value.  The simplest * approach is to use kzalloc() to allocate the structure containing * @dev. * * NOTE: Use put_device() to give up your reference instead of freeing * @dev directly once you have called this function. */     void device_initialize(struct device *dev){        dev-&gt;kobj.kset = devices_kset;        kobject_init(&amp;dev-&gt;kobj, &amp;device_ktype);        INIT_LIST_HEAD(&amp;dev-&gt;dma_pools);        mutex_init(&amp;dev-&gt;mutex);#ifdef CONFIG_PROVE_LOCKING        mutex_init(&amp;dev-&gt;lockdep_mutex);#endif                          lockdep_set_novalidate_class(&amp;dev-&gt;mutex);        spin_lock_init(&amp;dev-&gt;devres_lock);        INIT_LIST_HEAD(&amp;dev-&gt;devres_head);        device_pm_init(dev);        set_dev_node(dev, -1);#ifdef CONFIG_GENERIC_MSI_IRQ        INIT_LIST_HEAD(&amp;dev-&gt;msi_list);#endif        INIT_LIST_HEAD(&amp;dev-&gt;links.consumers);        INIT_LIST_HEAD(&amp;dev-&gt;links.suppliers);        INIT_LIST_HEAD(&amp;dev-&gt;links.defer_sync);        dev-&gt;links.status = DL_DEV_NO_DRIVER;}EXPORT_SYMBOL_GPL(device_initialize);The first part of the device_register function is device_initialize,which initialize the device structure passed to the device_register.The most important part of the device_initialize is assign devices_kset tokset of the kobject of the initialized device. We will see later, but to add the kobject of the device,kobj-&gt;kset must be assigned before calling kobject_add function.Also, to manage the device structure using kobject,its ktypes should be set by the kojbect_init function.The second parameter device_ktype is a kobj_type objectthat contains some callback functions.The most important one is release callback functionthat is going to be invokedwhen the reference count of the device structure (maintained by the kobject) becomes zero.static struct kobj_type device_ktype = {        .release        = device_release,        .sysfs_ops      = &amp;dev_sysfs_ops,        .namespace      = device_namespace,        .get_ownership  = device_get_ownership,};/** * device_release - free device structure. * @kobj: device's kobject. * * This is called once the reference count for the object * reaches 0. We forward the call to the device's release * method, which should handle actually freeing the structure. */static void device_release(struct kobject *kobj){        struct device *dev = kobj_to_dev(kobj);        struct device_private *p = dev-&gt;p;        /*         * Some platform devices are driven without driver attached         * and managed resources may have been acquired.  Make sure         * all resources are released.         *         * Drivers still can add resources into device after device         * is deleted but alive, so release devres here to avoid         * possible memory leak.         */        devres_release_all(dev);        kfree(dev-&gt;dma_range_map);        if (dev-&gt;release)                dev-&gt;release(dev);        else if (dev-&gt;type &amp;&amp; dev-&gt;type-&gt;release)                dev-&gt;type-&gt;release(dev);        else if (dev-&gt;class &amp;&amp; dev-&gt;class-&gt;dev_release)                dev-&gt;class-&gt;dev_release(dev);        else                WARN(1, KERN_ERR \"Device '%s' does not have a release() function, it is broken and must be fixed. See Documentation/core-api/kobject.rst.\\n\",                        dev_name(dev));        kfree(p);}When the device_relase function is invoked,it firstly free the all resources allocated for the device, devres_release_all(dev).After that, by invoking the release function registered in the device structure,it can release the device structure which is not used anymore.Now, let’s take a look at rest of the device_initialize.The passed device argument of the function is platform_bus variable in this case.struct device platform_bus = {        .init_name      = \"platform\",};EXPORT_SYMBOL_GPL(platform_bus);Although this device structure has only single initialized member field,device structure contains bunch of other fields used to abstract physical/virtual device in the driver sub-system./** * struct device - The basic device structure * @parent:     The device's \"parent\" device, the device to which it is attached. *              In most cases, a parent device is some sort of bus or host *              controller. If parent is NULL, the device, is a top-level device, *              which is not usually what you want. * @p:          Holds the private data of the driver core portions of the device. *              See the comment of the struct device_private for detail. * @kobj:       A top-level, abstract class from which other classes are derived. * @init_name:  Initial name of the device. * @type:       The type of device. *              This identifies the device type and carries type-specific *              information. * @mutex:      Mutex to synchronize calls to its driver. * @lockdep_mutex: An optional debug lock that a subsystem can use as a *              peer lock to gain localized lockdep coverage of the device_lock. * @bus:        Type of bus device is on. * @driver:     Which driver has allocated this * @platform_data: Platform data specific to the device. *              Example: For devices on custom boards, as typical of embedded *              and SOC based hardware, Linux often uses platform_data to point *              to board-specific structures describing devices and how they *              are wired.  That can include what ports are available, chip *              variants, which GPIO pins act in what additional roles, and so *              on.  This shrinks the \"Board Support Packages\" (BSPs) and *              minimizes board-specific #ifdefs in drivers. * @driver_data: Private pointer for driver specific info. * @links:      Links to suppliers and consumers of this device. * @power:      For device power management. *              See Documentation/driver-api/pm/devices.rst for details. * @pm_domain:  Provide callbacks that are executed during system suspend, *              hibernation, system resume and during runtime PM transitions *              along with subsystem-level and driver-level callbacks. * @em_pd:      device's energy model performance domain * @pins:       For device pin management. *              See Documentation/driver-api/pinctl.rst for details. * @msi_list:   Hosts MSI descriptors * @msi_domain: The generic MSI domain this device is using. * @numa_node:  NUMA node this device is close to. * @dma_ops:    DMA mapping operations for this device. * @dma_mask:   Dma mask (if dma'ble device). * @coherent_dma_mask: Like dma_mask, but for alloc_coherent mapping as not all *              hardware supports 64-bit addresses for consistent allocations *              such descriptors. * @bus_dma_limit: Limit of an upstream bridge or bus which imposes a smaller *              DMA limit than the device itself supports. * @dma_range_map: map for DMA memory ranges relative to that of RAM * @dma_parms:  A low level driver may set these to teach IOMMU code about *              segment limitations. * @dma_pools:  Dma pools (if dma'ble device). * @dma_mem:    Internal for coherent mem override. * @cma_area:   Contiguous memory area for dma allocations * @archdata:   For arch-specific additions. * @of_node:    Associated device tree node. * @fwnode:     Associated device node supplied by platform firmware. * @devt:       For creating the sysfs \"dev\". * @id:         device instance * @devres_lock: Spinlock to protect the resource of the device. * @devres_head: The resources list of the device. * @knode_class: The node used to add the device to the class list. * @class:      The class of the device. * @groups:     Optional attribute groups. * @release:    Callback to free the device after all references have *              gone away. This should be set by the allocator of the *              device (i.e. the bus driver that discovered the device). * @iommu_group: IOMMU group the device belongs to. * @iommu:      Per device generic IOMMU runtime data * * @offline_disabled: If set, the device is permanently online. * @offline:    Set after successful invocation of bus type's .offline(). * @of_node_reused: Set if the device-tree node is shared with an ancestor *              device. * @state_synced: The hardware state of this device has been synced to match *                the software state of this device by calling the driver/bus *                sync_state() callback. * @dma_coherent: this particular device is dma coherent, even if the *              architecture supports non-coherent devices. * @dma_ops_bypass: If set to %true then the dma_ops are bypassed for the *              streaming DMA operations (-&gt;map_* / -&gt;unmap_* / -&gt;sync_*), *              and optionall (if the coherent mask is large enough) also *              for dma allocations.  This flag is managed by the dma ops *              instance from -&gt;dma_supported. * * At the lowest level, every device in a Linux system is represented by an * instance of struct device. The device structure contains the information * that the device model core needs to model the system. Most subsystems, * however, track additional information about the devices they host. As a * result, it is rare for devices to be represented by bare device structures; * instead, that structure, like kobject structures, is usually embedded within * a higher-level representation of the device. */struct device {        struct kobject kobj;        struct device           *parent;        struct device_private   *p;        const char              *init_name; /* initial name of the device */        const struct device_type *type;        struct bus_type *bus;           /* type of bus device is on */        struct device_driver *driver;   /* which driver has allocated this                                           device */        void            *platform_data; /* Platform specific data, device                                           core doesn't touch it */        void            *driver_data;   /* Driver data, set and get with                                           dev_set_drvdata/dev_get_drvdata */#ifdef CONFIG_PROVE_LOCKING        struct mutex            lockdep_mutex;#endif        struct mutex            mutex;  /* mutex to synchronize calls to                                         * its driver.                                         */        struct dev_links_info   links;        struct dev_pm_info      power;        struct dev_pm_domain    *pm_domain#ifdef CONFIG_ENERGY_MODEL        struct em_perf_domain   *em_pd;#endif#ifdef CONFIG_GENERIC_MSI_IRQ_DOMAIN        struct irq_domain       *msi_domain;#endif#ifdef CONFIG_PINCTRL        struct dev_pin_info     *pins;#endif#ifdef CONFIG_GENERIC_MSI_IRQ        struct list_head        msi_list;#endif#ifdef CONFIG_DMA_OPS        const struct dma_map_ops *dma_ops;#endif        u64             *dma_mask;      /* dma mask (if dma'able device) */        u64             coherent_dma_mask;/* Like dma_mask, but for                                             alloc_coherent mappings as                                             not all hardware supports                                             64 bit addresses for consistent                                             allocations such descriptors. */        u64             bus_dma_limit;  /* upstream dma constraint */        const struct bus_dma_region *dma_range_map;        struct device_dma_parameters *dma_parms;        struct list_head        dma_pools;      /* dma pools (if dma'ble) */#ifdef CONFIG_DMA_DECLARE_COHERENT        struct dma_coherent_mem *dma_mem; /* internal for coherent mem                                             override */#endif#ifdef CONFIG_DMA_CMA        struct cma *cma_area;           /* contiguous memory area for dma                                           allocations */#endif        /* arch specific additions */        struct dev_archdata     archdata;        struct device_node      *of_node; /* associated device tree node */        struct fwnode_handle    *fwnode; /* firmware device node */#ifdef CONFIG_NUMA        int             numa_node;      /* NUMA node this device is close to */#endif        dev_t                   devt;   /* dev_t, creates the sysfs \"dev\" */        u32                     id;     /* device instance */        spinlock_t              devres_lock;        struct list_head        devres_head;        struct class            *class;        const struct attribute_group **groups;  /* optional groups */        void    (*release)(struct device *dev);        struct iommu_group      *iommu_group;        struct dev_iommu        *iommu;        bool                    offline_disabled:1;        bool                    offline:1;        bool                    of_node_reused:1;        bool                    state_synced:1;#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \\    defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \\    defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)        bool                    dma_coherent:1;#endif#ifdef CONFIG_DMA_OPS_BYPASS        bool                    dma_ops_bypass : 1;#endif};Thedevice_intialize function initializes device links that manage consumer and suppliers of this device,devres list which manages all resources used by the device,dma, mutex, etc. Lastly, it sets current device status as DL_DEV_NO_DRIVER which means the device has no driver attached to it./**      * enum dl_dev_state - Device driver presence tracking information. * @DL_DEV_NO_DRIVER: There is no driver attached to the device. * @DL_DEV_PROBING: A driver is probing. * @DL_DEV_DRIVER_BOUND: The driver has been bound to the device. * @DL_DEV_UNBINDING: The driver is unbinding from the device. */     enum dl_dev_state {        DL_DEV_NO_DRIVER = 0,        DL_DEV_PROBING,        DL_DEV_DRIVER_BOUND,        DL_DEV_UNBINDING,};Add platform_bus device to the driver subsystemAfter initializing kobject and some fields of the platform_bus device,device_add function will register the deviceto the driver sub-system./** * device_add - add device to device hierarchy. * @dev: device. * * This is part 2 of device_register(), though may be called * separately _iff_ device_initialize() has been called separately. * * This adds @dev to the kobject hierarchy via kobject_add(), adds it * to the global and sibling lists for the device, then * adds it to the other relevant subsystems of the driver model. * * Do not call this routine or device_register() more than once for * any device structure.  The driver model core is not designed to work * with devices that get unregistered and then spring back to life. * (Among other things, it's very hard to guarantee that all references * to the previous incarnation of @dev have been dropped.)  Allocate * and register a fresh new struct device instead. * * NOTE: _Never_ directly free @dev after calling this function, even * if it returned an error! Always use put_device() to give up your * reference instead. * * Rule of thumb is: if device_add() succeeds, you should call * device_del() when you want to get rid of it. If device_add() has * *not* succeeded, use *only* put_device() to drop the reference * count. */int device_add(struct device *dev){        struct device *parent;        struct kobject *kobj;        struct class_interface *class_intf;        int error = -EINVAL;        struct kobject *glue_dir = NULL;        dev = get_device(dev);        if (!dev)                goto done;        if (!dev-&gt;p) {                error = device_private_init(dev);                if (error)                        goto done;        }        /*         * for statically allocated devices, which should all be converted         * some day, we need to initialize the name. We prevent reading back         * the name, and force the use of dev_name()         */        if (dev-&gt;init_name) {                dev_set_name(dev, \"%s\", dev-&gt;init_name);                dev-&gt;init_name = NULL;        }        /* subsystems can specify simple device enumeration */        if (!dev_name(dev) &amp;&amp; dev-&gt;bus &amp;&amp; dev-&gt;bus-&gt;dev_name)                dev_set_name(dev, \"%s%u\", dev-&gt;bus-&gt;dev_name, dev-&gt;id);        if (!dev_name(dev)) {                error = -EINVAL;                goto name_error;        }        pr_debug(\"device: '%s': %s\\n\", dev_name(dev), __func__);        parent = get_device(dev-&gt;parent);        kobj = get_device_parent(dev, parent);        if (IS_ERR(kobj)) {                error = PTR_ERR(kobj);                goto parent_error;        }        if (kobj)                dev-&gt;kobj.parent = kobj;        /* use parent numa_node */        if (parent &amp;&amp; (dev_to_node(dev) == NUMA_NO_NODE))                set_dev_node(dev, dev_to_node(parent));        /* first, register with generic layer. */        /* we require the name to be set before, and pass NULL */        error = kobject_add(&amp;dev-&gt;kobj, dev-&gt;kobj.parent, NULL);        if (error) {                glue_dir = get_glue_dir(dev);                goto Error;        }        /* notify platform of device entry */        error = device_platform_notify(dev, KOBJ_ADD);        if (error)                goto platform_error;        error = device_create_file(dev, &amp;dev_attr_uevent);        if (error)                goto attrError;        error = device_add_class_symlinks(dev);        if (error)                goto SymlinkError;        error = device_add_attrs(dev);        if (error)                goto AttrsError;        error = bus_add_device(dev);        if (error)                goto BusError;        error = dpm_sysfs_add(dev);        if (error)                goto DPMError;        device_pm_add(dev);        if (MAJOR(dev-&gt;devt)) {                error = device_create_file(dev, &amp;dev_attr_dev);                if (error)                        goto DevAttrError;                error = device_create_sys_dev_entry(dev);                if (error)                        goto SysEntryError;                devtmpfs_create_node(dev);        }        /* Notify clients of device addition.  This call must come         * after dpm_sysfs_add() and before kobject_uevent().         */        if (dev-&gt;bus)                blocking_notifier_call_chain(&amp;dev-&gt;bus-&gt;p-&gt;bus_notifier,                                             BUS_NOTIFY_ADD_DEVICE, dev);        kobject_uevent(&amp;dev-&gt;kobj, KOBJ_ADD);        /*         * Check if any of the other devices (consumers) have been waiting for         * this device (supplier) to be added so that they can create a device         * link to it.         *         * This needs to happen after device_pm_add() because device_link_add()         * requires the supplier be registered before it's called.         *         * But this also needs to happen before bus_probe_device() to make sure         * waiting consumers can link to it before the driver is bound to the         * device and the driver sync_state callback is called for this device.         */        if (dev-&gt;fwnode &amp;&amp; !dev-&gt;fwnode-&gt;dev) {                dev-&gt;fwnode-&gt;dev = dev;                fw_devlink_link_device(dev);        }        bus_probe_device(dev);        if (parent)                klist_add_tail(&amp;dev-&gt;p-&gt;knode_parent,                               &amp;parent-&gt;p-&gt;klist_children);        if (dev-&gt;class) {                mutex_lock(&amp;dev-&gt;class-&gt;p-&gt;mutex);                /* tie the class to the device */                klist_add_tail(&amp;dev-&gt;p-&gt;knode_class,                               &amp;dev-&gt;class-&gt;p-&gt;klist_devices);                /* notify any interfaces that the device is here */                list_for_each_entry(class_intf,                                    &amp;dev-&gt;class-&gt;p-&gt;interfaces, node)                        if (class_intf-&gt;add_dev)                                class_intf-&gt;add_dev(dev, class_intf);                mutex_unlock(&amp;dev-&gt;class-&gt;p-&gt;mutex);        }done:        put_device(dev);        return error; SysEntryError:        if (MAJOR(dev-&gt;devt))                device_remove_file(dev, &amp;dev_attr_dev); DevAttrError:        device_pm_remove(dev);        dpm_sysfs_remove(dev); DPMError:        bus_remove_device(dev); BusError:        device_remove_attrs(dev); AttrsError:        device_remove_class_symlinks(dev); SymlinkError:        device_remove_file(dev, &amp;dev_attr_uevent); attrError:        device_platform_notify(dev, KOBJ_REMOVE);platform_error:        kobject_uevent(&amp;dev-&gt;kobj, KOBJ_REMOVE);        glue_dir = get_glue_dir(dev);        kobject_del(&amp;dev-&gt;kobj); Error:        cleanup_glue_dir(dev, glue_dir);parent_error:        put_device(parent);name_error:        kfree(dev-&gt;p);        dev-&gt;p = NULL;        goto done;}EXPORT_SYMBOL_GPL(device_add);We can easily find that weird behavior of the device_add,which invokes get_device(dev) function to retrieve the reference to the device even though we have an access to it.struct device *get_device(struct device *dev){        return dev ? kobj_to_dev(kobject_get(&amp;dev-&gt;kobj)) : NULL;}EXPORT_SYMBOL_GPL(get_device);struct kobject *kobject_get(struct kobject *kobj){        if (kobj) {                if (!kobj-&gt;state_initialized)                        WARN(1, KERN_WARNING                                \"kobject: '%s' (%p): is not initialized, yet kobject_get() is being called.\\n\",                             kobject_name(kobj), kobj);                kref_get(&amp;kobj-&gt;kref);        }        return kobj; }EXPORT_SYMBOL(kobject_get);static inline struct device *kobj_to_dev(struct kobject *kobj){        return container_of(kobj, struct device, kobj);}This is because we now have an initialized kobject embedded in the device.When the get_device function is invoked,it just returns the device reference,but it increases the reference counter of the embedded kobject of the device.The kobject_get function increases the reference counter,and return the kobject reference of the device.The kobj_to_dev function just utilize the container_of macroto retrieve the device structureassociated with current kobject.Because platform_bus device is statically allocatedand has no parent device,its parent field is NULL and we can just ignore all functions related with retrieving its parent device object. After that, it invokes kobject_add functionwhich register the platform_bus device to its kset, devices_kset./** * kobject_add() - The main kobject add function. * @kobj: the kobject to add * @parent: pointer to the parent of the kobject. * @fmt: format to name the kobject with. * * The kobject name is set and added to the kobject hierarchy in this * function. * * If @parent is set, then the parent of the @kobj will be set to it. * If @parent is NULL, then the parent of the @kobj will be set to the * kobject associated with the kset assigned to this kobject.  If no kset * is assigned to the kobject, then the kobject will be located in the * root of the sysfs tree. * * Note, no \"add\" uevent will be created with this call, the caller should set * up all of the necessary sysfs files for the object and then call * kobject_uevent() with the UEVENT_ADD parameter to ensure that * userspace is properly notified of this kobject's creation. *       * Return: If this function returns an error, kobject_put() must be *         called to properly clean up the memory associated with the *         object.  Under no instance should the kobject that is passed *         to this function be directly freed with a call to kfree(), *         that can leak memory. * *         If this function returns success, kobject_put() must also be called *         in order to properly clean up the memory associated with the object. *               *         In short, once this function is called, kobject_put() MUST be called *         when the use of the object is finished in order to properly free *         everything. */     int kobject_add(struct kobject *kobj, struct kobject *parent,                 const char *fmt, ...){                va_list args;        int retval;                        if (!kobj)                return -EINVAL;        if (!kobj-&gt;state_initialized) {                pr_err(\"kobject '%s' (%p): tried to add an uninitialized object, something is seriously wrong.\\n\",                       kobject_name(kobj), kobj);                dump_stack();                return -EINVAL;        }               va_start(args, fmt);        retval = kobject_add_varg(kobj, parent, fmt, args);        va_end(args);                return retval;}       EXPORT_SYMBOL(kobject_add);Here, note that its parent and fmt argument is set as NULLbecause platform_bus device has no parent device,and kobj points to the platform_bus device itself. First conditional statement confirms whether the device structure has been initialized by checking state_initialized field of kobject of platform_bus device.This field has been initialized at the first part of device_register function,device_initialize.static int kobject_add_internal(struct kobject *kobj){        int error = 0;        struct kobject *parent;        if (!kobj)                return -ENOENT;        if (!kobj-&gt;name || !kobj-&gt;name[0]) {                WARN(1,                     \"kobject: (%p): attempted to be registered with empty name!\\n\",                     kobj);                return -EINVAL;        }        parent = kobject_get(kobj-&gt;parent);        /* join kset if set, use it as parent if we do not already have one */        if (kobj-&gt;kset) {                if (!parent)                        parent = kobject_get(&amp;kobj-&gt;kset-&gt;kobj);                kobj_kset_join(kobj);                kobj-&gt;parent = parent;        }        pr_debug(\"kobject: '%s' (%p): %s: parent: '%s', set: '%s'\\n\",                 kobject_name(kobj), kobj, __func__,                 parent ? kobject_name(parent) : \"&lt;NULL&gt;\",                 kobj-&gt;kset ? kobject_name(&amp;kobj-&gt;kset-&gt;kobj) : \"&lt;NULL&gt;\");        error = create_dir(kobj);        if (error) {                kobj_kset_leave(kobj);                kobject_put(parent);                kobj-&gt;parent = NULL;                /* be noisy on error issues */                if (error == -EEXIST)                        pr_err(\"%s failed for %s with -EEXIST, don't try to register things with the same name in the same directory.\\n\",                               __func__, kobject_name(kobj));                else                        pr_err(\"%s failed for %s (error: %d parent: %s)\\n\",                               __func__, kobject_name(kobj), error,                               parent ? kobject_name(parent) : \"'none'\");        } else                kobj-&gt;state_in_sysfs = 1;        return error;}It register the kobject of the platform_bus to its ksetusing kobj_kset_join function.Also because this device doesn’t have parent,its kobject deosn’t have parent neither. Therefore, it allocates the kset’s kobject as its parent kobject.Although the current device has no attached busbecause it is the bus device itself,if it were a end-device, it needs to be attached to the proper busby bus_add_device function./** * bus_add_device - add device to bus * @dev: device being added *               * - Add device's bus attributes. * - Create links to device's bus. * - Add the device to its bus's list of devices. */     int bus_add_device(struct device *dev){                       struct bus_type *bus = bus_get(dev-&gt;bus);        int error = 0;                        if (bus) {                pr_debug(\"bus: '%s': add device %s\\n\", bus-&gt;name, dev_name(dev));                error = device_add_groups(dev, bus-&gt;dev_groups);                if (error)                        goto out_put;                error = sysfs_create_link(&amp;bus-&gt;p-&gt;devices_kset-&gt;kobj,                                                &amp;dev-&gt;kobj, dev_name(dev));                if (error)                        goto out_groups;                error = sysfs_create_link(&amp;dev-&gt;kobj,                                &amp;dev-&gt;bus-&gt;p-&gt;subsys.kobj, \"subsystem\");                if (error)                        goto out_subsys;                klist_add_tail(&amp;dev-&gt;p-&gt;knode_bus, &amp;bus-&gt;p-&gt;klist_devices);        }        return 0;         out_subsys:        sysfs_remove_link(&amp;bus-&gt;p-&gt;devices_kset-&gt;kobj, dev_name(dev));out_groups:                                          device_remove_groups(dev, bus-&gt;dev_groups);out_put:        bus_put(dev-&gt;bus);        return error;}  The most important part of this function is registering the device’s knode for bus, knode_busto the bus’ klist_device klist.This registration is done by klist_add_tail function. After device attachment is done,the device driver that can handle the registered device should be bound to the device./**              * bus_probe_device - probe drivers for a new device * @dev: device to probe * * - Automatically probe for a driver if the bus allows it. */     void bus_probe_device(struct device *dev){                                      struct bus_type *bus = dev-&gt;bus;        struct subsys_interface *sif;                        if (!bus)                return;                                       if (bus-&gt;p-&gt;drivers_autoprobe)                device_initial_probe(dev);                        mutex_lock(&amp;bus-&gt;p-&gt;mutex);         list_for_each_entry(sif, &amp;bus-&gt;p-&gt;interfaces, node)                if (sif-&gt;add_dev)                        sif-&gt;add_dev(dev, sif);        mutex_unlock(&amp;bus-&gt;p-&gt;mutex);}If the current device is attached to the bus and autoprobe of the bus has been enabled,it invokes device_initial_probe that will actually searches the device driver and binding.The details will be coverd in this post.Bus registrationWhat we have done for platform_bus so far is registering platform_bus as device to the driver sub-system.In other words, because even the bus is treated and managed by the driver sub-system in the Linux kernel,we have to firstly create the device for the busand register it to the system.However, because the bus device is used to manage other devices attached to itit should have its private sub-system for managing the attached devices.int __init platform_bus_init(void){        int error;        early_platform_cleanup();        error = device_register(&amp;platform_bus);        if (error) {                put_device(&amp;platform_bus);                return error;        }        error =  bus_register(&amp;platform_bus_type);        if (error)                device_unregister(&amp;platform_bus);        of_platform_register_reconfig_notifier();        return error;}When we revisit the platform_bus_init function,we can find that bus_register function is called with platform_bus_type which is an objectof bus_type.Let’s take what is the purpose of bus_typeand implementation of platform_bus_type./** * struct bus_type - The bus type of the device * * @name:       The name of the bus. * @dev_name:   Used for subsystems to enumerate devices like (\"foo%u\", dev-&gt;id). * @dev_root:   Default device to use as the parent. * @bus_groups: Default attributes of the bus. * @dev_groups: Default attributes of the devices on the bus. * @drv_groups: Default attributes of the device drivers on the bus. * @match:      Called, perhaps multiple times, whenever a new device or driver *              is added for this bus. It should return a positive value if the *              given device can be handled by the given driver and zero *              otherwise. It may also return error code if determining that *              the driver supports the device is not possible. In case of *              -EPROBE_DEFER it will queue the device for deferred probing. * @uevent:     Called when a device is added, removed, or a few other things *              that generate uevents to add the environment variables. * @probe:      Called when a new device or driver add to this bus, and callback *              the specific driver's probe to initial the matched device. * @sync_state: Called to sync device state to software state after all the *              state tracking consumers linked to this device (present at *              the time of late_initcall) have successfully bound to a *              driver. If the device has no consumers, this function will *              be called at late_initcall_sync level. If the device has *              consumers that are never bound to a driver, this function *              will never get called until they do. * @remove:     Called when a device removed from this bus. * @shutdown:   Called at shut-down time to quiesce the device. * * @online:     Called to put the device back online (after offlining it). * @offline:    Called to put the device offline for hot-removal. May fail. * * @suspend:    Called when a device on this bus wants to go to sleep mode. * @resume:     Called to bring a device on this bus out of sleep mode. * @num_vf:     Called to find out how many virtual functions a device on this *              bus supports. * @dma_configure:      Called to setup DMA configuration on a device on *                      this bus. * @pm:         Power management operations of this bus, callback the specific *              device driver's pm-ops. * @iommu_ops:  IOMMU specific operations for this bus, used to attach IOMMU *              driver implementations to a bus and allow the driver to do *              bus-specific setup * @p:          The private data of the driver core, only the driver core can *              touch this. * @lock_key:   Lock class key for use by the lock validator * @need_parent_lock:   When probing or removing a device on this bus, the *                      device core should lock the device's parent. * * A bus is a channel between the processor and one or more devices. For the * purposes of the device model, all devices are connected via a bus, even if * it is an internal, virtual, \"platform\" bus. Buses can plug into each other. * A USB controller is usually a PCI device, for example. The device model * represents the actual connections between buses and the devices they control. * A bus is represented by the bus_type structure. It contains the name, the * default attributes, the bus' methods, PM operations, and the driver core's * private data. */struct bus_type {        const char              *name;        const char              *dev_name;        struct device           *dev_root;        const struct attribute_group **bus_groups;        const struct attribute_group **dev_groups;        const struct attribute_group **drv_groups;        int (*match)(struct device *dev, struct device_driver *drv);        int (*uevent)(struct device *dev, struct kobj_uevent_env *env);        int (*probe)(struct device *dev);        void (*sync_state)(struct device *dev);        int (*remove)(struct device *dev);        void (*shutdown)(struct device *dev);        int (*online)(struct device *dev);        int (*offline)(struct device *dev);        int (*suspend)(struct device *dev, pm_message_t state);        int (*resume)(struct device *dev);        int (*num_vf)(struct device *dev);        int (*dma_configure)(struct device *dev);        const struct dev_pm_ops *pm;        const struct iommu_ops *iommu_ops;        struct subsys_private *p;        struct lock_class_key lock_key;        bool need_parent_lock;};struct bus_type platform_bus_type = {        .name           = \"platform\",        .dev_groups     = platform_dev_groups,        .match          = platform_match,        .uevent         = platform_uevent,        .probe          = platform_probe,        .remove         = platform_remove,        .shutdown       = platform_shutdown,        .dma_configure  = platform_dma_configure,        .pm             = &amp;platform_dev_pm_ops,};The bus_type structure containssome information about the bus itself and bunch of call-back functions required to manage bus and its sub-devicesattached to the bus.Also, it maintains subsys_private pointerthat actually used to manage the busand its associated devices and drivers.Let’s take a look athow the bus_register function actually register a new bus sub-system./** * bus_register - register a driver-core subsystem * @bus: bus to register * * Once we have that, we register the bus with the kobject * infrastructure, then register the children subsystems it has: * the devices and drivers that belong to the subsystem. */int bus_register(struct bus_type *bus){        int retval;        struct subsys_private *priv;        struct lock_class_key *key = &amp;bus-&gt;lock_key;        priv = kzalloc(sizeof(struct subsys_private), GFP_KERNEL);        if (!priv)                return -ENOMEM;        priv-&gt;bus = bus;        bus-&gt;p = priv;        BLOCKING_INIT_NOTIFIER_HEAD(&amp;priv-&gt;bus_notifier);        retval = kobject_set_name(&amp;priv-&gt;subsys.kobj, \"%s\", bus-&gt;name);        if (retval)                goto out;        priv-&gt;subsys.kobj.kset = bus_kset;        priv-&gt;subsys.kobj.ktype = &amp;bus_ktype;        priv-&gt;drivers_autoprobe = 1;        retval = kset_register(&amp;priv-&gt;subsys);        if (retval)                goto out;        retval = bus_create_file(bus, &amp;bus_attr_uevent);        if (retval)                goto bus_uevent_fail;        priv-&gt;devices_kset = kset_create_and_add(\"devices\", NULL,                                                 &amp;priv-&gt;subsys.kobj);        if (!priv-&gt;devices_kset) {                retval = -ENOMEM;                goto bus_devices_fail;        }        priv-&gt;drivers_kset = kset_create_and_add(\"drivers\", NULL,                                                 &amp;priv-&gt;subsys.kobj);        if (!priv-&gt;drivers_kset) {                retval = -ENOMEM;                goto bus_drivers_fail;        }        INIT_LIST_HEAD(&amp;priv-&gt;interfaces);        __mutex_init(&amp;priv-&gt;mutex, \"subsys mutex\", key);        klist_init(&amp;priv-&gt;klist_devices, klist_devices_get, klist_devices_put);        klist_init(&amp;priv-&gt;klist_drivers, NULL, NULL);        retval = add_probe_files(bus);        if (retval)                goto bus_probe_files_fail;        retval = bus_add_groups(bus, bus-&gt;bus_groups);        if (retval)                goto bus_groups_fail;        pr_debug(\"bus: '%s': registered\\n\", bus-&gt;name);        return 0;bus_groups_fail:        remove_probe_files(bus);bus_probe_files_fail:        kset_unregister(bus-&gt;p-&gt;drivers_kset);bus_drivers_fail:        kset_unregister(bus-&gt;p-&gt;devices_kset);bus_devices_fail:        bus_remove_file(bus, &amp;bus_attr_uevent);bus_uevent_fail:        kset_unregister(&amp;bus-&gt;p-&gt;subsys);out:        kfree(bus-&gt;p);        bus-&gt;p = NULL;        return retval;}EXPORT_SYMBOL_GPL(bus_register);/** * struct subsys_private - structure to hold the private to the driver core portions of the bus_type/class structure. * * @subsys - the struct kset that defines this subsystem * @devices_kset - the subsystem's 'devices' directory * @interfaces - list of subsystem interfaces associated * @mutex - protect the devices, and interfaces lists. * * @drivers_kset - the list of drivers associated * @klist_devices - the klist to iterate over the @devices_kset * @klist_drivers - the klist to iterate over the @drivers_kset * @bus_notifier - the bus notifier list for anything that cares about things *                 on this bus. * @bus - pointer back to the struct bus_type that this structure is associated *        with. * * @glue_dirs - \"glue\" directory to put in-between the parent device to *              avoid namespace conflicts * @class - pointer back to the struct class that this structure is associated *          with. * * This structure is the one that is the actual kobject allowing struct * bus_type/class to be statically allocated safely.  Nothing outside of the * driver core should ever touch these fields. */struct subsys_private {        struct kset subsys;        struct kset *devices_kset;        struct list_head interfaces;        struct mutex mutex;        struct kset *drivers_kset;        struct klist klist_devices;        struct klist klist_drivers;        struct blocking_notifier_head bus_notifier;        unsigned int drivers_autoprobe:1;        struct bus_type *bus;        struct kset glue_dirs;        struct class *class;};The most important role of the bus_register function is generating a subsystem for the bus device,which is used to manage the devices and their drivers sitting on the bus. Therefore,to understand how the registered bus managesdevices attached to the bus and its associated drivers,we have to understand the data structure subsys_privatewhich contains most important data fieldsregarding device and driver management.The first thing done by the bus_register function isallocating the subsys_private object and initialization.The function chains the bus_type structure and subsys_private structureby assigning the pointers to each otheras their memeber fields respectively.Therefore, when you have either bus_type or subsys_private object,you can reference the other also.It assigns the generated subsys_private object tobus object memeber field, p.Because subsystems are also managed by as kset and kobjects system,the kobject should be properly registered to a kset.You might remeber that buses_init function genereates kset named bus_kset.Note that generated subsystem’s kobject’s kset field points to the bus_kset,which means the generated subsystem will be managed by bus_kset.kset_register function actually handles registration process.Next important thing is intializing klist for managing devices and its associated driversthat could sit on the current bus,klist_devices and klist_drivers.Note that this klist_devices kset is dedicated for the current bus subsystem.int __init devices_init(void){        devices_kset = kset_create_and_add(\"devices\", &amp;device_uevent_ops, NULL);        ...}Although previous devices_init function also genereated the kset named “devices”and assign this to devices_kset,priv-&gt;devices_kset which is dedicated to a one bus sub-system is totally different.The devices_kset of the bus subsystem only manages devices attached to the bus,but the devices_kset is orchestratesdevices in the whole Linux driver system.Add notifier block to device file notifier chainBecause device file heavily make use of platform device,it needs to register notifer of platform device,platform_of_notifer,to the devie file’s notifier chain.Before we take a look at which platform related function of the notifier block will be added to the device file’s notifier chain,let’s take a look at the API functions that help us to register platform notifier block.void of_platform_register_reconfig_notifier(void){        WARN_ON(of_reconfig_notifier_register(&amp;platform_of_notifier));}   static BLOCKING_NOTIFIER_HEAD(of_reconfig_chain);int of_reconfig_notifier_register(struct notifier_block *nb){        return blocking_notifier_chain_register(&amp;of_reconfig_chain, nb);}       /*  *      Blocking notifier chain routines.  All access to the chain is *      synchronized by an rwsem. */     /** *      blocking_notifier_chain_register - Add notifier to a blocking notifier chain *      @nh: Pointer to head of the blocking notifier chain *      @n: New entry in notifier chain * *      Adds a notifier to a blocking notifier chain. *      Must be called in process context. * *      Currently always returns zero. */int blocking_notifier_chain_register(struct blocking_notifier_head *nh,                struct notifier_block *n){                       int ret;        /*         * This code gets used during boot-up, when task switching is         * not yet working and interrupts must remain disabled.  At         * such times we must not call down_write().         */        if (unlikely(system_state == SYSTEM_BOOTING))                return notifier_chain_register(&amp;nh-&gt;head, n);        down_write(&amp;nh-&gt;rwsem);         ret = notifier_chain_register(&amp;nh-&gt;head, n);        up_write(&amp;nh-&gt;rwsem);        return ret;}EXPORT_SYMBOL_GPL(blocking_notifier_chain_register);of_reconfig_chain is a list that manages all registered notifier blocks of the device file.The blocking_notifier_chain_register helps usregister the notifier block to the list.Let’s take what function is included in the platform notifier block.static struct notifier_block platform_of_notifier = {        .notifier_call = of_platform_notify,};  static int of_platform_notify(struct notifier_block *nb,                                unsigned long action, void *arg){        struct of_reconfig_data *rd = arg;         struct platform_device *pdev_parent, *pdev;        bool children_left;         switch (of_reconfig_get_state_change(action, rd)) {        case OF_RECONFIG_CHANGE_ADD:                /* verify that the parent is a bus */                if (!of_node_check_flag(rd-&gt;dn-&gt;parent, OF_POPULATED_BUS))                        return NOTIFY_OK;       /* not for us */                /* already populated? (driver using of_populate manually) */                if (of_node_check_flag(rd-&gt;dn, OF_POPULATED))                        return NOTIFY_OK;                /* pdev_parent may be NULL when no bus platform device */                pdev_parent = of_find_device_by_node(rd-&gt;dn-&gt;parent);                pdev = of_platform_device_create(rd-&gt;dn, NULL,                                pdev_parent ? &amp;pdev_parent-&gt;dev : NULL);                of_dev_put(pdev_parent);                if (pdev == NULL) {                        pr_err(\"%s: failed to create for '%pOF'\\n\",                                        __func__, rd-&gt;dn);                        /* of_platform_device_create tosses the error code */                        return notifier_from_errno(-EINVAL);                }                break;                case OF_RECONFIG_CHANGE_REMOVE:                        /* already depopulated? */                if (!of_node_check_flag(rd-&gt;dn, OF_POPULATED))                        return NOTIFY_OK;                /* find our device by node */                pdev = of_find_device_by_node(rd-&gt;dn);                if (pdev == NULL)                        return NOTIFY_OK;       /* no? not meant for us */                /* unregister takes one ref away */                of_platform_device_destroy(&amp;pdev-&gt;dev, &amp;children_left);                        /* and put the reference of the find */                of_dev_put(pdev);                break;        }                                       return NOTIFY_OK;}       Detect and add platform devicesSo far we explored how the platform bus has been initialized as sub-system of the entire driver system of the Linux kernel.Then how can we detect the devices that needs to be registered as platform deviceinstead of other usb or pci devices which can be automatically detectedby the bus supporting hot-plugging?Although there are several other parts of the kernelregister the devices as platform device,the biggest chance to discover and register most of the platform devices exists in the device tree. Therefore, let’s take a look at the initialization functionof the device tree.static int __init of_platform_default_populate_init(void){               struct device_node *node;        device_links_supplier_sync_state_pause();        if (!of_have_populated_dt())                return -ENODEV;         /*         * Handle certain compatibles explicitly, since we don't want to create         * platform_devices for every node in /reserved-memory with a         * \"compatible\",         */                              for_each_matching_node(node, reserved_mem_matches)                of_platform_device_create(node, NULL, NULL);         node = of_find_node_by_path(\"/firmware\");        if (node) {                 of_platform_populate(node, NULL, NULL, NULL);                of_node_put(node);        }                /* Populate everything else. */         of_platform_default_populate(NULL, NULL, NULL);        return 0;}arch_initcall_sync(of_platform_default_populate_init);We can find that of_platform_default_populate_init function is set as initcall function that will be executed during do_initcall function.The detailed information of the do_initcalland its related macros are going to explained in other posts.int of_platform_default_populate(struct device_node *root,                                 const struct of_dev_auxdata *lookup,                                 struct device *parent){        return of_platform_populate(root, of_default_bus_match_table, lookup,                                    parent);}EXPORT_SYMBOL_GPL(of_platform_default_populate)const struct of_device_id of_default_bus_match_table[] = {        { .compatible = \"simple-bus\", },        { .compatible = \"simple-mfd\", },        { .compatible = \"isa\", },#ifdef CONFIG_ARM_AMBA        { .compatible = \"arm,amba-bus\", },#endif /* CONFIG_ARM_AMBA */        {} /* Empty terminated list */};of_platform_default_populate function is a wrapper function of the of_platform_populate function which is set to be invoked with of_default_bus_match_table as its second parameter.The string contained in the match table will be used to find a node where its compatible string is same as one of the specified compatible string.Also because this is the first function to traverse the device tree,it have NULL value for first and third argument, root and parent./** * of_platform_populate() - Populate platform_devices from device tree data * @root: parent of the first level to probe or NULL for the root of the tree * @matches: match table, NULL to use the default * @lookup: auxdata table for matching id and platform_data with device nodes * @parent: parent to hook devices from, NULL for toplevel * * Similar to of_platform_bus_probe(), this function walks the device tree * and creates devices from nodes.  It differs in that it follows the modern * convention of requiring all device nodes to have a 'compatible' property, * and it is suitable for creating devices which are children of the root * node (of_platform_bus_probe will only create children of the root which * are selected by the @matches argument). * * New board support should be using this function instead of * of_platform_bus_probe(). * * Returns 0 on success, &lt; 0 on failure. */int of_platform_populate(struct device_node *root,                        const struct of_device_id *matches,                        const struct of_dev_auxdata *lookup,                        struct device *parent){               struct device_node *child;        int rc = 0;                root = root ? of_node_get(root) : of_find_node_by_path(\"/\");        if (!root)                return -EINVAL;        pr_debug(\"%s()\\n\", __func__);        pr_debug(\" starting at: %pOF\\n\", root);        device_links_supplier_sync_state_pause();        for_each_child_of_node(root, child) {                rc = of_platform_bus_create(child, matches, lookup, parent, true);                if (rc) {                        of_node_put(child);                        break;                }        }        device_links_supplier_sync_state_resume();        of_node_set_flag(root, OF_POPULATED_BUS);        of_node_put(root);        return rc;}EXPORT_SYMBOL_GPL(of_platform_populate)Becuase the root device_node has been passed as NULL,it should first find the root node by traversing the device tree.The of_find_node_by_path will find the root node.After the root node has been find, it traverse every node in the tree with for_each_child_of_node macro. Note that it invokes of_platform_bus_create functionfor every children of the root which means entire nodes in the device tree.Although it passes 5 parameters,only child and matches parameters are meaningful because others are set as NULL./** * of_platform_bus_create() - Create a device for a node and its children. * @bus: device node of the bus to instantiate * @matches: match table for bus nodes * @lookup: auxdata table for matching id and platform_data with device nodes * @parent: parent for new device, or NULL for top level. * @strict: require compatible property * * Creates a platform_device for the provided device_node, and optionally * recursively create devices for all the child nodes. */static int of_platform_bus_create(struct device_node *bus,                                  const struct of_device_id *matches,                                  const struct of_dev_auxdata *lookup,                                  struct device *parent, bool strict){        const struct of_dev_auxdata *auxdata;        struct device_node *child;        struct platform_device *dev;        const char *bus_id = NULL;        void *platform_data = NULL;        int rc = 0;        /* Make sure it has a compatible property */        if (strict &amp;&amp; (!of_get_property(bus, \"compatible\", NULL))) {                pr_debug(\"%s() - skipping %pOF, no compatible prop\\n\",                         __func__, bus);                return 0;        }        /* Skip nodes for which we don't want to create devices */        if (unlikely(of_match_node(of_skipped_node_table, bus))) {                pr_debug(\"%s() - skipping %pOF node\\n\", __func__, bus);                return 0;        }        if (of_node_check_flag(bus, OF_POPULATED_BUS)) {                pr_debug(\"%s() - skipping %pOF, already populated\\n\",                        __func__, bus);                return 0;        }        auxdata = of_dev_lookup(lookup, bus);        if (auxdata) {                bus_id = auxdata-&gt;name;                platform_data = auxdata-&gt;platform_data;        }        if (of_device_is_compatible(bus, \"arm,primecell\")) {                /*                 * Don't return an error here to keep compatibility with older                 * device tree files.                 */                of_amba_device_create(bus, bus_id, platform_data, parent);                return 0;        }        dev = of_platform_device_create_pdata(bus, bus_id, platform_data, parent);        if (!dev || !of_match_node(matches, bus))                return 0;        for_each_child_of_node(bus, child) {                pr_debug(\"   create child: %pOF\\n\", child);                rc = of_platform_bus_create(child, matches, lookup, &amp;dev-&gt;dev, strict);                if (rc) {                        of_node_put(child);                        break;                }        }        of_node_set_flag(bus, OF_POPULATED_BUS);        return rc;}Some readers might wonder why the name of the function isnot “of_platform_device_create”, but bus. Also, remember that the matches parameter passed to this function is of_default_bus_match_table,and this is usually matches with a node that has compatible string names “simple-bus” in most cases.Allocating and intializing platform device from device node/** * of_platform_device_create_pdata - Alloc, initialize and register an of_device * @np: pointer to node to create device for * @bus_id: name to assign device * @platform_data: pointer to populate platform_data pointer with * @parent: Linux device model parent device. * * Returns pointer to created platform device, or NULL if a device was not * registered.  Unavailable devices will not get registered. */static struct platform_device *of_platform_device_create_pdata(                                        struct device_node *np,                                        const char *bus_id,                                        void *platform_data,                                        struct device *parent){        struct platform_device *dev;        if (!of_device_is_available(np) ||            of_node_test_and_set_flag(np, OF_POPULATED))                return NULL;        dev = of_device_alloc(np, bus_id, parent);        if (!dev)                goto err_clear_flag;        dev-&gt;dev.coherent_dma_mask = DMA_BIT_MASK(32);        if (!dev-&gt;dev.dma_mask)                dev-&gt;dev.dma_mask = &amp;dev-&gt;dev.coherent_dma_mask;        dev-&gt;dev.bus = &amp;platform_bus_type;        dev-&gt;dev.platform_data = platform_data;        of_msi_configure(&amp;dev-&gt;dev, dev-&gt;dev.of_node);        if (of_device_add(dev) != 0) {                platform_device_put(dev);                goto err_clear_flag;        }        return dev;err_clear_flag:        of_node_clear_flag(np, OF_POPULATED);        return NULL;}The above function allocates the platform_device (of_device_alloc)and assign the platform_bus bus_type to the generated device.After that, the generated device is registered to the bus sub-system with the device_add functionthat we covered before. The device_add function is invoked inside the of_device_add function.Let’s take a look at step by stepfrom the device creation to its register.struct platform_device {        const char      *name;        int             id;        bool            id_auto;        struct device   dev;        u64             platform_dma_mask;        struct device_dma_parameters dma_parms;        u32             num_resources;        struct resource *resource;        const struct platform_device_id *id_entry;        char *driver_override; /* Driver name to force a match */        /* MFD cell pointer */        struct mfd_cell *mfd_cell;        /* arch specific additions */        struct pdev_archdata    archdata;};/** * of_device_alloc - Allocate and initialize an of_device * @np: device node to assign to device * @bus_id: Name to assign to the device.  May be null to use default name. * @parent: Parent device. */struct platform_device *of_device_alloc(struct device_node *np,                                  const char *bus_id,                                  struct device *parent){        struct platform_device *dev;        int rc, i, num_reg = 0, num_irq;        struct resource *res, temp_res;        dev = platform_device_alloc(\"\", PLATFORM_DEVID_NONE);        if (!dev)                return NULL;        /* count the io and irq resources */        while (of_address_to_resource(np, num_reg, &amp;temp_res) == 0)                num_reg++;        num_irq = of_irq_count(np);        /* Populate the resource table */        if (num_irq || num_reg) {                res = kcalloc(num_irq + num_reg, sizeof(*res), GFP_KERNEL);                if (!res) {                        platform_device_put(dev);                        return NULL;                }                dev-&gt;num_resources = num_reg + num_irq;                dev-&gt;resource = res;                for (i = 0; i &lt; num_reg; i++, res++) {                        rc = of_address_to_resource(np, i, res);                        WARN_ON(rc);                }                if (of_irq_to_resource_table(np, res, num_irq) != num_irq)                        pr_debug(\"not all legacy IRQ resources mapped for %pOFn\\n\",                                 np);        }        dev-&gt;dev.of_node = of_node_get(np);        dev-&gt;dev.fwnode = &amp;np-&gt;fwnode;        dev-&gt;dev.parent = parent ? : &amp;platform_bus;        if (bus_id)                dev_set_name(&amp;dev-&gt;dev, \"%s\", bus_id);        else                of_device_make_bus_id(&amp;dev-&gt;dev);        return dev;}EXPORT_SYMBOL(of_device_alloc);Compared to other devices registration,all required information to populate and manage the device is described in the device tree itself. Those information should be stored in the generated device structureto allow the device driver which will be bound to the current device to manage the device properly.One device tree node can contain two important resource related to the device: register and interrupt.However, until the device tree has been parsed,we cannot know how many resource the device does have.Therefore, to allocate an array dynamically based on the number of resource,it needs number indication how many register and interrupt resources are available for the device. Based on that information, it allocates a resource array and copiesall resource from the device tree to the resource array.After the resource has been successfully parsed and stored in the resource array of the platform device, it sets the device node of the current deviceto its of_node member field which will be required to access the device tree node in the device driver later.Also, it sets parent node as platform_bus when it is NULL.After the platform_device object for the current device node has been properly allocated and set,it returns the device.The returned device is passed to of_device_add function,which register the generated device to the bus subsystem.        dev = of_device_alloc(np, bus_id, parent);        if (!dev)                goto err_clear_flag;                dev-&gt;dev.coherent_dma_mask = DMA_BIT_MASK(32);        if (!dev-&gt;dev.dma_mask)                dev-&gt;dev.dma_mask = &amp;dev-&gt;dev.coherent_dma_mask;        dev-&gt;dev.bus = &amp;platform_bus_type;        dev-&gt;dev.platform_data = platform_data;        of_msi_configure(&amp;dev-&gt;dev, dev-&gt;dev.of_node);Note that below code of the of_platform_device_create_pdatasets the bus of the generated platform device as platform_bus_type which has been registered as busfor platform devices at platform_bus_init function.Adding generated platform_deviceThen let’s take a look at how the generated platform device can be addedto the driver sub-system.int of_device_add(struct platform_device *ofdev){        BUG_ON(ofdev-&gt;dev.of_node == NULL);        /* name and id have to be set so that the platform bus doesn't get         * confused on matching */                  ofdev-&gt;name = dev_name(&amp;ofdev-&gt;dev);        ofdev-&gt;id = PLATFORM_DEVID_NONE;        /*         * If this device has not binding numa node in devicetree, that is         * of_node_to_nid returns NUMA_NO_NODE. device_add will assume that this         * device is on the same node as the parent.         */                                              set_dev_node(&amp;ofdev-&gt;dev, of_node_to_nid(ofdev-&gt;dev.of_node));                                                         return device_add(&amp;ofdev-&gt;dev);}  The of_device_add function is kind of a wrapper functionthat sets some fields of the platform device for one device node and just invokes the device_add functionwith its device structure embedded in the platform_device structure.Now it’s time to revisit device_add function once again.Compared to when the device_add function is invoked to add the platform bus as device to the driver system,current device_add function is invoked to add the platform device to the platform bus.Becuase most details are already covered,we will highlights some part of it related to registering the device to the bus.In the middle of device_add function,it invokes bus_add_device function with the passed device structure.int bus_add_device(struct device *dev){               struct bus_type *bus = bus_get(dev-&gt;bus);        int error = 0;        if (bus) {                pr_debug(\"bus: '%s': add device %s\\n\", bus-&gt;name, dev_name(dev));                error = device_add_groups(dev, bus-&gt;dev_groups);                if (error)                        goto out_put;                error = sysfs_create_link(&amp;bus-&gt;p-&gt;devices_kset-&gt;kobj,                                                &amp;dev-&gt;kobj, dev_name(dev));                if (error)                        goto out_groups;                error = sysfs_create_link(&amp;dev-&gt;kobj,                                &amp;dev-&gt;bus-&gt;p-&gt;subsys.kobj, \"subsystem\");                if (error)                        goto out_subsys;                klist_add_tail(&amp;dev-&gt;p-&gt;knode_bus, &amp;bus-&gt;p-&gt;klist_devices);        }        return 0;        out_subsys:        sysfs_remove_link(&amp;bus-&gt;p-&gt;devices_kset-&gt;kobj, dev_name(dev));out_groups:             device_remove_groups(dev, bus-&gt;dev_groups);out_put:                        bus_put(dev-&gt;bus);        return error; }   Compared to previous device_add, which doesn’t have a bus field because it was a platform device itself,current platfrom device’s device has bus which is a platform_bus_type.After invoking several functions to register the device to the sysfs,klist_add_tail(&amp;dev-&gt;p-&gt;knode_bus, &amp;bus-&gt;p-&gt;klist_devices)macro adds the current device to the klist_devices klist which are managed by the target bus.Note that the private field of the bus is used to register the device to the bus subsystem.Remember thatwe allocated a private platform bus subsystem using the platform_bus_typewhen the bus_register function has been invoked at the platform_bus_init.Because platform_bus_type is a global structure and has been initialized to have private sub-system,whenever any device who wants to be attached to that busshould make the bus member field of the device to reference platform_bus_type.Binding the device to the driver/**       * bus_probe_device - probe drivers for a new device * @dev: device to probe  *        * - Automatically probe for a driver if the bus allows it. */      void bus_probe_device(struct device *dev){                       struct bus_type *bus = dev-&gt;bus;        struct subsys_interface *sif;        if (!bus)                return;                        if (bus-&gt;p-&gt;drivers_autoprobe)                device_initial_probe(dev);                mutex_lock(&amp;bus-&gt;p-&gt;mutex);        list_for_each_entry(sif, &amp;bus-&gt;p-&gt;interfaces, node)                if (sif-&gt;add_dev)                        sif-&gt;add_dev(dev, sif);        mutex_unlock(&amp;bus-&gt;p-&gt;mutex);}               After the device has been registered to the platform_bus bus type, the device can be bound to the corresponding driver if possible.We didn’t cover this function before because previous platform_bus device doesn’t have bus field and it just returned instead of trying to bind the driver to the device.However, current platform device has the bus,it can ask the bus to find the driver associated with current device.drivers/base/dd.cvoid device_initial_probe(struct device *dev){        __device_attach(dev, true);}static int __device_attach(struct device *dev, bool allow_async){        int ret = 0;        device_lock(dev);        if (dev-&gt;p-&gt;dead) {                goto out_unlock;        } else if (dev-&gt;driver) {                if (device_is_bound(dev)) {                        ret = 1;                        goto out_unlock;                }                ret = device_bind_driver(dev);                if (ret == 0)                        ret = 1;                else {                        dev-&gt;driver = NULL;                        ret = 0;                }        } else {                struct device_attach_data data = {                        .dev = dev,                        .check_async = allow_async,                        .want_async = false,                };                if (dev-&gt;parent)                        pm_runtime_get_sync(dev-&gt;parent);                ret = bus_for_each_drv(dev-&gt;bus, NULL, &amp;data,                                        __device_attach_driver);                if (!ret &amp;&amp; allow_async &amp;&amp; data.have_async) {                        /*                         * If we could not find appropriate driver                         * synchronously and we are allowed to do                         * async probes and there are drivers that                         * want to probe asynchronously, we'll                         * try them.                         */                        dev_dbg(dev, \"scheduling asynchronous probe\\n\");                        get_device(dev);                        async_schedule_dev(__device_attach_async_helper, dev);                } else {                        pm_request_idle(dev);                }                if (dev-&gt;parent)                        pm_runtime_put(dev-&gt;parent);        }out_unlock:        device_unlock(dev);        return ret;}Because our device is not dead,the else statement should be executed,and bus_for_each_drv function will run __device_attach_driver function against all drivers managed by the bus.Before the iteration starts,it sets the data which containsthe device that we are trying to registerso that the __device_attach_driver functioncan have access on the device.And the function traverse the klist_drivers of the bus and select one driver registered to the bus one by oneand pass it to the __device_attach_driver function.As a result,the function can have access not only on the device but alsothe possible candidate driver that might manage our device.static int __device_attach_driver(struct device_driver *drv, void *_data){        struct device_attach_data *data = _data;        struct device *dev = data-&gt;dev;        bool async_allowed;        int ret;        ret = driver_match_device(drv, dev);        if (ret == 0) {                /* no match */                return 0;        } else if (ret == -EPROBE_DEFER) {                dev_dbg(dev, \"Device match requests probe deferral\\n\");                driver_deferred_probe_add(dev);        } else if (ret &lt; 0) {                dev_dbg(dev, \"Bus failed to match device: %d\\n\", ret);                return ret;        } /* ret &gt; 0 means positive match */        async_allowed = driver_allows_async_probing(drv);        if (async_allowed)                data-&gt;have_async = true;        if (data-&gt;check_async &amp;&amp; async_allowed != data-&gt;want_async)                return 0;        return driver_probe_device(drv, dev);}static inline int driver_match_device(struct device_driver *drv,                                      struct device *dev){        return drv-&gt;bus-&gt;match ? drv-&gt;bus-&gt;match(dev, drv) : 1;}Note that the currently selected driver,passed as the first argument of the __device_attach_driver functionis just one of the drivers registered to the bus.Therefore, first job of this function is trying to figure out if the currently selected driver has capability to manage our device.%To achieve this, it invokes the driver_match_device functionwith the selected driver and device object.This function is a simple macro that invokes the match functionof the bus attached to the device.Platform device match functionstruct bus_type platform_bus_type = {        .name           = \"platform\",        .dev_groups     = platform_dev_groups,        .match          = platform_match,        .uevent         = platform_uevent,        .probe          = platform_probe,        .remove         = platform_remove,        .shutdown       = platform_shutdown,        .dma_configure  = platform_dma_configure,        .pm             = &amp;platform_dev_pm_ops,};In this case, we assign the platform_bus_type as our platform device’s bus,its match function, platform_match, should be invoked./** * platform_match - bind platform device to platform driver. * @dev: device. * @drv: driver. * * Platform device IDs are assumed to be encoded like this: * \"&lt;name&gt;&lt;instance&gt;\", where &lt;name&gt; is a short description of the type of * device, like \"pci\" or \"floppy\", and &lt;instance&gt; is the enumerated * instance of the device, like '0' or '42'.  Driver IDs are simply * \"&lt;name&gt;\".  So, extract the &lt;name&gt; from the platform_device structure, * and compare it against the name of the driver. Return whether they match * or not. */static int platform_match(struct device *dev, struct device_driver *drv){        struct platform_device *pdev = to_platform_device(dev);        struct platform_driver *pdrv = to_platform_driver(drv);        /* When driver_override is set, only bind to the matching driver */        if (pdev-&gt;driver_override)                return !strcmp(pdev-&gt;driver_override, drv-&gt;name);        /* Attempt an OF style match first */        if (of_driver_match_device(dev, drv))                return 1;        /* Then try ACPI style match */        if (acpi_driver_match_device(dev, drv))                return 1;        /* Then try to match against the id table */        if (pdrv-&gt;id_table)                return platform_match_id(pdrv-&gt;id_table, pdev) != NULL;        /* fall-back to driver name match */        return (strcmp(pdev-&gt;name, drv-&gt;name) == 0);}When the platform_match function is invoked,it firstly translates the generic drvier and device to platform driver and platform device.Even though each bus utilize different type of device and driver following the detailed implementation of the bus,because the probe function is invoked through a generic interface of linux driver system, the match function of the bus should have generic type for device and driver, and tralsate them to the bus specific ones.After that, it invokes different match functionsbecause platform device can be registered through multiple different methods.We have a platform device detected and generated by the device file, so of_driver_match function should match the device and driver using the compatible strings specified in the device driver supporting device tree. When the driver does not support the device tree,then other matching functions should be in charge of device-to-driver matching.Binding the matching driver and deviceIf the matching function finds a driver claiming that it can support currently being registered device,__device_attach_driver function invokes the driver_probe_device functionwhich finally binds the device and its corresponding driver./** * driver_probe_device - attempt to bind device &amp; driver together * @drv: driver to bind a device to * @dev: device to try to bind to the driver * * This function returns -ENODEV if the device is not registered, * 1 if the device is bound successfully and 0 otherwise. * * This function must be called with @dev lock held.  When called for a * USB interface, @dev-&gt;parent lock must be held as well. * * If the device has a parent, runtime-resume the parent before driver probing. */static int driver_probe_device(struct device_driver *drv, struct device *dev){        int ret = 0;        if (!device_is_registered(dev))                return -ENODEV;        pr_debug(\"bus: '%s': %s: matched device %s with driver %s\\n\",                 drv-&gt;bus-&gt;name, __func__, dev_name(dev), drv-&gt;name);        pm_runtime_get_suppliers(dev);        if (dev-&gt;parent)                pm_runtime_get_sync(dev-&gt;parent);        pm_runtime_barrier(dev);        if (initcall_debug)                ret = really_probe_debug(dev, drv);        else                ret = really_probe(dev, drv);        pm_request_idle(dev);        if (dev-&gt;parent)                pm_runtime_put(dev-&gt;parent);        pm_runtime_put_suppliers(dev);        return ret;}To bind the driver to the device,the matching driver should be invoked.The probe function of the matching driver provides an entry function for this binding process. As shown in the code,when it is not in debug mode,the really_probe function will be invoked,and will call the probe function of the matching driver.static int really_probe(struct device *dev, struct device_driver *drv){        int ret = -EPROBE_DEFER;        int local_trigger_count = atomic_read(&amp;deferred_trigger_count);        bool test_remove = IS_ENABLED(CONFIG_DEBUG_TEST_DRIVER_REMOVE) &amp;&amp;                           !drv-&gt;suppress_bind_attrs;        if (defer_all_probes) {                /*                 * Value of defer_all_probes can be set only by                 * device_block_probing() which, in turn, will call                 * wait_for_device_probe() right after that to avoid any races.                 */                dev_dbg(dev, \"Driver %s force probe deferral\\n\", drv-&gt;name);                driver_deferred_probe_add(dev);                return ret;        }        ret = device_links_check_suppliers(dev);        if (ret == -EPROBE_DEFER)                driver_deferred_probe_add_trigger(dev, local_trigger_count);        if (ret)                return ret;        atomic_inc(&amp;probe_count);        pr_debug(\"bus: '%s': %s: probing driver %s with device %s\\n\",                 drv-&gt;bus-&gt;name, __func__, drv-&gt;name, dev_name(dev));        if (!list_empty(&amp;dev-&gt;devres_head)) {                dev_crit(dev, \"Resources present before probing\\n\");                ret = -EBUSY;                goto done;        }re_probe:        dev-&gt;driver = drv;        /* If using pinctrl, bind pins now before probing */        ret = pinctrl_bind_pins(dev);        if (ret)                goto pinctrl_bind_failed;        if (dev-&gt;bus-&gt;dma_configure) {                ret = dev-&gt;bus-&gt;dma_configure(dev);                if (ret)                        goto probe_failed;        }        if (driver_sysfs_add(dev)) {                pr_err(\"%s: driver_sysfs_add(%s) failed\\n\",                       __func__, dev_name(dev));                goto probe_failed;        }        if (dev-&gt;pm_domain &amp;&amp; dev-&gt;pm_domain-&gt;activate) {                ret = dev-&gt;pm_domain-&gt;activate(dev);                if (ret)                        goto probe_failed;        }        if (dev-&gt;bus-&gt;probe) {                ret = dev-&gt;bus-&gt;probe(dev);                if (ret)                        goto probe_failed;        } else if (drv-&gt;probe) {                ret = drv-&gt;probe(dev);                if (ret)                        goto probe_failed;        }        if (device_add_groups(dev, drv-&gt;dev_groups)) {                dev_err(dev, \"device_add_groups() failed\\n\");                goto dev_groups_failed;        }        if (dev_has_sync_state(dev) &amp;&amp;            device_create_file(dev, &amp;dev_attr_state_synced)) {                dev_err(dev, \"state_synced sysfs add failed\\n\");                goto dev_sysfs_state_synced_failed;        }        if (test_remove) {                test_remove = false;                device_remove_file(dev, &amp;dev_attr_state_synced);                device_remove_groups(dev, drv-&gt;dev_groups);                if (dev-&gt;bus-&gt;remove)                        dev-&gt;bus-&gt;remove(dev);                else if (drv-&gt;remove)                        drv-&gt;remove(dev);                devres_release_all(dev);                driver_sysfs_remove(dev);                dev-&gt;driver = NULL;                dev_set_drvdata(dev, NULL);                if (dev-&gt;pm_domain &amp;&amp; dev-&gt;pm_domain-&gt;dismiss)                        dev-&gt;pm_domain-&gt;dismiss(dev);                pm_runtime_reinit(dev);                goto re_probe;        }        pinctrl_init_done(dev);        if (dev-&gt;pm_domain &amp;&amp; dev-&gt;pm_domain-&gt;sync)                dev-&gt;pm_domain-&gt;sync(dev);        driver_bound(dev);        ret = 1;        pr_debug(\"bus: '%s': %s: bound device %s to driver %s\\n\",                 drv-&gt;bus-&gt;name, __func__, dev_name(dev), drv-&gt;name);        goto done;dev_sysfs_state_synced_failed:        device_remove_groups(dev, drv-&gt;dev_groups);dev_groups_failed:        if (dev-&gt;bus-&gt;remove)                dev-&gt;bus-&gt;remove(dev);        else if (drv-&gt;remove)                drv-&gt;remove(dev);probe_failed:        if (dev-&gt;bus)                blocking_notifier_call_chain(&amp;dev-&gt;bus-&gt;p-&gt;bus_notifier,                                             BUS_NOTIFY_DRIVER_NOT_BOUND, dev);pinctrl_bind_failed:        device_links_no_driver(dev);        devres_release_all(dev);        arch_teardown_dma_ops(dev);        driver_sysfs_remove(dev);        dev-&gt;driver = NULL;        dev_set_drvdata(dev, NULL);        if (dev-&gt;pm_domain &amp;&amp; dev-&gt;pm_domain-&gt;dismiss)                dev-&gt;pm_domain-&gt;dismiss(dev);        pm_runtime_reinit(dev);        dev_pm_set_driver_flags(dev, 0);        switch (ret) {        case -EPROBE_DEFER:                /* Driver requested deferred probing */                dev_dbg(dev, \"Driver %s requests probe deferral\\n\", drv-&gt;name);                driver_deferred_probe_add_trigger(dev, local_trigger_count);                break;        case -ENODEV:        case -ENXIO:                pr_debug(\"%s: probe of %s rejects match %d\\n\",                         drv-&gt;name, dev_name(dev), ret);                break;        default:                /* driver matched but the probe failed */                pr_warn(\"%s: probe of %s failed with error %d\\n\",                        drv-&gt;name, dev_name(dev), ret);        }        /*         * Ignore errors returned by -&gt;probe so that the next driver can try         * its luck.         */        ret = 0;done:        atomic_dec(&amp;probe_count);        wake_up_all(&amp;probe_waitqueue);        return ret;}Although there are many complex details in the really_probe function,our interest is only when the probe of the matching driver is invoked.When you look at the middle of the function,you can find that below code block invokes the probe function of the bus or the driver based on condition.        if (dev-&gt;bus-&gt;probe) {                ret = dev-&gt;bus-&gt;probe(dev);                if (ret)                        goto probe_failed;        } else if (drv-&gt;probe) {                ret = drv-&gt;probe(dev);                if (ret)                        goto probe_failed;        }Because our bus, platform_bus_typehas its own probe function, platform_probe,the probe function of the bus should be calledinstead of invoking the driver’s probe function directly.Let’s see the detailed implementation of the probe function of our bus,platform_probe.static int platform_probe(struct device *_dev){        struct platform_driver *drv = to_platform_driver(_dev-&gt;driver);        struct platform_device *dev = to_platform_device(_dev);        int ret;        /*         * A driver registered using platform_driver_probe() cannot be bound         * again later because the probe function usually lives in __init code         * and so is gone. For these drivers .probe is set to         * platform_probe_fail in __platform_driver_probe(). Don't even prepare         * clocks and PM domains for these to match the traditional behaviour.         */        if (unlikely(drv-&gt;probe == platform_probe_fail))                return -ENXIO;        ret = of_clk_set_defaults(_dev-&gt;of_node, false);        if (ret &lt; 0)                return ret;        ret = dev_pm_domain_attach(_dev, true);        if (ret)                goto out;        if (drv-&gt;probe) {                ret = drv-&gt;probe(dev);                if (ret)                        dev_pm_domain_detach(_dev, true);        }out:        if (drv-&gt;prevent_deferred_probe &amp;&amp; ret == -EPROBE_DEFER) {                dev_warn(_dev, \"probe deferral not supported\\n\");                ret = -ENXIO;        }        return ret;}Although we only have access to the device, because we already register the matching driver to device’s driver field beforewe can retrieve the matching driver’s object(check the re_probe jump flag of the really_probe function).Because platform_probe function is a generic wrapper probe for all platform devices,it invokes several functions to manage the device as platform devicesuch as attaching power domain or setting the clk for the device.After those generic settings are done,the real probe function of the matching driver is invoked. Although, the platform_probe function only passesthe platform device object to the probe function,different buses can support different prototype of probe function.In that case the bus’ probe function will feed those operands before the driver’s probe function is invoked.For the probe function of the matching driver,you should take a look at the implementation of the probe functionin the corresponding device driver.We are not going to take a look at probe function of one particular device in this posting.After the probing function of the matching driver is invoked,rest part of the device_register function."
  },
  
  {
    "title": "Initcalls",
    "url": "/posts/initcalls/",
    "categories": "linux,, embedded-linux",
    "tags": "",
    "date": "2021-04-28 00:00:00 -0400",
    





    
    "snippet": "do_initcallsstatic void __init do_basic_setup(void){        cpuset_init_smp();        driver_init();        init_irq_proc();        do_ctors();        usermodehelper_enable();        do_initcalls()...",
    "content": "do_initcallsstatic void __init do_basic_setup(void){        cpuset_init_smp();        driver_init();        init_irq_proc();        do_ctors();        usermodehelper_enable();        do_initcalls();}Now let’s go back to previous setup function do_basic_setup.Because rest of the init functions are not important in this posting,so we will directly jump into do_initcalls function.static void __init do_initcalls(void){        int level;        size_t len = strlen(saved_command_line) + 1;        char *command_line;        command_line = kzalloc(len, GFP_KERNEL);        if (!command_line)                panic(\"%s: Failed to allocate %zu bytes\\n\", __func__, len);        for (level = 0; level &lt; ARRAY_SIZE(initcall_levels) - 1; level++) {                /* Parser modifies command_line, restore it each time */                strcpy(command_line, saved_command_line);                do_initcall_level(level, command_line);        }        kfree(command_line);}static void __init do_initcall_level(int level, char *command_line){        initcall_entry_t *fn;        parse_args(initcall_level_names[level],                   command_line, __start___param,                   __stop___param - __start___param,                   level, level,                   NULL, ignore_unknown_bootoption);        trace_initcall_level(initcall_level_names[level]);        for (fn = initcall_levels[level]; fn &lt; initcall_levels[level+1]; fn++)                do_one_initcall(initcall_from_entry(fn));}/* Keep these in sync with initcalls in include/linux/init.h */static const char *initcall_level_names[] __initdata = {        \"pure\",        \"core\",        \"postcore\",        \"arch\",        \"subsys\",        \"fs\",        \"device\",        \"late\",};typedef int (*initcall_t)(void);int __init_or_module do_one_initcall(initcall_t fn){        int count = preempt_count();        char msgbuf[64];        int ret;        if (initcall_blacklisted(fn))                return -EPERM;        do_trace_initcall_start(fn);        ret = fn();        do_trace_initcall_finish(fn, ret);        msgbuf[0] = 0;        if (preempt_count() != count) {                sprintf(msgbuf, \"preemption imbalance \");                preempt_count_set(count);        }        if (irqs_disabled()) {                strlcat(msgbuf, \"disabled interrupts \", sizeof(msgbuf));                local_irq_enable();        }        WARN(msgbuf[0], \"initcall %pS returned with %s\\n\", fn, msgbuf);        add_latent_entropy();        return ret;}For each predefined level, do_initcall invokes all the relevant init functionsfor that level.As shown in the initcall_level_names, there are 8 levels of init,and do_initcall_level function is invoked per level. This function actually invokes all init functionsstored in a particular code sectiondedicated for one level.The do_one_initcall function actually invokes the initcall functions one by one.The type of function pointer of the initcalls are defined as initcall_t.The function pointer is passed from the do_initcall_level function.Each function pointer is retrieved as a result of initcall_from_entry function, and its parameter fn is retrievedfrom the initcall_levels array.Let’s take a look at those structures and functions one by one.#ifdef CONFIG_HAVE_ARCH_PREL32_RELOCATIONStypedef int initcall_entry_t;        static inline initcall_t initcall_from_entry(initcall_entry_t *entry){               return offset_to_ptr(entry);}#elsetypedef initcall_t initcall_entry_t;static inline initcall_t initcall_from_entry(initcall_entry_t *entry){               return *entry;}#endif  extern initcall_entry_t __initcall_start[];extern initcall_entry_t __initcall0_start[];extern initcall_entry_t __initcall1_start[];extern initcall_entry_t __initcall2_start[];extern initcall_entry_t __initcall3_start[];extern initcall_entry_t __initcall4_start[];extern initcall_entry_t __initcall5_start[];extern initcall_entry_t __initcall6_start[];extern initcall_entry_t __initcall7_start[];extern initcall_entry_t __initcall_end[];static initcall_entry_t *initcall_levels[] __initdata = {        __initcall0_start,        __initcall1_start,        __initcall2_start,        __initcall3_start,        __initcall4_start,        __initcall5_start,        __initcall6_start,        __initcall7_start,        __initcall_end,};initcall_levels array consists of multiple initcall_entry_twhich is a intger value imported from linux kernel header.include/asm-generic/vmlinux.lds.h#define INIT_CALLS_LEVEL(level)                                         \\                __initcall##level##_start = .;                          \\                KEEP(*(.initcall##level##.init))                        \\                KEEP(*(.initcall##level##s.init))                       \\#define INIT_CALLS                                                      \\                __initcall_start = .;                                   \\                KEEP(*(.initcallearly.init))                            \\                INIT_CALLS_LEVEL(0)                                     \\                INIT_CALLS_LEVEL(1)                                     \\                INIT_CALLS_LEVEL(2)                                     \\                INIT_CALLS_LEVEL(3)                                     \\                INIT_CALLS_LEVEL(4)                                     \\                INIT_CALLS_LEVEL(5)                                     \\                INIT_CALLS_LEVEL(rootfs)                                \\                INIT_CALLS_LEVEL(6)                                     \\                INIT_CALLS_LEVEL(7)                                     \\                __initcall_end = .;#define INIT_DATA_SECTION(initsetup_align)                              \\        .init.data : AT(ADDR(.init.data) - LOAD_OFFSET) {               \\                INIT_DATA                                               \\                INIT_SETUP(initsetup_align)                             \\                INIT_CALLS                                              \\                CON_INITCALL                                            \\                INIT_RAM_FS                                             \\        }Linux kernel linker script defines INIT_CALLS_LEVEL macro that defines variablethat contains the starting address of memory region that have all initcall of specific level.It also provides INIT_CALLS macrothat populates all the memory addresses used by the initcall_level array.We can find that each _initcall##level##_start is followed by the .initcall##level##.init and .initcall##level##s.init section.Because level is a integer from 0 to 7the section name should be from.initcall0.initto .initcall7.initLet’s try to figure out where those sections are defined,and what content are stored in that section.include/linux/init.h/* * initcalls are now grouped by functionality into separate * subsections. Ordering inside the subsections is determined * by link order. * For backwards compatibility, initcall() puts the call in * the device init subsection. * * The `id' arg to __define_initcall() is needed so that multiple initcalls * can point at the same handler without causing duplicate-symbol build errors. * * Initcalls are run by placing pointers in initcall sections that the * kernel iterates at runtime. The linker can do dead code / data elimination * and remove that completely, so the initcall sections have to be marked * as KEEP() in the linker script. */#ifdef CONFIG_HAVE_ARCH_PREL32_RELOCATIONS#define ___define_initcall(fn, id, __sec)                       \\        __ADDRESSABLE(fn)                                       \\        asm(\".section   \\\"\" #__sec \".init\\\", \\\"a\\\"      \\n\"     \\        \"__initcall_\" #fn #id \":                        \\n\"     \\            \".long      \" #fn \" - .                     \\n\"     \\            \".previous                                  \\n\");#else#define ___define_initcall(fn, id, __sec) \\        static initcall_t __initcall_##fn##id __used \\                __attribute__((__section__(#__sec \".init\"))) = fn;#endif#define __define_initcall(fn, id) ___define_initcall(fn, id, .initcall##id)#define pure_initcall(fn)               __define_initcall(fn, 0)#define core_initcall(fn)               __define_initcall(fn, 1)#define core_initcall_sync(fn)          __define_initcall(fn, 1s)#define postcore_initcall(fn)           __define_initcall(fn, 2)#define postcore_initcall_sync(fn)      __define_initcall(fn, 2s)#define arch_initcall(fn)               __define_initcall(fn, 3)#define arch_initcall_sync(fn)          __define_initcall(fn, 3s)#define subsys_initcall(fn)             __define_initcall(fn, 4)#define subsys_initcall_sync(fn)        __define_initcall(fn, 4s)#define fs_initcall(fn)                 __define_initcall(fn, 5)#define fs_initcall_sync(fn)            __define_initcall(fn, 5s)#define rootfs_initcall(fn)             __define_initcall(fn, rootfs)#define device_initcall(fn)             __define_initcall(fn, 6)#define device_initcall_sync(fn)        __define_initcall(fn, 6s)#define late_initcall(fn)               __define_initcall(fn, 7)#define late_initcall_sync(fn)          __define_initcall(fn, 7s)Following the above macros,it is easy to understand how the .initcallX.init section is generated, and each function is located in that section."
  },
  
  {
    "title": "Gem5 Memaccess",
    "url": "/posts/gem5-memaccess/",
    "categories": "GEM5,, Microops",
    "tags": "",
    "date": "2020-06-05 00:00:00 -0400",
    





    
    "snippet": "In my previous post, I discussed the automatic generation of C++ classes for macroops and microops using various GEM5 tools, including a Python-based parser and string-based template substitution. ...",
    "content": "In my previous post, I discussed the automatic generation of C++ classes for macroops and microops using various GEM5 tools, including a Python-based parser and string-based template substitution. I also provided an example, explaining how a class definition and its constructor for micro-load instructions are generated. Additionally, I presented several definitions that implement the actual semantics of micro-load operations, including the execute function.The instructions are designed to change internal state of the system. More specifically, by executing certain instructions, they can induce specific changes in registers, memory, or internal states that are represented as architectural elements.Given that GEM5 operates as an architecture-level emulator, the execution of a single micro-op should result in the alteration of a particular data structure representing a segment of the architecture. To achieve this, GEM5 provides the“ExecContext” class, which emulates the entire underlying architecture. Additionally, the “execute” method and other definitions of the micro-operationsare designed to modify the “ExecContext” as a consequence of their execution. In essence, these definitions emulate the semantics of the instructions.We will explore how the execution of a micro-op can modify the underlying architectural state through updating “ExecContext. To comprehend how GEM5 executes micro-operations, we will briefly examine the pipeline of a simple processor.CPU pipeline of the simple processor: fetch-decode-executeTo understand how GEM5 emulates the entire architecture, one crucial question toaddress is: When and how does GEM5 execute the next instruction? In other words, we must understand who utilizes the automatically generated micro-op class and its functions.Each CPU model features a distinct pipeline architecture, and this difference significantly influences the execution of instructions within the pipeline. To shed light on this, we will examine the TimingSimple CPU model, which is the most basic CPU pipeline model supported by GEM5.Here are the key characteristics of the SimpleTiming processor model in GEM5:  Single-Cycle Execution: It operates on a single-cycle execution model,      where each instruction is executed in one clock cycle.    Minimal Microarchitecture: It lacks the complexity of multiple pipeline      stages, making it relatively simple and easy to understand.    Idealized Timing: The SimpleTiming model does not account for detailed  timing, such as pipeline hazards or stalls, and assumes that instructions  progress through the pipeline without delays.Processor invokes fetch at every clock tickTo understand how the TimingSimple processor process the events,we should understand which function will be invoked at the schedule event. Scheduling a specific event requires a EventFunctionWrapper instancewhich contains information about event handler function.gem5/src/cpu/simple/timing.hh 51 class TimingSimpleCPU : public BaseSimpleCPU 52 {......325   private:326 327     EventFunctionWrapper fetchEvent;As shown in the above class declaration of the TimingSimpleCPU, I can find that fetchEvent member field is declared as EventFunctionWrapper.To utilize the wrapper to schedule event, proper initialization code is required.gem5/src/cpu/simple/timing.cc  82 TimingSimpleCPU::TimingSimpleCPU(TimingSimpleCPUParams *p)  83     : BaseSimpleCPU(p), fetchTranslation(this), icachePort(this),  84       dcachePort(this), ifetch_pkt(NULL), dcache_pkt(NULL), previousCycle(0),  85       fetchEvent([this]{ fetch(); }, name())  86 {  87     _status = Idle;  88 }As shown in the constructor of the TimingSimpleCPU, it initialize the fetchEvent member field with a function called fetch.Therefore, whenever the fetchEvent is scheduled, the GEM5 will invoke the fetch function and start to fetch the instructionfrom the memory (or cache).fetch: retrieving next instruction to execute from memorygem5/src/cpu/simple/timing.cc 653 void 654 TimingSimpleCPU::fetch() 655 { 656     // Change thread if multi-threaded 657     swapActiveThread(); 658 659     SimpleExecContext &amp;t_info = *threadInfo[curThread]; 660     SimpleThread* thread = t_info.thread; 661 662     DPRINTF(SimpleCPU, \"Fetch\\n\"); 663 664     if (!curStaticInst || !curStaticInst-&gt;isDelayedCommit()) { 665         checkForInterrupts(); 666         checkPcEventQueue(); 667     } 668 669     // We must have just got suspended by a PC event 670     if (_status == Idle) 671         return; 672 673     TheISA::PCState pcState = thread-&gt;pcState(); 674     bool needToFetch = !isRomMicroPC(pcState.microPC()) &amp;&amp; 675                        !curMacroStaticInst; 676 677     if (needToFetch) { 678         _status = BaseSimpleCPU::Running; 679         RequestPtr ifetch_req = std::make_shared&lt;Request&gt;(); 680         ifetch_req-&gt;taskId(taskId()); 681         ifetch_req-&gt;setContext(thread-&gt;contextId()); 682         setupFetchRequest(ifetch_req); 683         DPRINTF(SimpleCPU, \"Translating address %#x\\n\", ifetch_req-&gt;getVaddr()); 684         thread-&gt;itb-&gt;translateTiming(ifetch_req, thread-&gt;getTC(), 685                 &amp;fetchTranslation, BaseTLB::Execute); 686     } else { 687         _status = IcacheWaitResponse; 688         completeIfetch(NULL); 689 690         updateCycleCounts(); 691         updateCycleCounters(BaseCPU::CPU_STATE_ON); 692     } 693 }decodeProcessor can decode the memory blocks as instructionafter the memory has been fetched from the cache or memory.Because timing simple CPU assume memory access takes more than single cycle,it needs to be notified when the requested memory block has been brought to the processor. 874 void 875 TimingSimpleCPU::IcachePort::ITickEvent::process() 876 { 877     cpu-&gt;completeIfetch(pkt); 878 } 879  880 bool 881 TimingSimpleCPU::IcachePort::recvTimingResp(PacketPtr pkt) 882 { 883     DPRINTF(SimpleCPU, \"Received fetch response %#x\\n\", pkt-&gt;getAddr()); 884     // we should only ever see one response per cycle since we only 885     // issue a new request once this response is sunk 886     assert(!tickEvent.scheduled()); 887     // delay processing of returned data until next CPU clock edge 888     tickEvent.schedule(pkt, cpu-&gt;clockEdge()); 889  890     return true; 891 }As a processor is connected to a memory subsystem through the bus,bus should be programmed to invoke a functionthat can handle the fetched instruction, completeIfetch.When IcachePort receive response from memory subsystem, it schedule event with received packet.Because it is scheduled to fire at right next cycle, it ends up invoking completeIfetch function of the TimingSimpleCPU. 775 void 776 TimingSimpleCPU::completeIfetch(PacketPtr pkt) 777 { 778     SimpleExecContext&amp; t_info = *threadInfo[curThread]; 779 780     DPRINTF(SimpleCPU, \"Complete ICache Fetch for addr %#x\\n\", pkt ? 781             pkt-&gt;getAddr() : 0); 782 783     // received a response from the icache: execute the received 784     // instruction 785     assert(!pkt || !pkt-&gt;isError()); 786     assert(_status == IcacheWaitResponse); 787 788     _status = BaseSimpleCPU::Running; 789 790     updateCycleCounts(); 791     updateCycleCounters(BaseCPU::CPU_STATE_ON); 792 793     if (pkt) 794         pkt-&gt;req-&gt;setAccessLatency(); 795 796 797     preExecute(); 798     if (curStaticInst &amp;&amp; curStaticInst-&gt;isMemRef()) { 799         // load or store: just send to dcache 800         Fault fault = curStaticInst-&gt;initiateAcc(&amp;t_info, traceData); 801 802         // If we're not running now the instruction will complete in a dcache 803         // response callback or the instruction faulted and has started an 804         // ifetch 805         if (_status == BaseSimpleCPU::Running) { 806             if (fault != NoFault &amp;&amp; traceData) { 807                 // If there was a fault, we shouldn't trace this instruction. 808                 delete traceData; 809                 traceData = NULL; 810             } 811 812             postExecute(); 813             // @todo remove me after debugging with legion done 814             if (curStaticInst &amp;&amp; (!curStaticInst-&gt;isMicroop() || 815                         curStaticInst-&gt;isFirstMicroop())) 816                 instCnt++; 817             advanceInst(fault); 818         } 819     } else if (curStaticInst) { 820         // non-memory instruction: execute completely now 821         Fault fault = curStaticInst-&gt;execute(&amp;t_info, traceData); 822 823         // keep an instruction count 824         if (fault == NoFault) 825             countInst(); 826         else if (traceData &amp;&amp; !DTRACE(ExecFaulting)) { 827             delete traceData; 828             traceData = NULL; 829         } 830 831         postExecute(); 832         // @todo remove me after debugging with legion done 833         if (curStaticInst &amp;&amp; (!curStaticInst-&gt;isMicroop() || 834                 curStaticInst-&gt;isFirstMicroop())) 835             instCnt++; 836         advanceInst(fault); 837     } else { 838         advanceInst(NoFault); 839     } 840 841     if (pkt) { 842         delete pkt; 843     } 844 }CompleteIfetch instruction consists of four parts:preExecute, instruction execution, postExecute, and advanceInst.By the way, although we cannot see any decoding logicit makes use of curStaticInst to execute fetched instruction.Who decodes the fetched packet and generate curStaticInst?that is a preExecute function.gem5/src/cpu/simple/base.cc481 void482 BaseSimpleCPU::preExecute()483 {484     SimpleExecContext &amp;t_info = *threadInfo[curThread];485     SimpleThread* thread = t_info.thread;486487     // maintain $r0 semantics488     thread-&gt;setIntReg(ZeroReg, 0);489 #if THE_ISA == ALPHA_ISA490     thread-&gt;setFloatReg(ZeroReg, 0);491 #endif // ALPHA_ISA492493     // resets predicates494     t_info.setPredicate(true);495     t_info.setMemAccPredicate(true);496497     // check for instruction-count-based events498     thread-&gt;comInstEventQueue.serviceEvents(t_info.numInst);499500     // decode the instruction501     TheISA::PCState pcState = thread-&gt;pcState();502503     if (isRomMicroPC(pcState.microPC())) {504         t_info.stayAtPC = false;505         curStaticInst = microcodeRom.fetchMicroop(pcState.microPC(),506                                                   curMacroStaticInst);507     } else if (!curMacroStaticInst) {508         //We're not in the middle of a macro instruction509         StaticInstPtr instPtr = NULL;510511         TheISA::Decoder *decoder = &amp;(thread-&gt;decoder);512513         //Predecode, ie bundle up an ExtMachInst514         //If more fetch data is needed, pass it in.515         Addr fetchPC = (pcState.instAddr() &amp; PCMask) + t_info.fetchOffset;516         //if (decoder-&gt;needMoreBytes())517             decoder-&gt;moreBytes(pcState, fetchPC, inst);518         //else519         //    decoder-&gt;process();520521         //Decode an instruction if one is ready. Otherwise, we'll have to522         //fetch beyond the MachInst at the current pc.523         instPtr = decoder-&gt;decode(pcState);524         if (instPtr) {525             t_info.stayAtPC = false;526             thread-&gt;pcState(pcState);527         } else {528             t_info.stayAtPC = true;529             t_info.fetchOffset += sizeof(MachInst);530         }531532         //If we decoded an instruction and it's microcoded, start pulling533         //out micro ops534         if (instPtr &amp;&amp; instPtr-&gt;isMacroop()) {535             curMacroStaticInst = instPtr;536             curStaticInst =537                 curMacroStaticInst-&gt;fetchMicroop(pcState.microPC());538         } else {539             curStaticInst = instPtr;540         }541     } else {542         //Read the next micro op from the macro op543         curStaticInst = curMacroStaticInst-&gt;fetchMicroop(pcState.microPC());544     }545546     //If we decoded an instruction this \"tick\", record information about it.547     if (curStaticInst) {548 #if TRACING_ON549         traceData = tracer-&gt;getInstRecord(curTick(), thread-&gt;getTC(),550                 curStaticInst, thread-&gt;pcState(), curMacroStaticInst);551552         DPRINTF(Decode,\"Decode: Decoded %s instruction: %#x\\n\",553                 curStaticInst-&gt;getName(), curStaticInst-&gt;machInst);554 #endif // TRACING_ON555     }556557     if (branchPred &amp;&amp; curStaticInst &amp;&amp;558         curStaticInst-&gt;isControl()) {559         // Use a fake sequence number since we only have one560         // instruction in flight at the same time.561         const InstSeqNum cur_sn(0);562         t_info.predPC = thread-&gt;pcState();563         const bool predict_taken(564             branchPred-&gt;predict(curStaticInst, cur_sn, t_info.predPC,565                                 curThread));566567         if (predict_taken)568             ++t_info.numPredictedBranches;569     }570 }execute: modify ExecContext based on instructiongem5/build/X86/arch/x86/generated/exec-ns.cc.inc19101     Fault Ld::execute(ExecContext *xc,19102           Trace::InstRecord *traceData) const19103     {19104         Fault fault = NoFault;19105         Addr EA;1910619107         uint64_t Index = 0;19108 uint64_t Base = 0;19109 uint64_t Data = 0;19110 uint64_t SegBase = 0;19111 uint64_t Mem;19112 ;19113         Index = xc-&gt;readIntRegOperand(this, 0);19114 Base = xc-&gt;readIntRegOperand(this, 1);19115 Data = xc-&gt;readIntRegOperand(this, 2);19116 SegBase = xc-&gt;readMiscRegOperand(this, 3);19117 ;19118         EA = SegBase + bits(scale * Index + Base + disp, addressSize * 8 - 1, 0);;19119         DPRINTF(X86, \"%s : %s: The address is %#x\\n\", instMnem, mnemonic, EA);1912019121         fault = readMemAtomic(xc, traceData, EA, Mem, dataSize, memFlags);1912219123         if (fault == NoFault) {19124             Data = merge(Data, Mem, dataSize);;19125         } else if (memFlags &amp; Request::PREFETCH) {19126             // For prefetches, ignore any faults/exceptions.19127             return NoFault;19128         }19129         if(fault == NoFault)19130         {191311913219133         {19134             uint64_t final_val = Data;19135             xc-&gt;setIntRegOperand(this, 0, final_val);1913619137             if (traceData) { traceData-&gt;setData(final_val); }19138         };19139         }1914019141         return fault;19142     }gem5/src/arch/x86/memhelpers.hh106 static Fault107 readMemAtomic(ExecContext *xc, Trace::InstRecord *traceData, Addr addr,108               uint64_t &amp;mem, unsigned dataSize, Request::Flags flags)109 {110     memset(&amp;mem, 0, sizeof(mem));111     Fault fault = xc-&gt;readMem(addr, (uint8_t *)&amp;mem, dataSize, flags);112     if (fault == NoFault) {113         // If LE to LE, this is a nop, if LE to BE, the actual data ends up114         // in the right place because the LSBs where at the low addresses on115         // access. This doesn't work for BE guests.116         mem = letoh(mem);117         if (traceData)118             traceData-&gt;setData(mem);119     }120     return fault;121 }gem5/src/cpu/exec_context.hh 57 /** 58  * The ExecContext is an abstract base class the provides the 59  * interface used by the ISA to manipulate the state of the CPU model. 60  * 61  * Register accessor methods in this class typically provide the index 62  * of the instruction's operand (e.g., 0 or 1), not the architectural 63  * register index, to simplify the implementation of register 64  * renaming.  The architectural register index can be found by 65  * indexing into the instruction's own operand index table. 66  * 67  * @note The methods in this class typically take a raw pointer to the 68  * StaticInst is provided instead of a ref-counted StaticInstPtr to 69  * reduce overhead as an argument. This is fine as long as the 70  * implementation doesn't copy the pointer into any long-term storage 71  * (which is pretty hard to imagine they would have reason to do). 72  */ 73 class ExecContext { 74   public: 75     typedef TheISA::PCState PCState; 76 77     using VecRegContainer = TheISA::VecRegContainer; 78     using VecElem = TheISA::VecElem; 79     using VecPredRegContainer = TheISA::VecPredRegContainer; ... 226     /** 227      * @{ 228      * @name Memory Interface 229      */ 230     /** 231      * Perform an atomic memory read operation.  Must be overridden 232      * for exec contexts that support atomic memory mode.  Not pure 233      * virtual since exec contexts that only support timing memory 234      * mode need not override (though in that case this function 235      * should never be called). 236      */ 237     virtual Fault readMem(Addr addr, uint8_t *data, unsigned int size, 238             Request::Flags flags, 239             const std::vector&lt;bool&gt;&amp; byte_enable = std::vector&lt;bool&gt;()) 240     { 241         panic(\"ExecContext::readMem() should be overridden\\n\"); 242     }As mentioned in the comment,ExecContext class is an abstract base classused to manipulate state of the CPU model. Therefore, each CPU model provides concrete interface that can actually updates CPU context.As an example, let’s take a loot at simple cpu model.gem5/src/cpu/simple/exec_context.hh 61 class SimpleExecContext : public ExecContext { 62   protected: 63     using VecRegContainer = TheISA::VecRegContainer; 64     using VecElem = TheISA::VecElem; 65 66   public: 67     BaseSimpleCPU *cpu; 68     SimpleThread* thread; 69 70     // This is the offset from the current pc that fetch should be performed 71     Addr fetchOffset; 72     // This flag says to stay at the current pc. This is useful for 73     // instructions which go beyond MachInst boundaries. 74     bool stayAtPC; 75 76     // Branch prediction 77     TheISA::PCState predPC; 78 79     /** PER-THREAD STATS */ 80 81     // Number of simulated instructions 82     Counter numInst; 83     Stats::Scalar numInsts; 84     Counter numOp; 85     Stats::Scalar numOps; 86 87     // Number of integer alu accesses 88     Stats::Scalar numIntAluAccesses;...437     Fault438     readMem(Addr addr, uint8_t *data, unsigned int size,439             Request::Flags flags,440             const std::vector&lt;bool&gt;&amp; byte_enable = std::vector&lt;bool&gt;())441         override442     {443         assert(byte_enable.empty() || byte_enable.size() == size);444         return cpu-&gt;readMem(addr, data, size, flags, byte_enable);445     }As shown in the line 437-445,readMem method is overridden by SimpleExecContext classinherited from ExecContext abstract class.Because ExecContext is an interface class,actual memory read operation is done by the corresponding CPU class.For timing CPU, it doesn’t make use of execute,but other autogenerated method to executed ld microop.InitiateAcc: send memory reference119 def template MicroLoadInitiateAcc {{120     Fault %(class_name)s::initiateAcc(ExecContext * xc,121             Trace::InstRecord * traceData) const122     {123         Fault fault = NoFault;124         Addr EA;125126         %(op_decl)s;127         %(op_rd)s;128         %(ea_code)s;129         DPRINTF(X86, \"%s : %s: The address is %#x\\n\", instMnem, mnemonic, EA);130131         fault = initiateMemRead(xc, traceData, EA,132                                 %(memDataSize)s, memFlags);133134         return fault;135     }136 }};137All the template code is required to generate load address.And the generated address is used by initiateMemRead methodto actually access the memory. Note that this method receive ExecContext which is a interface to CPU moduleand the generated logical address EA.Also, memory flags such as prefetch are delivered to the memory module.Remember that memFlags are passed to the class when the microop is constructed.gem5/build/X86/arch/x86/generated/exec-ns.cc.inc19144     Fault Ld::initiateAcc(ExecContext * xc,19145             Trace::InstRecord * traceData) const19146     {19147         Fault fault = NoFault;19148         Addr EA;1914919150         uint64_t Index = 0;19151 uint64_t Base = 0;19152 uint64_t SegBase = 0;19153 ;19154         Index = xc-&gt;readIntRegOperand(this, 0);19155 Base = xc-&gt;readIntRegOperand(this, 1);19156 SegBase = xc-&gt;readMiscRegOperand(this, 3);19157 ;19158         EA = SegBase + bits(scale * Index + Base + disp, addressSize * 8 - 1, 0);;19159         DPRINTF(X86, \"%s : %s: The address is %#x\\n\", instMnem, mnemonic, EA);1916019161         fault = initiateMemRead(xc, traceData, EA,19162                                 dataSize, memFlags);1916319164         return fault;19165     }For memory operation, initiateAcc is the most important functionthat actually initiate memory access. initiateAcc invokes initiateMemRead function, andeach CPU class overrides initiateMemRead method.Before we take a look at the detail implementation,we have to understand that all the CPU specific functions are invoked through the interface ExecContext class.gem5/src/arch/x86/memhelpers.hh 45 /// Initiate a read from memory in timing mode. 46 static Fault 47 initiateMemRead(ExecContext *xc, Trace::InstRecord *traceData, Addr addr, 48                 unsigned dataSize, Request::Flags flags) 49 { 50     return xc-&gt;initiateMemRead(addr, dataSize, flags); 51 }initiateMemRead helper function defined in x86 arch directoryinvokes actual initiateMemRead functionthrough the ExecContext interface.gem5/src/cpu/simple/exec_context.hh447     Fault448     initiateMemRead(Addr addr, unsigned int size,449                     Request::Flags flags,450                     const std::vector&lt;bool&gt;&amp; byte_enable = std::vector&lt;bool&gt;())451         override452     {453         assert(byte_enable.empty() || byte_enable.size() == size);454         return cpu-&gt;initiateMemRead(addr, size, flags, byte_enable);455     }Because we have interest in timing cpu model,let’s figure how the timing cpu model implements initiateMemRead.gem5/src/cpu/simple/timing.cc 418 Fault 419 TimingSimpleCPU::initiateMemRead(Addr addr, unsigned size, 420                                  Request::Flags flags, 421                                  const std::vector&lt;bool&gt;&amp; byte_enable) 422 { 423     SimpleExecContext &amp;t_info = *threadInfo[curThread]; 424     SimpleThread* thread = t_info.thread; 425 426     Fault fault; 427     const int asid = 0; 428     const Addr pc = thread-&gt;instAddr(); 429     unsigned block_size = cacheLineSize(); 430     BaseTLB::Mode mode = BaseTLB::Read; 431 432     if (traceData) 433         traceData-&gt;setMem(addr, size, flags); 434 435     RequestPtr req = std::make_shared&lt;Request&gt;( 436         asid, addr, size, flags, dataMasterId(), pc, 437         thread-&gt;contextId()); 438     if (!byte_enable.empty()) { 439         req-&gt;setByteEnable(byte_enable); 440     } 441 442     req-&gt;taskId(taskId()); 443 444     Addr split_addr = roundDown(addr + size - 1, block_size); 445     assert(split_addr &lt;= addr || split_addr - addr &lt; block_size); 446 447     _status = DTBWaitResponse; 448     if (split_addr &gt; addr) { 449         RequestPtr req1, req2; 450         assert(!req-&gt;isLLSC() &amp;&amp; !req-&gt;isSwap()); 451         req-&gt;splitOnVaddr(split_addr, req1, req2); 452 453         WholeTranslationState *state = 454             new WholeTranslationState(req, req1, req2, new uint8_t[size], 455                                       NULL, mode); 456         DataTranslation&lt;TimingSimpleCPU *&gt; *trans1 = 457             new DataTranslation&lt;TimingSimpleCPU *&gt;(this, state, 0); 458         DataTranslation&lt;TimingSimpleCPU *&gt; *trans2 = 459             new DataTranslation&lt;TimingSimpleCPU *&gt;(this, state, 1); 460 461         thread-&gt;dtb-&gt;translateTiming(req1, thread-&gt;getTC(), trans1, mode); 462         thread-&gt;dtb-&gt;translateTiming(req2, thread-&gt;getTC(), trans2, mode); 463     } else { 464         WholeTranslationState *state = 465             new WholeTranslationState(req, new uint8_t[size], NULL, mode); 466         DataTranslation&lt;TimingSimpleCPU *&gt; *translation 467             = new DataTranslation&lt;TimingSimpleCPU *&gt;(this, state); 468         thread-&gt;dtb-&gt;translateTiming(req, thread-&gt;getTC(), translation, mode); 469     } 470 471     return NoFault; 472 }This function first handles split memory access that needs two memory access requests.When the memory address is not aligned, and the access crosses the memory block boundary,then it should be handled with two separate memory requests.Otherwise, it invokes translateTiming function defined in data tlb object(dtb).Note that initiateMemRead doesn’t actually bring the data from the memory to cache.It first check the tlb for virtual to physical mapping, and if the mapping doesn’t exist,it initiate translation request to TLBcompleteAcc: execute memory instruction and bring the data138 def template MicroLoadCompleteAcc {{139     Fault %(class_name)s::completeAcc(PacketPtr pkt, ExecContext * xc,140                                       Trace::InstRecord * traceData) const141     {142         Fault fault = NoFault;143144         %(op_decl)s;145         %(op_rd)s;146147         getMem(pkt, Mem, dataSize, traceData);148149         %(code)s;150151         if(fault == NoFault)152         {153             %(op_wb)s;154         }155156         return fault;157     }158 }};19167     Fault Ld::completeAcc(PacketPtr pkt, ExecContext * xc,19168                                       Trace::InstRecord * traceData) const19169     {19170         Fault fault = NoFault;1917119172         uint64_t Data = 0;19173 uint64_t Mem;19174 ;19175         Data = xc-&gt;readIntRegOperand(this, 2);19176 ;1917719178         getMem(pkt, Mem, dataSize, traceData);1917919180         Data = merge(Data, Mem, dataSize);;1918119182         if(fault == NoFault)19183         {191841918519186         {19187             uint64_t final_val = Data;19188             xc-&gt;setIntRegOperand(this, 0, final_val);1918919190             if (traceData) { traceData-&gt;setData(final_val); }19191         };19192         }1919319194         return fault;19195     }completeAcc function receives the pkt as its parameter.pkt contains the actual data read from the memory,so getMem function reads the proper amount of the data from the pkt data structure.Because memory operation reads 64bytes of data at once it should be properly feed to the pipeline depending on the data read size.###execute###preExecute: decode instruction and predict branchInstruction execution: execute decoded microop instructionFor memory operation (line 798-819),it invokes initiateAcc method of current microop represented by the curStaticInst (line 800).As we have seen before,for each load/store microop, it defines initiateAcc function,for example, for Ld, it defines Ld::initiateAcc.Otherwise, for non-memory instruction,it invokes execute method of microop instead of initiateAcc.###postExecute: manage statistics related with execution###adnvanceInst: start to fetch next instructionAfter finishing translation,and when the fault has not been deteced by the finish function,it starts to read the actual data from the memory. 627 void 628 TimingSimpleCPU::finishTranslation(WholeTranslationState *state) 629 { 630     _status = BaseSimpleCPU::Running; 631 632     if (state-&gt;getFault() != NoFault) { 633         if (state-&gt;isPrefetch()) { 634             state-&gt;setNoFault(); 635         } 636         delete [] state-&gt;data; 637         state-&gt;deleteReqs(); 638         translationFault(state-&gt;getFault()); 639     } else { 640         if (!state-&gt;isSplit) { 641             sendData(state-&gt;mainReq, state-&gt;data, state-&gt;res, 642                      state-&gt;mode == BaseTLB::Read); 643         } else { 644             sendSplitData(state-&gt;sreqLow, state-&gt;sreqHigh, state-&gt;mainReq, 645                           state-&gt;data, state-&gt;mode == BaseTLB::Read); 646         } 647     } 648 649     delete state; 650 }As shown in line 639-649, when the fault has not been raised during translation,then it sends memory access packet to DRAM through sendData function. 287 void 288 TimingSimpleCPU::sendData(const RequestPtr &amp;req, uint8_t *data, uint64_t *res, 289                           bool read) 290 { 291     SimpleExecContext &amp;t_info = *threadInfo[curThread]; 292     SimpleThread* thread = t_info.thread; 293 294     PacketPtr pkt = buildPacket(req, read); 295     pkt-&gt;dataDynamic&lt;uint8_t&gt;(data); 296 297     if (req-&gt;getFlags().isSet(Request::NO_ACCESS)) { 298         assert(!dcache_pkt); 299         pkt-&gt;makeResponse(); 300         completeDataAccess(pkt); 301     } else if (read) { 302         handleReadPacket(pkt); 303     } else { 304         bool do_access = true;  // flag to suppress cache access 305 306         if (req-&gt;isLLSC()) { 307             do_access = TheISA::handleLockedWrite(thread, req, dcachePort.cacheBlockMask); 308         } else if (req-&gt;isCondSwap()) { 309             assert(res); 310             req-&gt;setExtraData(*res); 311         } 312 313         if (do_access) { 314             dcache_pkt = pkt; 315             handleWritePacket(); 316             threadSnoop(pkt, curThread); 317         } else { 318             _status = DcacheWaitResponse; 319             completeDataAccess(pkt); 320         } 321     } 322 }Currently we are looking at load instruction not the store,we are going to assume that read flag has been set.Therefore, it invoked handleReadPacket(pkt) function in line 301-302.Note that packer pkk is created as a combination of req and read(line 294).As req variable contains all the required address and data size to access memory, it should be contained in the request packet. 258 bool 259 TimingSimpleCPU::handleReadPacket(PacketPtr pkt) 260 { 261     SimpleExecContext &amp;t_info = *threadInfo[curThread]; 262     SimpleThread* thread = t_info.thread; 263 264     const RequestPtr &amp;req = pkt-&gt;req; 265 266     // We're about the issues a locked load, so tell the monitor 267     // to start caring about this address 268     if (pkt-&gt;isRead() &amp;&amp; pkt-&gt;req-&gt;isLLSC()) { 269         TheISA::handleLockedRead(thread, pkt-&gt;req); 270     } 271     if (req-&gt;isMmappedIpr()) { 272         Cycles delay = TheISA::handleIprRead(thread-&gt;getTC(), pkt); 273         new IprEvent(pkt, this, clockEdge(delay)); 274         _status = DcacheWaitResponse; 275         dcache_pkt = NULL; 276     } else if (!dcachePort.sendTimingReq(pkt)) { 277         _status = DcacheRetry; 278         dcache_pkt = pkt; 279     } else { 280         _status = DcacheWaitResponse; 281         // memory system takes ownership of packet 282         dcache_pkt = NULL; 283     } 284     return dcache_pkt == NULL; 285 }Because CPU is connected to memory component through master slave ports in GEM5,it can initiate memory access by sending request packet through a sendTimingReq method.Because CPU goes through the data cache before touching the physical memory, the sendTimingReq is invoked on the DcachePort.gem5/src/mem/port.hh444 inline bool445 MasterPort::sendTimingReq(PacketPtr pkt)446 {447     return TimingRequestProtocol::sendReq(_slavePort, pkt);448 }mem/protocol/timing.cc 47 /* The request protocol. */ 48  49 bool 50 TimingRequestProtocol::sendReq(TimingResponseProtocol *peer, PacketPtr pkt) 51 { 52     assert(pkt-&gt;isRequest()); 53     return peer-&gt;recvTimingReq(pkt); 54 }recvTimingRespWhen the request has been handled by the slave (DCache),recvTimingResp method of DcachePort will be invoked to handle result of memory access. 978 bool 979 TimingSimpleCPU::DcachePort::recvTimingResp(PacketPtr pkt) 980 { 981     DPRINTF(SimpleCPU, \"Received load/store response %#x\\n\", pkt-&gt;getAddr()); 982 983     // The timing CPU is not really ticked, instead it relies on the 984     // memory system (fetch and load/store) to set the pace. 985     if (!tickEvent.scheduled()) { 986         // Delay processing of returned data until next CPU clock edge 987         tickEvent.schedule(pkt, cpu-&gt;clockEdge()); 988         return true; 989     } else { 990         // In the case of a split transaction and a cache that is 991         // faster than a CPU we could get two responses in the 992         // same tick, delay the second one 993         if (!retryRespEvent.scheduled()) 994             cpu-&gt;schedule(retryRespEvent, cpu-&gt;clockEdge(Cycles(1))); 995         return false; 996     } 997 }It seems that it doesn’t handle the received packet.However, it schedules tickEventto process the recevied packet. 999 void1000 TimingSimpleCPU::DcachePort::DTickEvent::process()1001 {1002     cpu-&gt;completeDataAccess(pkt);1003 }"
  },
  
  {
    "title": "Gem5 X86 Tlb",
    "url": "/posts/gem5-x86-tlb/",
    "categories": "",
    "tags": "",
    "date": "2020-06-03 00:00:00 -0400",
    





    
    "snippet": "layout: posttittle: “Pagetable walking and pagefault handling in Gem5”categories: GEM5, TLB—In this posting, we are going to take a look at how the memory accessescan be resolved through the TLB an...",
    "content": "layout: posttittle: “Pagetable walking and pagefault handling in Gem5”categories: GEM5, TLB—In this posting, we are going to take a look at how the memory accessescan be resolved through the TLB and pagetable walking.Who initiates TLB access?TLB maintains a virtual to physical address translation informationto reduce time of walking the entire page table at every memory access.In other words, it is a cache of virtual to physical mapping maintained by the processor usually. Then which part of the CPU logic initiates the TLB logic, and what operations should be done by the TLB component?Interface between CPU pipeline and TLB component 425 Fault 426 TimingSimpleCPU::initiateMemRead(Addr addr, unsigned size, 427                                  Request::Flags flags, 428                                  const std::vector&lt;bool&gt;&amp; byte_enable) 429 { 430     SimpleExecContext &amp;t_info = *threadInfo[curThread]; 431     SimpleThread* thread = t_info.thread; 432  439     Fault fault; 440     const int asid = 0; 441     const Addr pc = thread-&gt;instAddr(); 442     unsigned block_size = cacheLineSize(); 443     BaseTLB::Mode mode = BaseTLB::Read; 444  445     if (traceData) 446         traceData-&gt;setMem(addr, size, flags); 447  448     RequestPtr req = std::make_shared&lt;Request&gt;( 449         asid, addr, size, flags, dataMasterId(), pc, 450         thread-&gt;contextId()); 451     if (!byte_enable.empty()) { 452         req-&gt;setByteEnable(byte_enable); 453     } 454     455     req-&gt;taskId(taskId()); 456  457     Addr split_addr = roundDown(addr + size - 1, block_size); 458     assert(split_addr &lt;= addr || split_addr - addr &lt; block_size); 459                                   460     _status = DTBWaitResponse; 461     if (split_addr &gt; addr) { 462         RequestPtr req1, req2; 463         assert(!req-&gt;isLLSC() &amp;&amp; !req-&gt;isSwap()); 464         req-&gt;splitOnVaddr(split_addr, req1, req2); 465     466         WholeTranslationState *state = 467             new WholeTranslationState(req, req1, req2, new uint8_t[size], 468                                       NULL, mode); 469         DataTranslation&lt;TimingSimpleCPU *&gt; *trans1 = 470             new DataTranslation&lt;TimingSimpleCPU *&gt;(this, state, 0); 471         DataTranslation&lt;TimingSimpleCPU *&gt; *trans2 = 472             new DataTranslation&lt;TimingSimpleCPU *&gt;(this, state, 1); 473  474         thread-&gt;dtb-&gt;translateTiming(req1, thread-&gt;getTC(), trans1, mode); 475         thread-&gt;dtb-&gt;translateTiming(req2, thread-&gt;getTC(), trans2, mode); 476     } else { 477         WholeTranslationState *state = 478             new WholeTranslationState(req, new uint8_t[size], NULL, mode); 479         DataTranslation&lt;TimingSimpleCPU *&gt; *translation 480             = new DataTranslation&lt;TimingSimpleCPU *&gt;(this, state); 481         thread-&gt;dtb-&gt;translateTiming(req, thread-&gt;getTC(), translation, mode); 482     } 483  484     return NoFault; 485 }One of the most important basic capability of processor is accessing memory.GEM5 make each processor implement their own memory access building blocks as member function of each processor class. We are going to take a look at simple processor, TimingSimpleCPU and corresponding memory function, initiateMemRead. Note that at the end of the initiateMemRead function, it generates DataTranslation object and pass it to the translateTiming function defined in the data TLB component of the processor. This translation object will be used to process current TLB access request.Also note that translateTiming function needs threadContext to execute TLB accessing and RequestPtr object containing all the memory access request information such as virtual address.It’s all about TLB! No actual memory access to the virtual address!  initiateMemRead function does not initiate actual memory access, it only asks TLB component to generate virtual address to physical address mapping in its TLB cache.It could be confusing because of its name initateMemRead but the actual memory access could only be occured after the TLB request can be successfully resolved. I will describe how actual memory access happens in this posting []Keep in mind that we will only focus on the translation part!//gem5/src/arch/x86/tlb.cc441 void442 TLB::translateTiming(const RequestPtr &amp;req, ThreadContext *tc,443         Translation *translation, Mode mode)444 {445     bool delayedResponse;446     assert(translation);447     Fault fault =448         TLB::translate(req, tc, translation, mode, delayedResponse, true);449450     if (!delayedResponse)451         translation-&gt;finish(fault, req, tc, mode);452     else453         translation-&gt;markDelayed();454 }As we assume that GEM5 is compiled for X86 architecture, it will invoke TLB implementation for X86 architecture. Please be aware that the translateTiming function is implemented as part of theTLB class, indicating that we are presently working with TLB components, transitioning away from the processor pipeline. The actual translation is done by TLB::translate function. Depending on whether the target virtual address has previously been resolved andits mapping cached in the TLB or not, the function can either retrieve the TLB entry from the cache or obtain it by traversing the page table.// gem5/src/arch/x86/tlb.cc277 Fault278 TLB::translate(const RequestPtr &amp;req,279         ThreadContext *tc, Translation *translation,280         Mode mode, bool &amp;delayedResponse, bool timing)281 {282     Request::Flags flags = req-&gt;getFlags();283     int seg = flags &amp; SegmentFlagMask;284     bool storeCheck = flags &amp; (StoreCheck &lt;&lt; FlagShift);...341         // If paging is enabled, do the translation.342         if (m5Reg.paging) {343             DPRINTF(TLB, \"Paging enabled.\\n\");344             // The vaddr already has the segment base applied.345             TlbEntry *entry = lookup(vaddr);346             if (mode == Read) {347                 rdAccesses++;348             } else {349                 wrAccesses++;350             }351             if (!entry) {352                 DPRINTF(TLB, \"Handling a TLB miss for \"353                         \"address %#x at pc %#x.\\n\",354                         vaddr, tc-&gt;instAddr());355                 if (mode == Read) {356                     rdMisses++;357                 } else {358                     wrMisses++;359                 }360                 if (FullSystem) {361                     Fault fault = walker-&gt;start(tc, translation, req, mode);362                     if (timing || fault != NoFault) {363                         // This gets ignored in atomic mode.364                         delayedResponse = true;365                         return fault;366                     }367                     entry = lookup(vaddr);368                     assert(entry);369                 } else {The initial step in the translate function involves a query to the TLB, inquiring whether the necessary translation entry is present in the TLB (line345). In cases where the TLB entry is absent, the process then proceeds to navigate through the page table, which is stored in memory, in order to acquire the virtual-to-physical translation (spanning from line 351 to 395). Given the presumed interest in utilizing full-system emulation, I will focus on FullSystemparts of TLB handling. In GEM5’s fullsystem mode, when a TLB miss occurs, the system proceeds to navigate the page table using the “pagetable_walker” object (as indicated in line 361). It’s important to note that the “req” parameter is passed to the pagetable_walker because it contains all the essential information, including the address and flags, necessary for correctly resolving memory access.Page table walking in TLBIn cases where it is either the first request or the previous TLB entry has beenevicted from the TLB cache, it is required to traverse the page table and obtainthe virtual to physical mapping. Let’s examine the process by which the TLB effectively navigates the page table and retrieves the final-level page table entry.WalkerState per request  In contrast to simpler operations, it’s typically not possible to resolve TLBmisses in a single cycle.As the page table is structured with multiple levels, the page table walking demands numerous memory accesses. These accesses are essential for reaching the leaf page table entry that contains the virtual-to-physical mapping and other pertinent flags.//gem5/src/arch/x86/pagetable_walker.cc 71 Fault 72 Walker::start(ThreadContext * _tc, BaseTLB::Translation *_translation, 73               const RequestPtr &amp;_req, BaseTLB::Mode _mode) 74 { 75     // TODO: in timing mode, instead of blocking when there are other 76     // outstanding requests, see if this request can be coalesced with 77     // another one (i.e. either coalesce or start walk) 78     WalkerState * newState = new WalkerState(this, _translation, _req); 79     newState-&gt;initState(_tc, _mode, sys-&gt;isTimingMode()); 80     if (currStates.size()) { 81         assert(newState-&gt;isTiming()); 82         DPRINTF(PageTableWalker, \"Walks in progress: %d\\n\", currStates.size()); 83         currStates.push_back(newState); 84         return NoFault; 85     } else { 86         currStates.push_back(newState); 87         Fault fault = newState-&gt;startWalk(); 88         if (!newState-&gt;isTiming()) { 89             currStates.pop_front(); 90             delete newState; 91         } 92         return fault; 93     } 94 }It is important to note that TLB misses can occur simultaneously because multiple processors might try to access a memory address for which the virtual-to-physical mapping is not stored in the TLB cache. Additional, since each request cannot be handled in a single clock cycle, there is a need to store the state of page table walking for each request. The “walkerState” is employed for this specific purpose, maintaining all the necessary information for page table walking on a per-request basis.The “currStates” keeps track of all the outstanding requests, which are those that have been requested previously but have not yet been resolved, in the form of a list. If there are any unresolved TLB misses, the current request is simply added to the list, and the system waits until the preceding requests have beenresolved, as seen in lines 80-84. Once the outstanding request has been resolved,the pending requests are then processed one after another.If there is no remaining requests in the list, as indicated in lines 85-92, a newly generated state should be added, and the “startWalk” function is called with the newly created state. Upon completion of the page table walking by the “startWalk” function, in the case of a timing CPU, there is no need to remove the current state from the “currStates” list, as another stage in the timing CPUmodel takes care of removing the current state from the list.startWalk, initiating page table walkinggem5/src/arch/x86/pagetable_walker.cc229 Fault230 Walker::WalkerState::startWalk()231 {232     Fault fault = NoFault;233     assert(!started);234     started = true;235     setupWalk(req-&gt;getVaddr());236     if (timing) {237         nextState = state;238         state = Waiting;239         timingFault = NoFault;240         sendPackets();241     } else {242         do {243             walker-&gt;port.sendAtomic(read);244             PacketPtr write = NULL;245             fault = stepWalk(write);246             assert(fault == NoFault || read == NULL);247             state = nextState;248             nextState = Ready;249             if (write)250                 walker-&gt;port.sendAtomic(write);251         } while (read);252         state = Ready;253         nextState = Waiting;254     }255     return fault;256 }Since the page table is stored in memory or cache, whenever the TLB miss happensit should retrieve page table content from the memory subsystem. To this end, TLB component initiates memory request through sendPackets function.multi-level page table walking process = multiple packets661 void662 Walker::WalkerState::sendPackets()663 {664     //If we're already waiting for the port to become available, just return.665     if (retrying)666         return;667668     //Reads always have priority669     if (read) {670         PacketPtr pkt = read;671         read = NULL;672         inflight++;673         if (!walker-&gt;sendTiming(this, pkt)) {674             retrying = true;675             read = pkt;676             inflight--;677             return;678         }679     }680     //Send off as many of the writes as we can.681     while (writes.size()) {682         PacketPtr write = writes.back();683         writes.pop_back();684         inflight++;685         if (!walker-&gt;sendTiming(this, write)) {686             retrying = true;687             writes.push_back(write);688             inflight--;689             return;690         }691     }692 }With modern processors making use of multi-level page tables, it becomes challenging to pre-determine which page table entries will be accessed before resolving the memory access at the previous level of page table entry. Because of this interdependence among page table access, the accesses to these entries must be carried out sequentially rather than in parallelConsequently, TLB accesses are structured into multiple stages, with each stage responsible for accessing one level of the page table.Since TLB should request memory subsystem to fetch next level of page table entry one by one, it should send send different packets at different stage to access a specific level of the page table.When you look at the “sendPackets” function, you will notice a familiar functionname, “sendTiming,” which dispatches page-table-access-request-packets to the memory subsystem (e.g., cache or memory).Initial page table access packet creationWhen you take a look at the “sendPackets” function, you won’t observe any packetcreation within it. However, you will notice that the “sendTiming” function receives a parameter named pkt. So, where does this pkt come from? The “setupWalk” function within the “startWalk” function is responsible forpopulating the appropriate request packet, which initiates the access to the page table.551 void552 Walker::WalkerState::setupWalk(Addr vaddr)553 {554     VAddr addr = vaddr;555     CR3 cr3 = tc-&gt;readMiscRegNoEffect(MISCREG_CR3);556     // Check if we're in long mode or not557     Efer efer = tc-&gt;readMiscRegNoEffect(MISCREG_EFER);558     dataSize = 8;559     Addr topAddr;560     if (efer.lma) {561         // Do long mode.562         state = LongPML4;563         topAddr = (cr3.longPdtb &lt;&lt; 12) + addr.longl4 * dataSize;564         enableNX = efer.nxe;565     } else {566         // We're in some flavor of legacy mode.567         CR4 cr4 = tc-&gt;readMiscRegNoEffect(MISCREG_CR4);568         if (cr4.pae) {569             // Do legacy PAE.570             state = PAEPDP;571             topAddr = (cr3.paePdtb &lt;&lt; 5) + addr.pael3 * dataSize;572             enableNX = efer.nxe;573         } else {574             dataSize = 4;575             topAddr = (cr3.pdtb &lt;&lt; 12) + addr.norml2 * dataSize;576             if (cr4.pse) {577                 // Do legacy PSE.578                 state = PSEPD;579             } else {580                 // Do legacy non PSE.581                 state = PD;582             }583             enableNX = false;584         }585     }586587     nextState = Ready;588     entry.vaddr = vaddr;589590     Request::Flags flags = Request::PHYSICAL;591     if (cr3.pcd)592         flags.set(Request::UNCACHEABLE);593594     RequestPtr request = std::make_shared&lt;Request&gt;(595         topAddr, dataSize, flags, walker-&gt;masterId);596597     read = new Packet(request, MemCmd::ReadReq);598     read-&gt;allocate();599 }We’ve learned that the “sendPackets” function is employed to transmit multiple page table access requests, depending on the various stages of the page table walking process. So, how are the packets for the subsequent stages created and provided to the “sendPackets” function? Please bear with me as we progress through one complete step of page table walking; I will address this aspect shortly.SendTiming function: sends request and save current stateNow, let’s explore how the “sendTiming” function transmits the generated page table access request packet to the memory subsystem via the designated port.156 bool Walker::sendTiming(WalkerState* sendingState, PacketPtr pkt)157 {158     WalkerSenderState* walker_state = new WalkerSenderState(sendingState);159     pkt-&gt;pushSenderState(walker_state);160     if (port.sendTimingReq(pkt)) {161         return true;162     } else {163         // undo the adding of the sender state and delete it, as we164         // will do it again the next time we attempt to send it165         pkt-&gt;popSenderState();166         delete walker_state;167         return false;168     }169170 }It’s worth noting that the “sendTiming” function initially generates a separate state called “WalkerSenderState.” This state variable is essential for handling the requested page table access and for processing the response from the memorysubsystem once the page table access has been completed.Handling return packet from memory sub-systemWhen memory sub-system successfully handled the page table access request,pagetable_walker receives the result packet through the port.When the packet arrives to the portconnecting pagetable_walker and memory sub-system,it invokes recvTimingResp function of the walker.104 bool105 Walker::WalkerPort::recvTimingResp(PacketPtr pkt)106 {107     return walker-&gt;recvTimingResp(pkt);108 }109110 bool111 Walker::recvTimingResp(PacketPtr pkt)112 {113     WalkerSenderState * senderState =114         dynamic_cast&lt;WalkerSenderState *&gt;(pkt-&gt;popSenderState());115     WalkerState * senderWalk = senderState-&gt;senderWalk;116     bool walkComplete = senderWalk-&gt;recvPacket(pkt);117     delete senderState;118     if (walkComplete) {119         std::list&lt;WalkerState *&gt;::iterator iter;120         for (iter = currStates.begin(); iter != currStates.end(); iter++) {121             WalkerState * walkerState = *(iter);122             if (walkerState == senderWalk) {123                 iter = currStates.erase(iter);124                 break;125             }126         }127         delete senderWalk;128         // Since we block requests when another is outstanding, we129         // need to check if there is a waiting request to be serviced130         if (currStates.size() &amp;&amp; !startWalkWrapperEvent.scheduled())131             // delay sending any new requests until we are finished132             // with the responses133             schedule(startWalkWrapperEvent, clockEdge());134     }135     return true;136 }As we’ve seen before,WalkerSenderState wraps up the walker instance (WalkerState) which has been used to send pagetable access request associated with currently received packet.recvPacket handles received packet and send another packet for next stage pagetable accessRetrieved WalkerState instance handles the received packetby calling recvPacket function of the WalkerState.602 bool603 Walker::WalkerState::recvPacket(PacketPtr pkt)604 {605     assert(pkt-&gt;isResponse());606     assert(inflight);607     assert(state == Waiting);608     inflight--;609     if (squashed) {610         // if were were squashed, return true once inflight is zero and611         // this WalkerState will be freed there.612         return (inflight == 0);613     }614     if (pkt-&gt;isRead()) {615         // should not have a pending read it we also had one outstanding616         assert(!read);617618         // @todo someone should pay for this619         pkt-&gt;headerDelay = pkt-&gt;payloadDelay = 0;620621         state = nextState;622         nextState = Ready;623         PacketPtr write = NULL;624         read = pkt;625         timingFault = stepWalk(write);626         state = Waiting;627         assert(timingFault == NoFault || read == NULL);628         if (write) {629             writes.push_back(write);630         }631         sendPackets();632     } else {633         sendPackets();634     }635     if (inflight == 0 &amp;&amp; read == NULL &amp;&amp; writes.size() == 0) {636         state = Ready;637         nextState = Waiting;638         if (timingFault == NoFault) {639             /*640              * Finish the translation. Now that we know the right entry is641              * in the TLB, this should work with no memory accesses.642              * There could be new faults unrelated to the table walk like643              * permissions violations, so we'll need the return value as644              * well.645              */646             bool delayedResponse;647             Fault fault = walker-&gt;tlb-&gt;translate(req, tc, NULL, mode,648                                                  delayedResponse, true);649             assert(!delayedResponse);650             // Let the CPU continue.651             translation-&gt;finish(fault, req, tc, mode);652         } else {653             // There was a fault during the walk. Let the CPU know.654             translation-&gt;finish(timingFault, req, tc, mode);655         }656         return true;657     }658659     return false;660 }Because the recvPacket function has been invoked as a result of memory read (initial pagetable access)614-634 will be executed. There are some functions that we don’t know,but it finally invokes sendPackets function again.Wait why sendPackets once again in receive function?Remember! Page table walking is not a single memory accessNote that we are currently dealing with the result packet from the memory sub-system as a result of sending initial pagetable access request (accessing first level of pagetable)Therefore, the received packet should containnext level page table information not the Page table entry which actually containsphysical address to virtual address mapping. Therefore, to acquire the last level page table entry,it needs additional memory accesses to the sub levels of pagetables,which should requires another sendPackets.Preparing packets for the next pagetable access requestsAs we generated initiating packet with the help of setupWalk,packets required for accessing further page table layers are prepared by stepWalk function.282 Fault283 Walker::WalkerState::stepWalk(PacketPtr &amp;write)284 {285     assert(state != Ready &amp;&amp; state != Waiting);286     Fault fault = NoFault;287     write = NULL;288     PageTableEntry pte;289     if (dataSize == 8)290         pte = read-&gt;getLE&lt;uint64_t&gt;();291     else292         pte = read-&gt;getLE&lt;uint32_t&gt;();293     VAddr vaddr = entry.vaddr;294     bool uncacheable = pte.pcd;295     Addr nextRead = 0;296     bool doWrite = false;297     bool doTLBInsert = false;298     bool doEndWalk = false;299     bool badNX = pte.nx &amp;&amp; mode == BaseTLB::Execute &amp;&amp; enableNX;300     switch(state) {301       case LongPML4:302         DPRINTF(PageTableWalker,303                 \"Got long mode PML4 entry %#016x.\\n\", (uint64_t)pte);304         nextRead = ((uint64_t)pte &amp; (mask(40) &lt;&lt; 12)) + vaddr.longl3 * dataSize;305         doWrite = !pte.a;306         pte.a = 1;307         entry.writable = pte.w;308         entry.user = pte.u;309         if (badNX || !pte.p) {310             doEndWalk = true;311             fault = pageFault(pte.p);312             break;313         }314         entry.noExec = pte.nx;315         nextState = LongPDP;316         break;317       case LongPDP:318         DPRINTF(PageTableWalker,319                 \"Got long mode PDP entry %#016x.\\n\", (uint64_t)pte);320         nextRead = ((uint64_t)pte &amp; (mask(40) &lt;&lt; 12)) + vaddr.longl2 * dataSize;321         doWrite = !pte.a;322         pte.a = 1;323         entry.writable = entry.writable &amp;&amp; pte.w;324         entry.user = entry.user &amp;&amp; pte.u;325         if (badNX || !pte.p) {326             doEndWalk = true;327             fault = pageFault(pte.p);328             break;329         }330         nextState = LongPD;331         break;332       case LongPD:333         DPRINTF(PageTableWalker,334                 \"Got long mode PD entry %#016x.\\n\", (uint64_t)pte);335         doWrite = !pte.a;336         pte.a = 1;337         entry.writable = entry.writable &amp;&amp; pte.w;338         entry.user = entry.user &amp;&amp; pte.u;339         if (badNX || !pte.p) {340             doEndWalk = true;341             fault = pageFault(pte.p);342             break;343         }344         if (!pte.ps) {345             // 4 KB page346             entry.logBytes = 12;347             nextRead =348                 ((uint64_t)pte &amp; (mask(40) &lt;&lt; 12)) + vaddr.longl1 * dataSize;349             nextState = LongPTE;350             break;351         } else {352             // 2 MB page353             entry.logBytes = 21;354             entry.paddr = (uint64_t)pte &amp; (mask(31) &lt;&lt; 21);355             entry.uncacheable = uncacheable;356             entry.global = pte.g;357             entry.patBit = bits(pte, 12);358             entry.vaddr = entry.vaddr &amp; ~((2 * (1 &lt;&lt; 20)) - 1);359             doTLBInsert = true;360             doEndWalk = true;361             break;362         }363       case LongPTE:364         DPRINTF(PageTableWalker,365                 \"Got long mode PTE entry %#016x.\\n\", (uint64_t)pte);366         doWrite = !pte.a;367         pte.a = 1;368         entry.writable = entry.writable &amp;&amp; pte.w;369         entry.user = entry.user &amp;&amp; pte.u;370         if (badNX || !pte.p) {371             doEndWalk = true;372             fault = pageFault(pte.p);373             break;374         }375         entry.paddr = (uint64_t)pte &amp; (mask(40) &lt;&lt; 12);376         entry.uncacheable = uncacheable;377         entry.global = pte.g;378         entry.patBit = bits(pte, 12);379         entry.vaddr = entry.vaddr &amp; ~((4 * (1 &lt;&lt; 10)) - 1);380         doTLBInsert = true;381         doEndWalk = true;382         break;383       case PAEPDP:384         DPRINTF(PageTableWalker,385                 \"Got legacy mode PAE PDP entry %#08x.\\n\", (uint32_t)pte);386         nextRead = ((uint64_t)pte &amp; (mask(40) &lt;&lt; 12)) + vaddr.pael2 * dataSize;387         if (!pte.p) {388             doEndWalk = true;389             fault = pageFault(pte.p);390             break;391         }392         nextState = PAEPD;393         break;394       case PAEPD:395         DPRINTF(PageTableWalker,396                 \"Got legacy mode PAE PD entry %#08x.\\n\", (uint32_t)pte);397         doWrite = !pte.a;398         pte.a = 1;399         entry.writable = pte.w;400         entry.user = pte.u;401         if (badNX || !pte.p) {402             doEndWalk = true;403             fault = pageFault(pte.p);404             break;405         }406         if (!pte.ps) {407             // 4 KB page408             entry.logBytes = 12;409             nextRead = ((uint64_t)pte &amp; (mask(40) &lt;&lt; 12)) + vaddr.pael1 * dataSize;410             nextState = PAEPTE;411             break;412         } else {413             // 2 MB page414             entry.logBytes = 21;415             entry.paddr = (uint64_t)pte &amp; (mask(31) &lt;&lt; 21);416             entry.uncacheable = uncacheable;417             entry.global = pte.g;418             entry.patBit = bits(pte, 12);419             entry.vaddr = entry.vaddr &amp; ~((2 * (1 &lt;&lt; 20)) - 1);420             doTLBInsert = true;421             doEndWalk = true;422             break;443         break;444       case PSEPD:                                                                                                                       445         DPRINTF(PageTableWalker,446                 \"Got legacy mode PSE PD entry %#08x.\\n\", (uint32_t)pte);447         doWrite = !pte.a;448         pte.a = 1;449         entry.writable = pte.w;450         entry.user = pte.u;451         if (!pte.p) {452             doEndWalk = true;453             fault = pageFault(pte.p);454             break;455         }456         if (!pte.ps) {457             // 4 KB page458             entry.logBytes = 12;459             nextRead =460                 ((uint64_t)pte &amp; (mask(20) &lt;&lt; 12)) + vaddr.norml2 * dataSize;461             nextState = PTE;462             break;463         } else {464             // 4 MB page465             entry.logBytes = 21;466             entry.paddr = bits(pte, 20, 13) &lt;&lt; 32 | bits(pte, 31, 22) &lt;&lt; 22;467             entry.uncacheable = uncacheable;468             entry.global = pte.g;469             entry.patBit = bits(pte, 12);470             entry.vaddr = entry.vaddr &amp; ~((4 * (1 &lt;&lt; 20)) - 1);471             doTLBInsert = true;472             doEndWalk = true;473             break;474         }475       case PD:476         DPRINTF(PageTableWalker,477                 \"Got legacy mode PD entry %#08x.\\n\", (uint32_t)pte);478         doWrite = !pte.a;479         pte.a = 1;480         entry.writable = pte.w;481         entry.user = pte.u;482         if (!pte.p) {483             doEndWalk = true;484             fault = pageFault(pte.p);485             break;486         }487         // 4 KB page488         entry.logBytes = 12;489         nextRead = ((uint64_t)pte &amp; (mask(20) &lt;&lt; 12)) + vaddr.norml2 * dataSize;490         nextState = PTE;491         break;492       case PTE:493         DPRINTF(PageTableWalker,494                 \"Got legacy mode PTE entry %#08x.\\n\", (uint32_t)pte);495         doWrite = !pte.a;496         pte.a = 1;497         entry.writable = pte.w;498         entry.user = pte.u;499         if (!pte.p) {500             doEndWalk = true;501             fault = pageFault(pte.p);502             break;503         }504         entry.paddr = (uint64_t)pte &amp; (mask(20) &lt;&lt; 12);505         entry.uncacheable = uncacheable;506         entry.global = pte.g;507         entry.patBit = bits(pte, 7);508         entry.vaddr = entry.vaddr &amp; ~((4 * (1 &lt;&lt; 10)) - 1);509         doTLBInsert = true;510         doEndWalk = true;511         break;512       default:513         panic(\"Unknown page table walker state %d!\\n\");514     }515     if (doEndWalk) {516         if (doTLBInsert)517             if (!functional)518                 walker-&gt;tlb-&gt;insert(entry.vaddr, entry, tc);519         endWalk();520     } else {521         PacketPtr oldRead = read;522         //If we didn't return, we're setting up another read.523         Request::Flags flags = oldRead-&gt;req-&gt;getFlags();524         flags.set(Request::UNCACHEABLE, uncacheable);525         RequestPtr request = std::make_shared&lt;Request&gt;(526             nextRead, oldRead-&gt;getSize(), flags, walker-&gt;masterId);527         read = new Packet(request, MemCmd::ReadReq);528         read-&gt;allocate();529         // If we need to write, adjust the read packet to write the modified530         // value back to memory.531         if (doWrite) {532             write = oldRead;533             write-&gt;setLE&lt;uint64_t&gt;(pte);534             write-&gt;cmd = MemCmd::WriteReq;535         } else {536             write = NULL;537             delete oldRead;538         }539     }540     return fault;541 }Even though it is very long,depending on the current staterepresenting a level of the page table accessed by the currently received packet,next level page table address and corresponding packet is populated.Because we are hereas a result of accessing PML4 (first level page table),line 301-316 will be executed and prepare the informationto access the next level pagetable. Note thatthe next state is set tothe next level of page table level, PDP.After setting fields associated with next page table level access,it generates another read packet (Line 520-539)to convey all the information required to access the next level pagetable. Note that the newly populated packet is assigned to the read field of the current WalkerState object.This read packet is used by the sendPackets function to access further pagetable layers.These sending and receiving steps are repeated until the final PTE is read.When PTE is read from the memory sub-system,it sets the doEndWalk flag and doTLBInsert flagWhen the flags are set,new TLB entry is inserted to the TLB module (line 515-520).Finish TLB translationAfter the translation has been finished,whether it ends up TLB hit, TLB miss and page table walking, or unexpected TLB fault,it invokes finish function through a translation object.gem5/src/arch/x86/tlb.cc441 void442 TLB::translateTiming(const RequestPtr &amp;req, ThreadContext *tc,443         Translation *translation, Mode mode)444 {445     bool delayedResponse;446     assert(translation);447     Fault fault =448         TLB::translate(req, tc, translation, mode, delayedResponse, true);449450     if (!delayedResponse)451         translation-&gt;finish(fault, req, tc, mode);452     else453         translation-&gt;markDelayed();454 }Translation objectWait, what is the translation object? We haven’t deal with it before.Let’s go back to initiateMemRead function againto understand what is the translation object.DataTranslation class and finish method 464         WholeTranslationState *state = 465             new WholeTranslationState(req, new uint8_t[size], NULL, mode); 466         DataTranslation&lt;TimingSimpleCPU *&gt; *translation 467             = new DataTranslation&lt;TimingSimpleCPU *&gt;(this, state); 468         thread-&gt;dtb-&gt;translateTiming(req, thread-&gt;getTC(), translation, mode);At line 464-468, we can find that it is an object of DataTranslation class.To find out implementation of finish function,let’s take a look at DataTranslation class.gem5/src/cpu/translation.hh208 /**209  * This class represents part of a data address translation.  All state for210  * the translation is held in WholeTranslationState (above).  Therefore this211  * class does not need to know whether the translation is split or not.  The212  * index variable determines this but is simply passed on to the state class.213  * When this part of the translation is completed, finish is called.  If the214  * translation state class indicate that the whole translation is complete215  * then the execution context is informed.216  */217 template &lt;class ExecContextPtr&gt;218 class DataTranslation : public BaseTLB::Translation219 {220   protected:221     ExecContextPtr xc;222     WholeTranslationState *state;223     int index;224225   public:226     DataTranslation(ExecContextPtr _xc, WholeTranslationState* _state)227         : xc(_xc), state(_state), index(0)228     {229     }230231     DataTranslation(ExecContextPtr _xc, WholeTranslationState* _state,232                     int _index)233         : xc(_xc), state(_state), index(_index)234     {235     }236237     /**238      * Signal the translation state that the translation has been delayed due239      * to a hw page table walk.  Split requests are transparently handled.240      */241     void242     markDelayed()243     {244         state-&gt;delay = true;245     }246247     /**248      * Finish this part of the translation and indicate that the whole249      * translation is complete if the state says so.250      */251     void252     finish(const Fault &amp;fault, const RequestPtr &amp;req, ThreadContext *tc,253            BaseTLB::Mode mode)254     {255         assert(state);256         assert(mode == state-&gt;mode);257         if (state-&gt;finish(fault, index)) {258             if (state-&gt;getFault() == NoFault) {259                 // Don't access the request if faulted (due to squash)260                 req-&gt;setTranslateLatency();261             }262             xc-&gt;finishTranslation(state);263         }264         delete this;265     }266267     bool268     squashed() const269     {270         return xc-&gt;isSquashed();271     }272 };We can find that finish function is implemented in the DataTranslation class.The finish function defined in DataTranslation classre-invokes another finish function through the state member field (line 257).Also after invoking finish function,it invokes finishTranslation methodof ThreadContextwhen Fault has been raised as a consequence of TLB processing.WholeTranslationState class object contains translation related infoWhen we look at the initiateMemRead function again,WholeTranslationState instance is passed to the DataTranslation constructor as a state parameter.Therefore, state-&gt;finish of the DataTranslation invokes WholeTranslationState::finish method. Note that WholeTranslationState contains actual requestused for accessing page table entry from tlb.gem5/src/cpu/translation.hh 51 /** 52  * This class captures the state of an address translation.  A translation 53  * can be split in two if the ISA supports it and the memory access crosses 54  * a page boundary.  In this case, this class is shared by two data 55  * translations (below).  Otherwise it is used by a single data translation 56  * class.  When each part of the translation is finished, the finish 57  * function is called which will indicate whether the whole translation is 58  * completed or not.  There are also functions for accessing parts of the 59  * translation state which deal with the possible split correctly. 60  */ 61 class WholeTranslationState 62 { 63   protected: 64     int outstanding; 65     Fault faults[2]; 66 67   public: 68     bool delay; 69     bool isSplit; 70     RequestPtr mainReq; 71     RequestPtr sreqLow; 72     RequestPtr sreqHigh; 73     uint8_t *data; 74     uint64_t *res; 75     BaseTLB::Mode mode; 76 77     /** 78      * Single translation state.  We set the number of outstanding 79      * translations to one and indicate that it is not split. 80      */ 81     WholeTranslationState(const RequestPtr &amp;_req, uint8_t *_data, 82                           uint64_t *_res, BaseTLB::Mode _mode) 83         : outstanding(1), delay(false), isSplit(false), mainReq(_req), 84           sreqLow(NULL), sreqHigh(NULL), data(_data), res(_res), mode(_mode) 85     { 86         faults[0] = faults[1] = NoFault; 87         assert(mode == BaseTLB::Read || mode == BaseTLB::Write); 88     } 89 90     /** 91      * Split translation state.  We copy all state into this class, set the 92      * number of outstanding translations to two and then mark this as a 93      * split translation. 94      */ 95     WholeTranslationState(const RequestPtr &amp;_req, const RequestPtr &amp;_sreqLow, 96                           const RequestPtr &amp;_sreqHigh, uint8_t *_data, 97                           uint64_t *_res, BaseTLB::Mode _mode) 98         : outstanding(2), delay(false), isSplit(true), mainReq(_req), 99           sreqLow(_sreqLow), sreqHigh(_sreqHigh), data(_data), res(_res),100           mode(_mode)101     {102         faults[0] = faults[1] = NoFault;103         assert(mode == BaseTLB::Read || mode == BaseTLB::Write);104     }105106     /**107      * Finish part of a translation.  If there is only one request then this108      * translation is completed.  If the request has been split in two then109      * the outstanding count determines whether the translation is complete.110      * In this case, flags from the split request are copied to the main111      * request to make it easier to access them later on.112      */113     bool114     finish(const Fault &amp;fault, int index)115     {116         assert(outstanding);117         faults[index] = fault;118         outstanding--;119         if (isSplit &amp;&amp; outstanding == 0) {120121             // For ease later, we copy some state to the main request.122             if (faults[0] == NoFault) {123                 mainReq-&gt;setPaddr(sreqLow-&gt;getPaddr());124             }125             mainReq-&gt;setFlags(sreqLow-&gt;getFlags());126             mainReq-&gt;setFlags(sreqHigh-&gt;getFlags());127         }128         return outstanding == 0;129     }the finish function of WholeTranslationStatestores generated fault on its internal bufferwhen meaningful fault has been raised in translation process(line 117).After the fault has been stored by WholeTranslationState’s finish function,remaining part of DataTranslation’s finish function invokesxc-&gt;finishTranslation(state) function.Note that finishTranslation function requiresWholeTranslationStation instance as state argument.To understand details, we have to look at what is the xc variable.Because DataTranslation class is declared as a template class, and xc is declared as template type,it will be an instance of template type class.Now the time of processor, not the TLBDataTranslation as an interface to interact with CPU 466         DataTranslation&lt;TimingSimpleCPU *&gt; *translation 467             = new DataTranslation&lt;TimingSimpleCPU *&gt;(this, state);Because the translation variable has been declared with *DataTranslation* type,xc variable is as an instance of *TimingSimpleCpu*.Therefore, when the xc-&gt;finishTranslation(state) is called,it will invoke TimingSimpleCPU::finishTranslation function.Note that we are jumping into the CPU codefrom the TLB module.What CPU has to do after the TLB finish its job 627 void 628 TimingSimpleCPU::finishTranslation(WholeTranslationState *state) 629 { 630     _status = BaseSimpleCPU::Running; 631 632     if (state-&gt;getFault() != NoFault) { 633         if (state-&gt;isPrefetch()) { 634             state-&gt;setNoFault(); 635         } 636         delete [] state-&gt;data; 637         state-&gt;deleteReqs(); 638         translationFault(state-&gt;getFault()); 639     } else { 640         if (!state-&gt;isSplit) { 641             sendData(state-&gt;mainReq, state-&gt;data, state-&gt;res, 642                      state-&gt;mode == BaseTLB::Read); 643         } else { 644             sendSplitData(state-&gt;sreqLow, state-&gt;sreqHigh, state-&gt;mainReq, 645                           state-&gt;data, state-&gt;mode == BaseTLB::Read); 646         } 647     } 648 649     delete state; 650 }When there exists translation fault,it ends up ivnoking translationFault function of the CPUwith a previously stored fault(line 638).Note that state-&gt;getFault method returns the faultpreviously stored by WholeTranslationState’s finish.When a translation has happendedbecause of prefetch instruction,it suppress generated fault because it is not critical for execution.However, when no fault has been encountered during the translation,it invokes sendData function.We will cover this later.Let CPU handle the TLB fault 361 void 362 TimingSimpleCPU::translationFault(const Fault &amp;fault) 363 { 364     // fault may be NoFault in cases where a fault is suppressed, 365     // for instance prefetches. 366     updateCycleCounts(); 367     updateCycleCounters(BaseCPU::CPU_STATE_ON); 368 369     if (traceData) { 370         // Since there was a fault, we shouldn't trace this instruction. 371         delete traceData; 372         traceData = NULL; 373     } 374 375     postExecute(); 376 377     advanceInst(fault); 378 }The translationFault function invokes postExecute and advanceInst function.By looking at the function argument,we can infer that advanceInst function actually deal with the fault.The postExecute function doesn’t invoke any meaningful function to proceed pipeline, butit updates stat of the processor such as power model, load instruction counter, etc.Therefore, let’s jump into the advanceInst function.advanceInst to process generated translation faultgem5/src/cpu/simple/timing.cc 734 void 735 TimingSimpleCPU::advanceInst(const Fault &amp;fault) 736 { 737     SimpleExecContext &amp;t_info = *threadInfo[curThread]; 738 739     if (_status == Faulting) 740         return; 741 742     if (fault != NoFault) { 743         DPRINTF(SimpleCPU, \"Fault occured. Handling the fault\\n\"); 744 745         advancePC(fault); 746 747         // A syscall fault could suspend this CPU (e.g., futex_wait) 748         // If the _status is not Idle, schedule an event to fetch the next 749         // instruction after 'stall' ticks. 750         // If the cpu has been suspended (i.e., _status == Idle), another 751         // cpu will wake this cpu up later. 752         if (_status != Idle) { 753             DPRINTF(SimpleCPU, \"Scheduling fetch event after the Fault\\n\"); 754 755             Tick stall = dynamic_pointer_cast&lt;SyscallRetryFault&gt;(fault) ? 756                          clockEdge(syscallRetryLatency) : clockEdge(); 757             reschedule(fetchEvent, stall, true); 758             _status = Faulting; 759         } 760 761         return; 762     } 763 764     if (!t_info.stayAtPC) 765         advancePC(fault); 766 767     if (tryCompleteDrain()) 768         return; 769 770     if (_status == BaseSimpleCPU::Running) { 771         // kick off fetch of next instruction... callback from icache 772         // response will cause that instruction to be executed, 773         // keeping the CPU running. 774         fetch(); 775     } 776 }When there is a pending translation fault,it delegates fault exception to the advancePC functionwhich actually controls the PC register of the CPU.TimingSimpleCPU inherits this function from BaseSimpleCPU,we will look at the BaseSimpleCPU class.gem5/src/cpu/simple/base.cc661 void662 BaseSimpleCPU::advancePC(const Fault &amp;fault)663 {664     SimpleExecContext &amp;t_info = *threadInfo[curThread];665     SimpleThread* thread = t_info.thread;666667     const bool branching(thread-&gt;pcState().branching());668669     //Since we're moving to a new pc, zero out the offset670     t_info.fetchOffset = 0;671     if (fault != NoFault) {672         curMacroStaticInst = StaticInst::nullStaticInstPtr;673         fault-&gt;invoke(threadContexts[curThread], curStaticInst);674         thread-&gt;decoder.reset();675     } else {676         if (curStaticInst) {677             if (curStaticInst-&gt;isLastMicroop())678                 curMacroStaticInst = StaticInst::nullStaticInstPtr;679             TheISA::PCState pcState = thread-&gt;pcState();680             TheISA::advancePC(pcState, curStaticInst);681             thread-&gt;pcState(pcState);682         }683     }684685     if (branchPred &amp;&amp; curStaticInst &amp;&amp; curStaticInst-&gt;isControl()) {686         // Use a fake sequence number since we only have one687         // instruction in flight at the same time.688         const InstSeqNum cur_sn(0);689690         if (t_info.predPC == thread-&gt;pcState()) {691             // Correctly predicted branch692             branchPred-&gt;update(cur_sn, curThread);693         } else {694             // Mis-predicted branch695             branchPred-&gt;squash(cur_sn, thread-&gt;pcState(), branching, curThread);696             ++t_info.numBranchMispred;697         }698     }699 }In general, the advancePC function updates current CPU context.However, depending on the current CPU state,whether the fault has been raised or not,it chooses different optionsto handle the generated fault and redirect the PC to move on.The invoke function called through the fault object handles the generated fault usually with the help of pre-defined ROM code.Also, it resets decoder and make curMacroStaticInst as Null.This is because we have to move on to the new PC after handling fault.On the other hand, as usually taken path,when the fault has not been raised during the current instruction execution,it updates micropc of the processor to the next instruction (line 676-682).pre-defined ROM code handles generated fault!Then let’s take a look athow the fault can be handled by the invoke functionimplemented in the fault class.gem5/srch/arch/x86/faults.cc 53 namespace X86ISA 54 { 55     void X86FaultBase::invoke(ThreadContext * tc, const StaticInstPtr &amp;inst) 56     { 57         if (!FullSystem) { 58             FaultBase::invoke(tc, inst); 59             return; 60         } 61 62         PCState pcState = tc-&gt;pcState(); 63         Addr pc = pcState.pc(); 64         DPRINTF(Faults, \"RIP %#x: vector %d: %s\\n\", 65                 pc, vector, describe()); 66         using namespace X86ISAInst::RomLabels; 67         HandyM5Reg m5reg = tc-&gt;readMiscRegNoEffect(MISCREG_M5_REG); 68         MicroPC entry; 69         if (m5reg.mode == LongMode) { 70             if (isSoft()) { 71                 entry = extern_label_longModeSoftInterrupt; 72             } else { 73                 entry = extern_label_longModeInterrupt; 74             } 75         } else { 76             entry = extern_label_legacyModeInterrupt; 77         } 78         tc-&gt;setIntReg(INTREG_MICRO(1), vector); 79         tc-&gt;setIntReg(INTREG_MICRO(7), pc); 80         if (errorCode != (uint64_t)(-1)) { 81             if (m5reg.mode == LongMode) { 82                 entry = extern_label_longModeInterruptWithError; 83             } else { 84                 panic(\"Legacy mode interrupts with error codes \" 85                         \"aren't implementde.\\n\"); 86             } 87             // Software interrupts shouldn't have error codes. If one 88             // does, there would need to be microcode to set it up. 89             assert(!isSoft()); 90             tc-&gt;setIntReg(INTREG_MICRO(15), errorCode); 91         } 92         pcState.upc(romMicroPC(entry)); 93         pcState.nupc(romMicroPC(entry) + 1); 94         tc-&gt;pcState(pcState); 95     }To look at the behavior of the invoke function of the fault,we have to look at the fault related classes first.GEM5 provides base interface for every faults defined in the x86 architecture.x86 provides different types of events that can intervene the execution flow,which are fault, abort, trap, interrupts. All those events inherit from the base x86 fault class X86FaultBasewhich provides general interfaces and semantics of the x86 fault events.However, depending on type of events,different classes inheriting the X86FaultBase can override invoke functionto define their own semantics of fault events. For example, PageFault class inherits X86FaultBase class and overrides invoke function to add its own pagefault related semantics before invoking the parent’s invoke function provided by the X86FaultBase class.Invoke change current RIP to pre-defined microopsBasically, invoke function makes the processor jump to the pre-defined microcode functionthat implements actual semantics of x86 fault handling.When the fault or interrupt is reported to the processor,first of all,it should stores current context of the processor.And then,it transfers a control flow of the processor to the designated fault handler represented by the IDTR register in x86.To jump to the pre-defined ROM codefrom the invoke function,it makes use of ROM labels that statically stores sequence of x86 microops.All the available ROM labels are defined in the RomLabels namespace as show in the below.gem5/build/X86/arch/x86/generated/decoder-ns.hh.inc 4587 namespace RomLabels { 4588 const static uint64_t label_longModeSoftInterrupt_stackSwitched = 92; 4589 const static uint64_t label_longModeInterrupt_processDescriptor = 11; 4590 const static uint64_t label_longModeInterruptWithError_cplStackSwitch = 152; 4591 const static uint64_t label_longModeInterrupt_istStackSwitch = 28; 4592 const static uint64_t label_jmpFarWork = 192; 4593 const static uint64_t label_farJmpSystemDescriptor = 207; 4594 const static uint64_t label_longModeSoftInterrupt_globalDescriptor = 71; 4595 const static uint64_t label_farJmpGlobalDescriptor = 199; 4596 const static uint64_t label_initIntHalt = 186; 4597 const static uint64_t label_longModeInterruptWithError_istStackSwitch = 150; 4598 const static uint64_t label_legacyModeInterrupt = 184; 4599 const static uint64_t label_longModeInterruptWithError_globalDescriptor = 132; 4600 const static uint64_t label_longModeSoftInterrupt_processDescriptor = 72; 4601 const static uint64_t label_longModeInterruptWithError = 122; 4602 const static uint64_t label_farJmpProcessDescriptor = 200; 4603 const static uint64_t label_longModeSoftInterrupt = 61; 4604 const static uint64_t label_longModeSoftInterrupt_istStackSwitch = 89; 4605 const static uint64_t label_longModeInterrupt_globalDescriptor = 10; 4606 const static uint64_t label_longModeInterrupt_cplStackSwitch = 30; 4607 const static uint64_t label_longModeInterrupt = 0; 4608 const static uint64_t label_longModeInterruptWithError_processDescriptor = 133; 4609 const static uint64_t label_longModeInterruptWithError_stackSwitched = 153; 4610 const static uint64_t label_longModeInterrupt_stackSwitched = 31; 4611 const static uint64_t label_longModeSoftInterrupt_cplStackSwitch = 91; 4612 const static MicroPC extern_label_initIntHalt = 186; 4613 const static MicroPC extern_label_longModeInterruptWithError = 122; 4614 const static MicroPC extern_label_longModeInterrupt = 0; 4615 const static MicroPC extern_label_longModeSoftInterrupt = 61; 4616 const static MicroPC extern_label_legacyModeInterrupt = 184; 4617 const static MicroPC extern_label_jmpFarWork = 192; 4618 }PageFault handling ROM codeAlthough we are looking at translation fault,note that is can be described as PageFault in x86.gem5/src/arch/x86/faults.cc137     void PageFault::invoke(ThreadContext * tc, const StaticInstPtr &amp;inst)138     {139         if (FullSystem) {140             /* Invalidate any matching TLB entries before handling the page fault */141             tc-&gt;getITBPtr()-&gt;demapPage(addr, 0);142             tc-&gt;getDTBPtr()-&gt;demapPage(addr, 0);143             HandyM5Reg m5reg = tc-&gt;readMiscRegNoEffect(MISCREG_M5_REG);144             X86FaultBase::invoke(tc);145             /*146              * If something bad happens while trying to enter the page fault147              * handler, I'm pretty sure that's a double fault and then all148              * bets are off. That means it should be safe to update this149              * state now.150              */151             if (m5reg.mode == LongMode) {152                 tc-&gt;setMiscReg(MISCREG_CR2, addr);153             } else {154                 tc-&gt;setMiscReg(MISCREG_CR2, (uint32_t)addr);155             }156         } else {157             PageFaultErrorCode code = errorCode;158             const char *modeStr = \"\";159             if (code.fetch)160                 modeStr = \"execute\";161             else if (code.write)162                 modeStr = \"write\";163             else164                 modeStr = \"read\";165166             // print information about what we are panic'ing on167             if (!inst) {168                 panic(\"Tried to %s unmapped address %#x.\\n\", modeStr, addr);169             } else {170                 panic(\"Tried to %s unmapped address %#x.\\nPC: %#x, Instr: %s\",171                       modeStr, addr, tc-&gt;pcState().pc(),172                       inst-&gt;disassemble(tc-&gt;pcState().pc(), debugSymbolTable));173             }174         }175     }Because most of the fault handling logic of the PageFault class overlaps with X86FaultBase,after handling TLB related issues,it just calls invoke function of X86FaultBase class.Because translation fault mainly happens in longmode,and generated fault is not software interrupt,we will take a look at the ROM label named label_longModeInterrupt.Pass arguments to the ROM codeAlso, before jumping to the ROM label,it sets micro architectural registers to pass interrupt number and PC address to the ROM code.Additionaly, when the interrupt makes use of error code,it should also be passed to the microcodeTo pass the arguments to the microcode world, it invokes setIntReg functions defined in the threadcontext. Threadcontext is instance of SimpleThread class defined in cpu/simple_thread.hh(When you use the o3 out-of-order cpu model, you have to look at O3ThreadContext class).Regardless of your processor model,both classes inherit ThreadContext class which provide generic register context and interface for manipulating the registers.gem5/src/cpu/simple_thread.hh 98 class SimpleThread : public ThreadState, public ThreadContext 99 {100   protected:101     typedef TheISA::MachInst MachInst;102     using VecRegContainer = TheISA::VecRegContainer;103     using VecElem = TheISA::VecElem;104     using VecPredRegContainer = TheISA::VecPredRegContainer;105   public:106     typedef ThreadContext::Status Status;107108   protected:109     std::array&lt;RegVal, TheISA::NumFloatRegs&gt; floatRegs;110     std::array&lt;RegVal, TheISA::NumIntRegs&gt; intRegs;111     std::array&lt;VecRegContainer, TheISA::NumVecRegs&gt; vecRegs;112     std::array&lt;VecPredRegContainer, TheISA::NumVecPredRegs&gt; vecPredRegs;113     std::array&lt;RegVal, TheISA::NumCCRegs&gt; ccRegs;114     TheISA::ISA *const isa;    // one \"instance\" of the current ISA.115116     TheISA::PCState _pcState;477     void478     setIntReg(RegIndex reg_idx, RegVal val) override479     {480         int flatIndex = isa-&gt;flattenIntIndex(reg_idx);481         assert(flatIndex &lt; TheISA::NumIntRegs);482         DPRINTF(IntRegs, \"Setting int reg %d (%d) to %#x.\\n\",483                 reg_idx, flatIndex, val);484         setIntRegFlat(flatIndex, val);485     }Detour to TheISA namespaceAlthough SimpleThread class can be seen as providing generic registers regardless of architectures, it declares ISA dependent registers. The magic is TheISA symbol. TheISA symbol will be translated to architecture specific namespace depending on the architecturethat the Gem5 has been compiled to.Let’s little bit detour and figure out how TheISA namespace works.When you don’t know what is the TheISA namesapce,you may want to grep “namespace TheISA”to find out files that define TheISA namespace.However, unfortunately,you can only find very few placeswhere the TheISA namespace has been declaredwith a handful of member functions.Then where those functions and variables of the TheISA namespace come from?To understand the TheISA:: namespace,we should look at the build files not the source file.build/X86/config/the_isa.hh  1 #ifndef __CONFIG_THE_ISA_HH__  2 #define __CONFIG_THE_ISA_HH__  3  4 #define ALPHA_ISA 1  5 #define ARM_ISA 2  6 #define MIPS_ISA 3  7 #define NULL_ISA 4  8 #define POWER_ISA 5  9 #define RISCV_ISA 6 10 #define SPARC_ISA 7 11 #define X86_ISA 8 12 13 enum class Arch { 14   AlphaISA = ALPHA_ISA, 15   ArmISA = ARM_ISA, 16   MipsISA = MIPS_ISA, 17   NullISA = NULL_ISA, 18   PowerISA = POWER_ISA, 19   RiscvISA = RISCV_ISA, 20   SparcISA = SPARC_ISA, 21   X86ISA = X86_ISA 22 }; 23 24 #define THE_ISA X86_ISA 25 #define TheISA X86ISA 26 #define THE_ISA_STR \"x86\" 27 28 #endif // __CONFIG_THE_ISA_HH__Here, we can easily find that TheISA is defined as X86ISA.Also when we look at the SConScript file,we can find python function names makeTheISA thatactually fills out content of config/the_isa.hh file.Here, because I compiled GEM5 with the X86 configuration,it defines the TheISA as X86ISA.Therefore, when the TheISA has been used on the cpu related files,it is not a actual namespace called “TheISA”,but the architecture dependent ISA namespace.Consequently,when you encounter namespace TheISA,first check whether the config/the_isa.hh header file has been includedin your target source file;and when the answer is yes,you have to look at the architecture dependent namespacedefined in the gem5/src/arch/YOUR_ARCHITECTURE directory.In my case, because I use the X86it should be X86ISA namespace.SetIntReg with TheISANow let’s go back to SimpleThread class.In addition to the architecture specific register context,it provides setIntReg function.It allows the processor to store the data on intRegs array located by the index.477     void478     setIntReg(RegIndex reg_idx, RegVal val) override479     {480         int flatIndex = isa-&gt;flattenIntIndex(reg_idx);481         assert(flatIndex &lt; TheISA::NumIntRegs);482         DPRINTF(IntRegs, \"Setting int reg %d (%d) to %#x.\\n\",483                 reg_idx, flatIndex, val);484         setIntRegFlat(flatIndex, val);485     }618     void619     setIntRegFlat(RegIndex idx, RegVal val) override620     {621         intRegs[idx] = val;622     }Note that the val is stored in the intRegs array through the unified interface setIntReg function.The IntRegs contains not only the architecture registers such as rsi,rdi,rcx in x86,but also the integer type micro-registers used only by the microops.Because x86 in GEM5 defines 16 Integer registers available to the microops,(look at gem5/src/arch/x86/x86_traits.hh) it can pass up to 16 Integer value to the microcodethrough the setIntReg function.As shown in the invoke function,micro register 1,7, and 15 has been used to pass the fault related arguments to the microops.Jump to the ROM code!After finishing setting the required parametersnow, it jumps to the stored ROM code pointed to by the label.This control flow transition is done by updating _pcState memeber field of the SimpleThread class object.gem5/srch/arch/x86/faults.cc 92         pcState.upc(romMicroPC(entry)); 93         pcState.nupc(romMicroPC(entry) + 1); 94         tc-&gt;pcState(pcState); 95     }When we look at the above codein the invoke function of X86FaultBase class,we can find that it updates upc field of the pcState to location of the ROM code.gem5/src/base/types.hh144 typedef uint16_t MicroPC;145146 static const MicroPC MicroPCRomBit = 1 &lt;&lt; (sizeof(MicroPC) * 8 - 1);147148 static inline MicroPC149 romMicroPC(MicroPC upc)150 {151     return upc | MicroPCRomBit;152 }153154 static inline MicroPC155 normalMicroPC(MicroPC upc)156 {157     return upc &amp; ~MicroPCRomBit;158 }159160 static inline bool161 isRomMicroPC(MicroPC upc)162 {163     return MicroPCRomBit &amp; upc;164 }Note that romMicroPC function sets flag to specify upc points to start address of ROM code.Here the flag is just bit-wise ored to the upc address.arch/generic/types.hh193 // A PC and microcode PC.194 template &lt;class MachInst&gt;195 class UPCState : public SimplePCState&lt;MachInst&gt;196 {197   protected:198     typedef SimplePCState&lt;MachInst&gt; Base;199 200     MicroPC _upc;201     MicroPC _nupc;202 203   public:204 205     MicroPC upc() const { return _upc; }206     void upc(MicroPC val) { _upc = val; }207 208     MicroPC nupc() const { return _nupc; }209     void nupc(MicroPC val) { _nupc = val; }After the upc address is generated,it needs to update the pcState variable to change the current upc address. You can also update the upc address of the current processor’s pcState variable, it is recommended to passnewly initialized pcState object to the processor context. Therefore, new pcState variable invokes upc functionto update its upc address.After that, by invoking tc-&gt;pcState(pcState),it update member field _pcState of a threadContextsto a new pcState,which makes the processor run from the updated micro pc addresswhen the next fetch happens.However, note that this function just updates _pcState member field of the ThreadContex.Then who actually redirects the pipeline to fetch the new instructions from the ROMnot from the faulting instruction?Let’s go back to the advancePC functionthat called the invoke function.Let’s go back to advancePC &amp; advanceInstgem5/src/cpu/simple/base.cc673         fault-&gt;invoke(threadContexts[curThread], curStaticInst);674         thread-&gt;decoder.reset();After the invoke function is called as part of the advancePC function,it resets the decoder,which updates decoder state as ResetState.gem5/src/cpu/simple/timing.cc 730 void 731 TimingSimpleCPU::advanceInst(const Fault &amp;fault) 732 { 733     SimpleExecContext &amp;t_info = *threadInfo[curThread]; 734 735     if (_status == Faulting) 736         return; 737 738     if (fault != NoFault) { 739         DPRINTF(SimpleCPU, \"Fault occured. Handling the fault\\n\"); 740 741         advancePC(fault); 742     if (fault != NoFault) { 743         DPRINTF(SimpleCPU, \"Fault occured. Handling the fault\\n\"); 744 745         advancePC(fault); 746 747         // A syscall fault could suspend this CPU (e.g., futex_wait) 748         // If the _status is not Idle, schedule an event to fetch the next 749         // instruction after 'stall' ticks. 750         // If the cpu has been suspended (i.e., _status == Idle), another 751         // cpu will wake this cpu up later. 752         if (_status != Idle) { 753             DPRINTF(SimpleCPU, \"Scheduling fetch event after the Fault\\n\"); 754 755             Tick stall = dynamic_pointer_cast&lt;SyscallRetryFault&gt;(fault) ? 756                          clockEdge(syscallRetryLatency) : clockEdge(); 757             reschedule(fetchEvent, stall, true); 758             _status = Faulting; 759         } 760 761         return; 762     }After returning from the advancePC instruction,advanceInst function checks status of the current processor.When the processor is not in idle state, it reschedules fetchEventto be executed again after stall ticks.Also note that status of the processor has been changed to Faulting.fetchEvent invokes fetch() functionBy the way what is the fetchEvent?  79 TimingSimpleCPU::TimingSimpleCPU(TimingSimpleCPUParams *p)  80     : BaseSimpleCPU(p), fetchTranslation(this), icachePort(this),  81       dcachePort(this), ifetch_pkt(NULL), dcache_pkt(NULL), previousCycle(0),  82       fetchEvent([this]{ fetch(); }, name())  83 {  84     _status = Idle;  85 }Because fetchEvent is initialized to invoke fetch() functionat TimingSimpleCPU constructor,after a stall time passed, it will invoke fetch function to fetch new instruction from the faulting address.fetchEvent is defined as a EventFunctionWrapper type used for registering event in GEM5.Also, the fetchEvent is initiated by the constructor of the TimingSimpleCPU class to invoke fetch() function.Therefore, after the stall ticks passed,it invokes fetch() function defined in the TimingSimpleCPU class.Now start to fetch from updatd RIP, the ROM code! 653 void 654 TimingSimpleCPU::fetch() 655 { 656     // Change thread if multi-threaded 657     swapActiveThread(); 658 659     SimpleExecContext &amp;t_info = *threadInfo[curThread]; 660     SimpleThread* thread = t_info.thread; 661 662     DPRINTF(SimpleCPU, \"Fetch\\n\"); 663 664     if (!curStaticInst || !curStaticInst-&gt;isDelayedCommit()) { 665         checkForInterrupts(); 666         checkPcEventQueue(); 667     } 668 669     // We must have just got suspended by a PC event 670     if (_status == Idle) 671         return; 672 673     TheISA::PCState pcState = thread-&gt;pcState(); 674     bool needToFetch = !isRomMicroPC(pcState.microPC()) &amp;&amp; 675                        !curMacroStaticInst; 676 677     if (needToFetch) { 678         _status = BaseSimpleCPU::Running; 679         RequestPtr ifetch_req = std::make_shared&lt;Request&gt;(); 680         ifetch_req-&gt;taskId(taskId()); 681         ifetch_req-&gt;setContext(thread-&gt;contextId()); 682         setupFetchRequest(ifetch_req); 683         DPRINTF(SimpleCPU, \"Translating address %#x\\n\", ifetch_req-&gt;getVaddr()); 684         thread-&gt;itb-&gt;translateTiming(ifetch_req, thread-&gt;getTC(), 685                 &amp;fetchTranslation, BaseTLB::Execute); 686     } else { 687         _status = IcacheWaitResponse; 688         completeIfetch(NULL); 689 690         updateCycleCounts(); 691         updateCycleCounters(BaseCPU::CPU_STATE_ON); 692     } 693 }Remeber that curMacroStaticInst has been set to StaticInst::nullStaticInstPtrby advancePC.Also, upc has been updated to the ROM code addresswith MicroPCRomBit flag.Therefore, it sets needToFetch True and start to fetch new instructions from the ROM code."
  },
  
  {
    "title": "Template Generating Microop",
    "url": "/posts/template-generating-microop/",
    "categories": "",
    "tags": "",
    "date": "2020-06-02 00:00:00 -0400",
    





    
    "snippet": "layout: posttitile: “GEM5 Template to replace code-literal”categories: [GEM5, Microops]—Automatic CPP Class Generation for Macroop and MicroopThis ISA includes macroop and microops of X86 architect...",
    "content": "layout: posttitile: “GEM5 Template to replace code-literal”categories: [GEM5, Microops]—Automatic CPP Class Generation for Macroop and MicroopThis ISA includes macroop and microops of X86 architecture. Therefore, to understand how GEM5 defines ISA, and how they are automatically translated into CPP classes, you should understand how PLY works. It is highly recommended to read this link.When you open the isa files in src/arch/x86/isa/microops/ directory, you will notice that it has two different types of statements defining the microop: let block and def template. Let’s take a look at the grammar rule for let block.Although actual simulation is achieved through the CPP implementations, GEM5  utilizes python to generate the CPP implementation automatically based on what the python classes define about each ISA. Therefore, the ISA file is usually  defines the python class required for representing ISA, especially the macroopand microop in our X86 case.let {                                                                               class LdStOp(X86Microop):                                                           def __init__(self, data, segment, addr, disp,                                           dataSize, addressSize, baseFlags, atCPL0, prefetch, nonSpec,                    implicitStack, uncacheable):                                                self.data = data                                                                [self.scale, self.index, self.base] = addr                                      self.disp = disp                                                                self.segment = segment                                                          self.dataSize = dataSize                                                        self.addressSize = addressSize                                                  self.memFlags = baseFlags                                                       if atCPL0:                                                                          self.memFlags += \" | (CPL0FlagBit &lt;&lt; FlagShift)\"                            self.instFlags = \"\"                                                             if prefetch:                                                                        self.memFlags += \" | Request::PREFETCH\"                                         self.instFlags += \" | (1ULL &lt;&lt; StaticInst::IsDataPrefetch)\"                 if nonSpec:                                                                         self.instFlags += \" | (1ULL &lt;&lt; StaticInst::IsNonSpeculative)\"               if uncacheable:                                                                     self.instFlags += \" | (Request::UNCACHEABLE)\"                               # For implicit stack operations, we should use *not* use the                    # alternative addressing mode for loads/stores if the prefix is set             if not implicitStack:                                                               self.memFlags += \" | (machInst.legacy.addr ? \" + \\                                               \"(AddrSizeFlagBit &lt;&lt; FlagShift) : 0)\"                                                                                                      ......                                                              }                                                                               def template ID {…};def template exampledef template MicroLeaExecute {    Fault %(class_name)s::execute(ExecContext *xc,          Trace::InstRecord *traceData) const    {        Fault fault = NoFault;        Addr EA;        %(op_decl)s;        %(op_rd)s;        %(ea_code)s;        DPRINTF(X86, \"%s : %s: The address is %#x\\n\", instMnem, mnemonic, EA);        %(code)s;        if(fault == NoFault)        {            %(op_wb)s;        }        return fault;    }};def template grammar rule*gem5/src/arch/isa_parser.py*    def p_def_template(self, t):        'def_template : DEF TEMPLATE ID CODELIT SEMI'        if t[3] in self.templateMap:            print(\"warning: template %s already defined\" % t[3])        self.templateMap[t[3]] = Template(self, t[4])As shown in the grammar rule, Template object is instantiated with the code literal of the def template block, t[4].The newly instantiated Template objects will be maintained by the templateMap of the parser.Note that its template name (t[3], ID) will be used to index the generated Template object inside the map. The GEM5 parser defines Template python class for this purpose. 106 #################### 107 # Template objects. 108 # 109 # Template objects are format strings that allow substitution from 110 # the attribute spaces of other objects (e.g. InstObjParams instances). 111  112 labelRE = re.compile(r'(?&lt;!%)%\\(([^\\)]+)\\)[sd]') 113  114 class Template(object): 115     def __init__(self, parser, t): 116         self.parser = parser 117         self.template = t 118  119     def subst(self, d): 120         myDict = None 121  122         # Protect non-Python-dict substitutions (e.g. if there's a printf 123         # in the templated C++ code) 124         template = self.parser.protectNonSubstPercents(self.template) 125  126         # Build a dict ('myDict') to use for the template substitution. 127         # Start with the template namespace.  Make a copy since we're 128         # going to modify it. 129         myDict = self.parser.templateMap.copy() 130  131         if isinstance(d, InstObjParams): 132             # If we're dealing with an InstObjParams object, we need 133             # to be a little more sophisticated.  The instruction-wide 134             # parameters are already formed, but the parameters which 135             # are only function wide still need to be generated. 136             compositeCode = '' 137  138             myDict.update(d.__dict__) 139             # The \"operands\" and \"snippets\" attributes of the InstObjParams 140             # objects are for internal use and not substitution. 141             del myDict['operands'] 142             del myDict['snippets'] 143  144             snippetLabels = [l for l in labelRE.findall(template) 145                              if l in d.snippets] 146  147             snippets = dict([(s, self.parser.mungeSnippet(d.snippets[s])) 148                              for s in snippetLabels]) 149  150             myDict.update(snippets) 151  152             compositeCode = ' '.join(map(str, snippets.values())) 153  154             # Add in template itself in case it references any 155             # operands explicitly (like Mem) 156             compositeCode += ' ' + template 157  158             operands = SubOperandList(self.parser, compositeCode, d.operands) 159  160             myDict['op_decl'] = operands.concatAttrStrings('op_decl') 161             if operands.readPC or operands.setPC: 162                 myDict['op_decl'] += 'TheISA::PCState __parserAutoPCState;\\n' 163  164             # In case there are predicated register reads and write, declare 165             # the variables for register indicies. It is being assumed that 166             # all the operands in the OperandList are also in the 167             # SubOperandList and in the same order. Otherwise, it is 168             # expected that predication would not be used for the operands. 169             if operands.predRead: 170                 myDict['op_decl'] += 'uint8_t _sourceIndex = 0;\\n' 171             if operands.predWrite: 172                 myDict['op_decl'] += 'uint8_t M5_VAR_USED _destIndex = 0;\\n' 173  174             is_src = lambda op: op.is_src 175             is_dest = lambda op: op.is_dest 176  177             myDict['op_src_decl'] = \\ 178                       operands.concatSomeAttrStrings(is_src, 'op_src_decl') 179             myDict['op_dest_decl'] = \\ 180                       operands.concatSomeAttrStrings(is_dest, 'op_dest_decl') 181             if operands.readPC: 182                 myDict['op_src_decl'] += \\ 183                     'TheISA::PCState __parserAutoPCState;\\n' 184             if operands.setPC: 185                 myDict['op_dest_decl'] += \\ 186                     'TheISA::PCState __parserAutoPCState;\\n' 187  188             myDict['op_rd'] = operands.concatAttrStrings('op_rd') 189             if operands.readPC: 190                 myDict['op_rd'] = '__parserAutoPCState = xc-&gt;pcState();\\n' + \\ 191                                   myDict['op_rd'] 192  193             # Compose the op_wb string. If we're going to write back the 194             # PC state because we changed some of its elements, we'll need to 195             # do that as early as possible. That allows later uncoordinated 196             # modifications to the PC to layer appropriately. 197             reordered = list(operands.items) 198             reordered.reverse() 199             op_wb_str = '' 200             pcWbStr = 'xc-&gt;pcState(__parserAutoPCState);\\n' 201             for op_desc in reordered: 202                 if op_desc.isPCPart() and op_desc.is_dest: 203                     op_wb_str = op_desc.op_wb + pcWbStr + op_wb_str 204                     pcWbStr = '' 205                 else: 206                     op_wb_str = op_desc.op_wb + op_wb_str 207             myDict['op_wb'] = op_wb_str 208  209         elif isinstance(d, dict): 210             # if the argument is a dictionary, we just use it. 211             myDict.update(d) 212         elif hasattr(d, '__dict__'): 213             # if the argument is an object, we use its attribute map. 214             myDict.update(d.__dict__) 215         else: 216             raise TypeError, \"Template.subst() arg must be or have dictionary\" 217         return template % myDict 218  219     # Convert to string. 220     def __str__(self): 221         return self.templateWhen the Template object is instantiated, its code-literal will be storedin the self.template field of the Template object. This template field will be used later in the subst method to substitute code-literal following the substitution string.One important definition provided by the Template class is subst. You can see that it returns template (code literal string)substituted with myDict. Note that the passed code literal contains unfinished parts that should be replaced before generating complete CPP statements. Therefore, the subst function creates the myDict based on the object passed to the subst, d.Usually, this passed object is InstObjParams providing microop or macroop specific information to complete general implementation provided by the template. Note that subst function manages myDict dictionary differently based on the type of the object passed to the subst function.When the InstObjParams type of object is passed,depending on the information provided by the items such as whether it is used for declaration, op_decl,or for data write-back, op_wb,it prepares myDict dictionary properly for later substitution.Automatically define CPP classes and associated methods for microop using templateIn the previous posting, we took a look at how the python class dedicated for one microop can be used to represent macroop and microop. Also, we saw that python class for macroop is used to populateCPP class counterpart that can be compiled with other CPP source code (GEM5 is CPP based project not python). In the middle of that journey, we saw that the getAllocator functionof the microop python class generates CPP code snippets instantiating CPP microop class which is the counter part of the microop python class. We will see how those CPP classes for microops are generated by utilizing templates.defineMicroLoadOp: define micro-load operations using templatesTo understand how the CPP class for one microop can be implemented,we will take a look at the load related micro instructions in x86 architecture. The most important function of this microop class generation is the subst method provided by the Template object.GEM5 utilize the substitution a lot to populate various instructions having similar semantics.gem5/src/arch/x86/isa/microops/ldstop.isa434 let {{435 436     # Make these empty strings so that concatenating onto437     # them will always work.438     header_output = \"\"439     decoder_output = \"\"440     exec_output = \"\"441 442     segmentEAExpr = \\443         'bits(scale * Index + Base + disp, addressSize * 8 - 1, 0);'444 445     calculateEA = 'EA = SegBase + ' + segmentEAExpr446 447     debuggingEA = \\448         'DPRINTF(X86, \"EA:%#x index:%#x base:%#x disp:%#x Segbase:%#x scale:%#x, addressSize:%#x, dataSize: %#x \\\\n\", EA, Index, Base, disp, SegBase, scale, addressSize, dataSize)'449 450 451     def defineMicroLoadOp(mnemonic, code, bigCode='',452                           mem_flags=\"0\", big=True, nonSpec=False,453                           implicitStack=False):454         global header_output455         global decoder_output456         global exec_output457         global microopClasses458         Name = mnemonic459         name = mnemonic.lower()460 461         # Build up the all register version of this micro op462         iops = [InstObjParams(name, Name, 'X86ISA::LdStOp',463                               { \"code\": code,464                                 \"ea_code\": calculateEA,465                                 \"memDataSize\": \"dataSize\" })]466         if big:467             iops += [InstObjParams(name, Name + \"Big\", 'X86ISA::LdStOp',468                                    { \"code\": bigCode,469                                      \"ea_code\": calculateEA,470                                      \"memDataSize\": \"dataSize\" })]471         for iop in iops:472             header_output += MicroLdStOpDeclare.subst(iop)473             decoder_output += MicroLdStOpConstructor.subst(iop)474             exec_output += MicroLoadExecute.subst(iop)475             exec_output += MicroLoadInitiateAcc.subst(iop)476             exec_output += MicroLoadCompleteAcc.subst(iop)477 478         if implicitStack:479             # For instructions that implicitly access the stack, the address480             # size is the same as the stack segment pointer size, not the481             # address size if specified by the instruction prefix482             addressSize = \"env.stackSize\"483         else:484             addressSize = \"env.addressSize\"485 486         base = LdStOp487         if big:488             base = BigLdStOp489         class LoadOp(base):490             def __init__(self, data, segment, addr, disp = 0,491                     dataSize=\"env.dataSize\",492                     addressSize=addressSize,493                     atCPL0=False, prefetch=False, nonSpec=nonSpec,494                     implicitStack=implicitStack,495                     uncacheable=False, EnTlb=False):496                 super(LoadOp, self).__init__(data, segment, addr,497                         disp, dataSize, addressSize, mem_flags,498                         atCPL0, prefetch, nonSpec, implicitStack,499                         uncacheable, EnTlb)500                 self.className = Name501                 self.mnemonic = name502 503         microopClasses[name] = LoadOp504 505     defineMicroLoadOp('Ld', 'Data = merge(Data, Mem, dataSize);',506                             'Data = Mem &amp; mask(dataSize * 8);')507     defineMicroLoadOp('Ldis', 'Data = merge(Data, Mem, dataSize);',508                               'Data = Mem &amp; mask(dataSize * 8);',509                                implicitStack=True)510     defineMicroLoadOp('Ldst', 'Data = merge(Data, Mem, dataSize);',511                               'Data = Mem &amp; mask(dataSize * 8);',512                       '(StoreCheck &lt;&lt; FlagShift)')513     defineMicroLoadOp('Ldstl', 'Data = merge(Data, Mem, dataSize);',514                                'Data = Mem &amp; mask(dataSize * 8);',515                       '(StoreCheck &lt;&lt; FlagShift) | Request::LOCKED_RMW',516                       nonSpec=True)As shown on the line 505-516, various load microops are populated by invoking defineMicroLoadOp python function. Because those microops have similar semantics which loads data from memory, defineMicroLoadOp function generates different microops by substituting generic template with microop-specific code-literals.You can find that multiple subst definitions frommultiple templates are invoked in the defineMicroLoadOp function (line 472-476)to generate complete implementation of each microop.Operands and its children classes can handle all operands in GEM5Before we take a look at how the template is used to generate actual code for the microops, we should understand what is the InstObjParams and why it is necessary for template substitutions.To understand InstObjParams, we further need a deeper understanding about parameter system deployed by the GEM5.This includes generic classes to represent parameters of microop and macroop, and architecture specific operands and its parsing.Generic classes representing various types of operands in GEM5First of all, we need to understand that GEM5 provide common classesthat can define multiple types of operands regardless of architecture.We will take a look at the class hierarchies representing various operands.gem5/src/arch/isa_parser.py 396 class Operand(object): 397     '''Base class for operand descriptors.  An instance of this class 398     (or actually a class derived from this one) represents a specific 399     operand for a code block (e.g, \"Rc.sq\" as a dest). Intermediate 400     derived classes encapsulates the traits of a particular operand 401     type (e.g., \"32-bit integer register\").''' 402  403     def buildReadCode(self, func = None): 404         subst_dict = {\"name\": self.base_name, 405                       \"func\": func, 406                       \"reg_idx\": self.reg_spec, 407                       \"ctype\": self.ctype} 408         if hasattr(self, 'src_reg_idx'): 409             subst_dict['op_idx'] = self.src_reg_idx 410         code = self.read_code % subst_dict 411         return '%s = %s;\\n' % (self.base_name, code) 412  413     def buildWriteCode(self, func = None): 414         subst_dict = {\"name\": self.base_name, 415                       \"func\": func, 416                       \"reg_idx\": self.reg_spec, 417                       \"ctype\": self.ctype, 418                       \"final_val\": self.base_name} 419         if hasattr(self, 'dest_reg_idx'): 420             subst_dict['op_idx'] = self.dest_reg_idx 421         code = self.write_code % subst_dict 422         return ''' 423         { 424             %s final_val = %s; 425             %s; 426             if (traceData) { traceData-&gt;setData(final_val); } 427         }''' % (self.dflt_ctype, self.base_name, code) 428  429     def __init__(self, parser, full_name, ext, is_src, is_dest): 430         self.full_name = full_name 431         self.ext = ext 432         self.is_src = is_src 433         self.is_dest = is_dest 434         # The 'effective extension' (eff_ext) is either the actual 435         # extension, if one was explicitly provided, or the default. 436         if ext: 437             self.eff_ext = ext 438         elif hasattr(self, 'dflt_ext'): 439             self.eff_ext = self.dflt_ext 440  441         if hasattr(self, 'eff_ext'): 442             self.ctype = parser.operandTypeMap[self.eff_ext] 443  444     # Finalize additional fields (primarily code fields).  This step 445     # is done separately since some of these fields may depend on the 446     # register index enumeration that hasn't been performed yet at the 447     # time of __init__(). The register index enumeration is affected 448     # by predicated register reads/writes. Hence, we forward the flags 449     # that indicate whether or not predication is in use. 450     def finalize(self, predRead, predWrite): 451         self.flags = self.getFlags() 452         self.constructor = self.makeConstructor(predRead, predWrite) 453         self.op_decl = self.makeDecl() 454  455         if self.is_src: 456             self.op_rd = self.makeRead(predRead) 457             self.op_src_decl = self.makeDecl() 458         else: 459             self.op_rd = '' 460             self.op_src_decl = '' 461  462         if self.is_dest: 463             self.op_wb = self.makeWrite(predWrite) 464             self.op_dest_decl = self.makeDecl() 465         else: 466             self.op_wb = '' 467             self.op_dest_decl = '' 468  469     def isMem(self): 470         return 0 471  472     def isReg(self): 473         return 0 474  475     def isFloatReg(self): 476         return 0 477  478     def isIntReg(self): 479         return 0 480  481     def isCCReg(self): 482         return 0 483  484     def isControlReg(self): 485         return 0 486  487     def isVecReg(self): 488         return 0 489  490     def isVecElem(self): 491         return 0 492  493     def isVecPredReg(self): 494         return 0 495  496     def isPCState(self): 497         return 0 498  499     def isPCPart(self): 500         return self.isPCState() and self.reg_spec 501  502     def hasReadPred(self): 503         return self.read_predicate != None 504  505     def hasWritePred(self): 506         return self.write_predicate != None 507  508     def getFlags(self): 509         # note the empty slice '[:]' gives us a copy of self.flags[0] 510         # instead of a reference to it 511         my_flags = self.flags[0][:] 512         if self.is_src: 513             my_flags += self.flags[1] 514         if self.is_dest: 515             my_flags += self.flags[2] 516         return my_flags 517  518     def makeDecl(self): 519         # Note that initializations in the declarations are solely 520         # to avoid 'uninitialized variable' errors from the compiler. 521         return self.ctype + ' ' + self.base_name + ' = 0;\\n'; 522  523  524 src_reg_constructor = '\\n\\t_srcRegIdx[_numSrcRegs++] = RegId(%s, %s);' 525 dst_reg_constructor = '\\n\\t_destRegIdx[_numDestRegs++] = RegId(%s, %s);'The Operand class is a generic class provides various definitions that can be overridden by its children classes.Only handful of them are overridden to tell a type of the current operand class represents.Let’s take a look at IntRegOperand class which inherits the base Operand class. 528 class IntRegOperand(Operand): 529     reg_class = 'IntRegClass' 530  531     def isReg(self): 532         return 1 533  534     def isIntReg(self): 535         return 1 536  537     def makeConstructor(self, predRead, predWrite): 538         c_src = '' 539         c_dest = '' 540  541         if self.is_src: 542             c_src = src_reg_constructor % (self.reg_class, self.reg_spec) 543             if self.hasReadPred(): 544                 c_src = '\\n\\tif (%s) {%s\\n\\t}' % \\ 545                         (self.read_predicate, c_src) 546  547         if self.is_dest: 548             c_dest = dst_reg_constructor % (self.reg_class, self.reg_spec) 549             c_dest += '\\n\\t_numIntDestRegs++;' 550             if self.hasWritePred(): 551                 c_dest = '\\n\\tif (%s) {%s\\n\\t}' % \\ 552                          (self.write_predicate, c_dest) 553  554         return c_src + c_destThe IntRegOperand class represents Integer type operand, thus it overrides isReg and isIntReg definition.One operand can be stored in a register or presented as a constant. Note that the IntRegOperand represents Integer type operand stored in the register.Finalize function generates actual code statements for operandOne most important definition provided by the base class is finalize. Note that all the Operands and its children classes and methods are defined as python syntax.Therefore, we should require a method to convert python representation to CPP which can be understandable by the GEM5. The finalize definition does this!Although different version of finalize implementation existsdepending on the operand type,we will take a look at the finalize of the Operand class. This is because most of the children classes of Operanddoesn’t override the finalize method. 450     def finalize(self, predRead, predWrite): 451         self.flags = self.getFlags() 452         self.constructor = self.makeConstructor(predRead, predWrite) 453         self.op_decl = self.makeDecl() 454 455         if self.is_src: 456             self.op_rd = self.makeRead(predRead) 457             self.op_src_decl = self.makeDecl() 458         else: 459             self.op_rd = '' 460             self.op_src_decl = '' 461 462         if self.is_dest: 463             self.op_wb = self.makeWrite(predWrite) 464             self.op_dest_decl = self.makeDecl() 465         else: 466             self.op_wb = '' 467             self.op_dest_decl = ''The finalize method generates mainly two code bloks:initialization code for operands generated by makeConstructorand code accessing operands such as register read or write retrieved by makeRead and makeWrite.Based on the operand type such as source and destination,either markeRead or makeWrite will be invoked.As a result, the actual CPP code statement that can access the operands will be generated. Let’s take a look at makeRead and makeWrite definitions provided by the IntRegOperand class as an example. 528 class IntRegOperand(Operand): ...... 556     def makeRead(self, predRead): 557         if (self.ctype == 'float' or self.ctype == 'double'): 558             error('Attempt to read integer register as FP') 559         if self.read_code != None: 560             return self.buildReadCode('readIntRegOperand') 561  562         int_reg_val = '' 563         if predRead: 564             int_reg_val = 'xc-&gt;readIntRegOperand(this, _sourceIndex++)' 565             if self.hasReadPred(): 566                 int_reg_val = '(%s) ? %s : 0' % \\ 567                               (self.read_predicate, int_reg_val) 568         else: 569             int_reg_val = 'xc-&gt;readIntRegOperand(this, %d)' % self.src_reg_idx 570  571         return '%s = %s;\\n' % (self.base_name, int_reg_val) 572  573     def makeWrite(self, predWrite): 574         if (self.ctype == 'float' or self.ctype == 'double'): 575             error('Attempt to write integer register as FP') 576         if self.write_code != None: 577             return self.buildWriteCode('setIntRegOperand') 578  579         if predWrite: 580             wp = 'true' 581             if self.hasWritePred(): 582                 wp = self.write_predicate 583  584             wcond = 'if (%s)' % (wp) 585             windex = '_destIndex++' 586         else: 587             wcond = '' 588             windex = '%d' % self.dest_reg_idx 589  590         wb = ''' 591         %s 592         { 593             %s final_val = %s; 594             xc-&gt;setIntRegOperand(this, %s, final_val);\\n 595             if (traceData) { traceData-&gt;setData(final_val); } 596         }''' % (wcond, self.ctype, self.base_name, windex) 597  598         return wbThe above two definitions check whether the current operands type matches the type represented by the IntRegOperand class. After that, it generates CPP statements which allow accesses to the operands and returns the string.Populating proper operand class instancesWe now understand GEM5 utilizes various types of operand classes to represent different type of operands independent on the architectures.Then how the each ISA of different architectures can utilize those classes to generate the operands initialization code and proper access codesformatted in CPP syntax? Yeah answer is the finalize method we’ve seen, but where and how can we generate instances of those operand classes?InstObjParams containing all information required for substitutionsNow it is time to go back to InstObjParams again!451     def defineMicroLoadOp(mnemonic, code, bigCode='',452                           mem_flags=\"0\", big=True, nonSpec=False,453                           implicitStack=False):......461         # Build up the all register version of this micro op462         iops = [InstObjParams(name, Name, 'X86ISA::LdStOp',463                               { \"code\": code,464                                 \"ea_code\": calculateEA,465                                 \"memDataSize\": \"dataSize\" })]466         if big:467             iops += [InstObjParams(name, Name + \"Big\", 'X86ISA::LdStOp',468                                    { \"code\": bigCode,469                                      \"ea_code\": calculateEA,470                                      \"memDataSize\": \"dataSize\" })]471         for iop in iops:472             header_output += MicroLdStOpDeclare.subst(iop)473             decoder_output += MicroLdStOpConstructor.subst(iop)474             exec_output += MicroLoadExecute.subst(iop)475             exec_output += MicroLoadInitiateAcc.subst(iop)476             exec_output += MicroLoadCompleteAcc.subst(iop)You might remember that InstObjParams is used for substituting the template.As shown in the code line 461-470 of the defineMicroLoadOp python definition,it defines iops which is the array of InstObjParams.After the iops array is populated, it is passed to the subst function of each template shown in the line 471-476.The subst function will replace the microop specific part of the implementationwith the information provided by the passed InstObjParams instance.Note that the code snippets defined as python dictionary using { } are passed tothe constructor of the InstObjParams python class.When you look up the code and calculateEA variables of the defineMicroLoadOp definition,you can easily find that they are code snippets also.Let’s take a look at InstObjParams python class.gem5/src/arch/isa_parser.py1413 class InstObjParams(object):1414     def __init__(self, parser, mnem, class_name, base_class = '',1415                  snippets = {}, opt_args = []):1416         self.mnemonic = mnem1417         self.class_name = class_name1418         self.base_class = base_class1419         if not isinstance(snippets, dict):1420             snippets = {'code' : snippets}1421         compositeCode = ' '.join(map(str, snippets.values()))1422         self.snippets = snippets14231424         self.operands = OperandList(parser, compositeCode)14251426         # The header of the constructor declares the variables to be used1427         # in the body of the constructor.1428         header = ''1429         header += '\\n\\t_numSrcRegs = 0;'1430         header += '\\n\\t_numDestRegs = 0;'1431         header += '\\n\\t_numFPDestRegs = 0;'1432         header += '\\n\\t_numVecDestRegs = 0;'1433         header += '\\n\\t_numVecElemDestRegs = 0;'1434         header += '\\n\\t_numVecPredDestRegs = 0;'1435         header += '\\n\\t_numIntDestRegs = 0;'1436         header += '\\n\\t_numCCDestRegs = 0;'14371438         self.constructor = header + \\1439                            self.operands.concatAttrStrings('constructor')14401441         self.flags = self.operands.concatAttrLists('flags')14421443         self.op_class = None14441445         # Optional arguments are assumed to be either StaticInst flags1446         # or an OpClass value.  To avoid having to import a complete1447         # list of these values to match against, we do it ad-hoc1448         # with regexps.1449         for oa in opt_args:1450             if instFlagRE.match(oa):1451                 self.flags.append(oa)1452             elif opClassRE.match(oa):1453                 self.op_class = oa1454             else:1455                 error('InstObjParams: optional arg \"%s\" not recognized '1456                       'as StaticInst::Flag or OpClass.' % oa)14571458         # Make a basic guess on the operand class if not set.1459         # These are good enough for most cases.1460         if not self.op_class:1461             if 'IsStore' in self.flags:1462                 # The order matters here: 'IsFloating' and 'IsInteger' are1463                 # usually set in FP instructions because of the base1464                 # register1465                 if 'IsFloating' in self.flags:1466                     self.op_class = 'FloatMemWriteOp'1467                 else:1468                     self.op_class = 'MemWriteOp'1469             elif 'IsLoad' in self.flags or 'IsPrefetch' in self.flags:1470                 # The order matters here: 'IsFloating' and 'IsInteger' are1471                 # usually set in FP instructions because of the base1472                 # register1473                 if 'IsFloating' in self.flags:1474                     self.op_class = 'FloatMemReadOp'1475                 else:1476                     self.op_class = 'MemReadOp'1477             elif 'IsFloating' in self.flags:1478                 self.op_class = 'FloatAddOp'1479             elif 'IsVector' in self.flags:1480                 self.op_class = 'SimdAddOp'1481             else:1482                 self.op_class = 'IntAluOp'14831484         # add flag initialization to contructor here to include1485         # any flags added via opt_args1486         self.constructor += makeFlagConstructor(self.flags)14871488         # if 'IsFloating' is set, add call to the FP enable check1489         # function (which should be provided by isa_desc via a declare)1490         # if 'IsVector' is set, add call to the Vector enable check1491         # function (which should be provided by isa_desc via a declare)1492         if 'IsFloating' in self.flags:1493             self.fp_enable_check = 'fault = checkFpEnableFault(xc);'1494         elif 'IsVector' in self.flags:1495             self.fp_enable_check = 'fault = checkVecEnableFault(xc);'1496         else:1497             self.fp_enable_check = ''The main purpose of InstObjParams is defining a particular dictionary. This dictionary stores all the passed information including class name and code snippets, which will be used later in subst definition of template objectto replace microcode specific parts of the microcode implementation template. One of the important information managed by the InstObjParams is the operands field (line 1424).Note that constructor of the InstObjParams instantiate another object called OperandList.OperandList parses operands from code snippetsThe OperandList parses code snippets of microopand generates Operand objects.Yeah this is one of the location where the Operand objects are populated.Each Operand provides useful information toconstructor creation and defining multiple definitions required for implementing one microop.The OperandList can generate Operand classes based on the operand keywords specified in the code-snippet. Note that OperandList takes second argument of the defineMicroLoadOp definition.defineMicroLoadOp('Ld', 'Data = merge(Data, Mem, dataSize);',For example, in the above defineMicroLoadOp invocation, ‘Data = merge(Data, Mem, dataSize);’ is passed to the OperandList’s constructor and stored to the operands field of the InstObjParams (populated in the defineMicroLoadOp).Note that this code snippet represents microop’s input and output operands.To understand details,Let’s take a look at OperandList python class.1127 class OperandList(object):1128     '''Find all the operands in the given code block.  Returns an operand1129     descriptor list (instance of class OperandList).'''1130     def __init__(self, parser, code):1131         self.items = []1132         self.bases = {}1133         # delete strings and comments so we don't match on operands inside1134         for regEx in (stringRE, commentRE):1135             code = regEx.sub('', code)1136         # search for operands1137         next_pos = 01138         while 1:1139             match = parser.operandsRE.search(code, next_pos)1140             if not match:1141                 # no more matches: we're done1142                 break1143             op = match.groups()1144             # regexp groups are operand full name, base, and extension1145             (op_full, op_base, op_ext) = op1146             # If is a elem operand, define or update the corresponding1147             # vector operand1148             isElem = False1149             if op_base in parser.elemToVector:1150                 isElem = True1151                 elem_op = (op_base, op_ext)1152                 op_base = parser.elemToVector[op_base]1153                 op_ext = '' # use the default one1154             # if the token following the operand is an assignment, this is1155             # a destination (LHS), else it's a source (RHS)1156             is_dest = (assignRE.match(code, match.end()) != None)1157             is_src = not is_dest11581159             # see if we've already seen this one1160             op_desc = self.find_base(op_base)1161             if op_desc:1162                 if op_ext and op_ext != '' and op_desc.ext != op_ext:1163                     error ('Inconsistent extensions for operand %s: %s - %s' \\1164                             % (op_base, op_desc.ext, op_ext))1165                 op_desc.is_src = op_desc.is_src or is_src1166                 op_desc.is_dest = op_desc.is_dest or is_dest1167                 if isElem:1168                     (elem_base, elem_ext) = elem_op1169                     found = False1170                     for ae in op_desc.active_elems:1171                         (ae_base, ae_ext) = ae1172                         if ae_base == elem_base:1173                             if ae_ext != elem_ext:1174                                 error('Inconsistent extensions for elem'1175                                       ' operand %s' % elem_base)1176                             else:1177                                 found = True1178                     if not found:1179                         op_desc.active_elems.append(elem_op)1180             else:1181                 # new operand: create new descriptor1182                 op_desc = parser.operandNameMap[op_base](parser,1183                     op_full, op_ext, is_src, is_dest)1184                 # if operand is a vector elem, add the corresponding vector1185                 # operand if not already done1186                 if isElem:1187                     op_desc.elemExt = elem_op[1]1188                     op_desc.active_elems = [elem_op]1189                 self.append(op_desc)1190             # start next search after end of current match1191             next_pos = match.end()1192         self.sort()1193         # enumerate source &amp; dest register operands... used in building1194         # constructor later1195         self.numSrcRegs = 01196         self.numDestRegs = 01197         self.numFPDestRegs = 01198         self.numIntDestRegs = 01199         self.numVecDestRegs = 01200         self.numVecPredDestRegs = 01201         self.numCCDestRegs = 01202         self.numMiscDestRegs = 01203         self.memOperand = None12041205         # Flags to keep track if one or more operands are to be read/written1206         # conditionally.1207         self.predRead = False1208         self.predWrite = False12091210         for op_desc in self.items:1211             if op_desc.isReg():1212                 if op_desc.is_src:1213                     op_desc.src_reg_idx = self.numSrcRegs1214                     self.numSrcRegs += 11215                 if op_desc.is_dest:1216                     op_desc.dest_reg_idx = self.numDestRegs1217                     self.numDestRegs += 11218                     if op_desc.isFloatReg():1219                         self.numFPDestRegs += 11220                     elif op_desc.isIntReg():1221                         self.numIntDestRegs += 11222                     elif op_desc.isVecReg():1223                         self.numVecDestRegs += 11224                     elif op_desc.isVecPredReg():1225                         self.numVecPredDestRegs += 11226                     elif op_desc.isCCReg():1227                         self.numCCDestRegs += 11228                     elif op_desc.isControlReg():1229                         self.numMiscDestRegs += 11230             elif op_desc.isMem():1231                 if self.memOperand:1232                     error(\"Code block has more than one memory operand.\")1233                 self.memOperand = op_desc12341235             # Check if this operand has read/write predication. If true, then1236             # the microop will dynamically index source/dest registers.1237             self.predRead = self.predRead or op_desc.hasReadPred()1238             self.predWrite = self.predWrite or op_desc.hasWritePred()12391240         if parser.maxInstSrcRegs &lt; self.numSrcRegs:1241             parser.maxInstSrcRegs = self.numSrcRegs1242         if parser.maxInstDestRegs &lt; self.numDestRegs:1243             parser.maxInstDestRegs = self.numDestRegs1244         if parser.maxMiscDestRegs &lt; self.numMiscDestRegs:1245             parser.maxMiscDestRegs = self.numMiscDestRegs12461247         # now make a final pass to finalize op_desc fields that may depend1248         # on the register enumeration1249         for op_desc in self.items:1250             op_desc.finalize(self.predRead, self.predWrite)OperandList parses code snippets with regular expression.Whenever a new keyword matches, it first checks its cache by invoking find_base definition of the OperandList class. If there has been a match, it will returns a proper Operand object that can represent the found keyword. If there is no matches, it should look up parser.operandNameMapwhich contains all mappings from specific keyword to particular Operand object (1180-1189).Note that the type of matching keyword can be anythingthat can be represented by the classes inheriting Operand class.Whenever, a matching Operand object is found, It stores parsed operand to the self.itemsthrough the self.append(op_desc) in line 1189.After parinsg the operands,it iterates every parsed operands stored in the self.itemsand invokes finalize function of each operand (1249-1250).The finalize function translates each tokens to a code block thatupdates or accesses registerdepending on destination, source, and type of the operands.Operand parsing and operandNameMapWhen the new keyword is found in the code snippet,it should look up the operandNameMap to find matching Operand object. Then where and how the operandNameMap has been initialized to contain all required information for mapping keyword to Operand object. The answer is on the parsing!gem5/src/arch/x86/isa/operands.isa 91 def operands {{ 92         'SrcReg1':       foldInt('src1', 'foldOBit', 1), 93         'SSrcReg1':      intReg('src1', 1), 94         'SrcReg2':       foldInt('src2', 'foldOBit', 2), 95         'SSrcReg2':      intReg('src2', 1), 96         'Index':         foldInt('index', 'foldABit', 3), 97         'Base':          foldInt('base', 'foldABit', 4), 98         'DestReg':       foldInt('dest', 'foldOBit', 5), 99         'SDestReg':      intReg('dest', 5),100         'Data':          foldInt('data', 'foldOBit', 6),101         'DataLow':       foldInt('dataLow', 'foldOBit', 6),102         'DataHi':        foldInt('dataHi', 'foldOBit', 6),103         'ProdLow':       impIntReg(0, 7),104         'ProdHi':        impIntReg(1, 8),105         'Quotient':      impIntReg(2, 9),106         'Remainder':     impIntReg(3, 10),107         'Divisor':       impIntReg(4, 11),108         'DoubleBits':    impIntReg(5, 11),109         'Rax':           intReg('(INTREG_RAX)', 12),110         'Rbx':           intReg('(INTREG_RBX)', 13),111         'Rcx':           intReg('(INTREG_RCX)', 14),112         'Rdx':           intReg('(INTREG_RDX)', 15),113         'Rsp':           intReg('(INTREG_RSP)', 16),114         'Rbp':           intReg('(INTREG_RBP)', 17),115         'Rsi':           intReg('(INTREG_RSI)', 18),116         'Rdi':           intReg('(INTREG_RDI)', 19),...As shown in the above operands definition, def operands,each architecture defines operands listthat can be used as operands of instructions. Although it could be seen as a function definition in the python,note that its file extension is not py but isa.Also, this is not a correct function definition semantics in python.Yeah parser needs to parse this python like block!gem5/src/arch/isa_parser.py2066     # Define the mapping from operand names to operand classes and2067     # other traits.  Stored in operandNameMap.2068     def p_def_operands(self, t):2069         'def_operands : DEF OPERANDS CODELIT SEMI'2070         if not hasattr(self, 'operandTypeMap'):2071             error(t.lineno(1),2072                   'error: operand types must be defined before operands')2073         try:2074             user_dict = eval('{' + t[3] + '}', self.exportContext)2075         except Exception, exc:2076             if debug:2077                 raise2078             error(t.lineno(1), 'In def operands: %s' % exc)2079         self.buildOperandNameMap(user_dict, t.lexer.lineno)The def operand block is parsed by the isa_parseras other isa definition.As shown on the above grammar rule,when the def operands block is found,it invokes buildOperandNameMap functionand generates operandNameMap.As a result, the operandNameMap can provide mapping betweenoperands keyword to suitable Operand object usedfor accessing that operands.For example, as shown in the above def operands blocks,Data keyword is translated into IntRegOperand object.finalize example. 450     def finalize(self, predRead, predWrite): 451         self.flags = self.getFlags() 452         self.constructor = self.makeConstructor(predRead, predWrite) 453         self.op_decl = self.makeDecl() 454  455         if self.is_src: 456             self.op_rd = self.makeRead(predRead) 457             self.op_src_decl = self.makeDecl() 458         else: 459             self.op_rd = '' 460             self.op_src_decl = '' 461  462         if self.is_dest: 463             self.op_wb = self.makeWrite(predWrite) 464             self.op_dest_decl = self.makeDecl() 465         else: 466             self.op_wb = '' 467             self.op_dest_decl = ''After all arguments are translated into proper Operand objects,the finalize definition of those objects should be invoked to generate CPP statements. Let’s take a look at the IntRegOperand object because Data keyword is mapped to this Operand object. Because the IntRegOperand does not override the finalize method,the finalize method of the base class (Operand) will be invoked.As a consequence, either makeRead or makeWrite of the IntRegOperand Because the Data keyword is located on the LHS of the statement,it will be set as destination, and the makeWrite operation will be invoked as a result of the finalize. Also the generated result will be stored in the op_wb field of the IntRegOperand object.We will see how this field will replace the template of the Ld micro-load instruction. Also, note that other fields such as op_xx are generated in the finalize definition(op_decl for declaring variables, op_rd for read operations for example). 524 src_reg_constructor = '\\n\\t_srcRegIdx[_numSrcRegs++] = RegId(%s, %s);' 525 dst_reg_constructor = '\\n\\t_destRegIdx[_numDestRegs++] = RegId(%s, %s);' 526  527  528 class IntRegOperand(Operand): 529     reg_class = 'IntRegClass' ...... 537     def makeConstructor(self, predRead, predWrite): 538         c_src = '' 539         c_dest = '' 540  541         if self.is_src: 542             c_src = src_reg_constructor % (self.reg_class, self.reg_spec) 543             if self.hasReadPred(): 544                 c_src = '\\n\\tif (%s) {%s\\n\\t}' % \\ 545                         (self.read_predicate, c_src) 546  547         if self.is_dest: 548             c_dest = dst_reg_constructor % (self.reg_class, self.reg_spec) 549             c_dest += '\\n\\t_numIntDestRegs++;' 550             if self.hasWritePred(): 551                 c_dest = '\\n\\tif (%s) {%s\\n\\t}' % \\ 552                          (self.write_predicate, c_dest) ...... 573     def makeWrite(self, predWrite): 574         if (self.ctype == 'float' or self.ctype == 'double'): 575             error('Attempt to write integer register as FP') 576         if self.write_code != None: 577             return self.buildWriteCode('setIntRegOperand') 578 579         if predWrite: 580             wp = 'true' 581             if self.hasWritePred(): 582                 wp = self.write_predicate 583 584             wcond = 'if (%s)' % (wp) 585             windex = '_destIndex++' 586         else: 587             wcond = '' 588             windex = '%d' % self.dest_reg_idx 589 590         wb = ''' 591         %s 592         { 593             %s final_val = %s; 594             xc-&gt;setIntRegOperand(this, %s, final_val);\\n 595             if (traceData) { traceData-&gt;setData(final_val); } 596         }''' % (wcond, self.ctype, self.base_name, windex) 597 598         return wbAs shown in the above code,the makeWrite definition of the IntRegOperand class also utilize string substitutions. The final_val local variable is declared as Integer type because it is IntRegOperand class,and the self.base_name which is the name of the keyword Data is assigned to the variable. After that, by invoking setIntRegOperand function, it sets the final_val to the destination register operand which can be accessible by the ExecContext (xc). The substituted string is returned as a result of finalize method, but note that still it is not printed out as CPP statement to the automatically generated code yet. Yeah! The code has been parsed, produced as the OperandList, and stored in the operand field of the InstObjParamsRemember that InstObjParams is used to replace generic template to generate microcode implementation!In a nutshell: generating CPP class for microopAlthough we spent a lot of times to cover many details of parser such as Template and Operands, the one of the most important goal of this posting is understanding how the CPP class associated with one microop can be automatically generated. In the previous posting, we only found that the getAllocator of the python class associated with one microop generates constructor code for initiating CPP class defined for the microop. However, to implement the CPP class, we also need class definition and member functions required to implement semantics of the microop in addition to the constructor method of the class.MicroLdStOpDeclare: generating CPP class for micro-load operationsAlthough there are several microops related with load operations, the skeleton of those microops are same (represented as Template) because they have similarities because of the characteristics of the load operation.First of all, the MicroLdStOpDeclare template is used to generate CPP class declaration.def template MicroLdStOpDeclare {{    class %(class_name)s : public %(base_class)s    {      public:        %(class_name)s(ExtMachInst _machInst,                const char * instMnem, uint64_t setFlags,                uint8_t _scale, InstRegIndex _index, InstRegIndex _base,                uint64_t _disp, InstRegIndex _segment,                InstRegIndex _data,                uint8_t _dataSize, uint8_t _addressSize,                Request::FlagsType _memFlags);        Fault execute(ExecContext *, Trace::InstRecord *) const;        Fault initiateAcc(ExecContext *, Trace::InstRecord *) const;        Fault completeAcc(PacketPtr, ExecContext *, Trace::InstRecord *) const;    };}};Based on the InstObjParams passed to the defineMicroLoadOp, microop specific strings will finish the uncompleted parts of the template.Note that the generated class also have the constructor which we were looking for.271 def template MicroLdStOpConstructor {{272     %(class_name)s::%(class_name)s(273             ExtMachInst machInst, const char * instMnem, uint64_t setFlags,274             uint8_t _scale, InstRegIndex _index, InstRegIndex _base,275             uint64_t _disp, InstRegIndex _segment,276             InstRegIndex _data,277             uint8_t _dataSize, uint8_t _addressSize,278             Request::FlagsType _memFlags) :279         %(base_class)s(machInst, \"%(mnemonic)s\", instMnem, setFlags,280                 _scale, _index, _base,281                 _disp, _segment, _data,282                 _dataSize, _addressSize, _memFlags, %(op_class)s)283     {284         %(constructor)s;285     }286 }};The constructor’s implementation itself can be also generated with the help of another Template substitution, MicroLdStOpConstructor.MicroLoadExecute: template used to implement micro-load operationMore importantly, in addition to the constructor for the microop, each microop should implement several definitions to have proper semantics of the microop.Let’s take a look at the MicroLoadExecute template. The definition generated by this template is called execute, and most of the Ld style microcode implements this function. However, depending on the semantics of micro-load instructions,different implementation of the execute will be populated. The different InstObjParams result in different replacement in the template,and the corresponding implementation will be produced as a consequence. 90 def template MicroLoadExecute {{ 91     Fault %(class_name)s::execute(ExecContext *xc, 92           Trace::InstRecord *traceData) const 93     { 94         Fault fault = NoFault; 95         Addr EA; 96 97         %(op_decl)s; 98         %(op_rd)s; 99         %(ea_code)s;100         DPRINTF(X86, \"%s : %s: The address is %#x\\n\", instMnem, mnemonic, EA);101102         fault = readMemAtomic(xc, traceData, EA, Mem, dataSize, memFlags);103104         if (fault == NoFault) {105             %(code)s;106         } else if (memFlags &amp; Request::PREFETCH) {107             // For prefetches, ignore any faults/exceptions.108             return NoFault;109         }110         if(fault == NoFault)111         {112             %(op_wb)s;113         }114115         return fault;116     }117 }};The above template contains incomplete code-snippets starting with % keyword. When the subst function of the corresponding template object is invoked,all those uncompleted parts will be replaced.For this replacement, we built the myDict dictionary.Note that this myDict initialize itself using the information provided by the InstObjParams such as operands field of it.Therefore, during substitution, if it encounters any keyword starting with %, it should refer to myDict to retrieve proper replacement for that. For example, class_name is provided by the InstObjParams.Also, op_wb is the CPP statements translated from the keyword Data to write back the result to the output register. Let’s take a look at how the execute function will be implemented after substitution.gem5/build/X86/arch/x86/generated/exec-ns.cc.inc19101     Fault Ld::execute(ExecContext *xc,19102           Trace::InstRecord *traceData) const19103     {19104         Fault fault = NoFault;19105         Addr EA;1910619107         uint64_t Index = 0;19108 uint64_t Base = 0;19109 uint64_t Data = 0;19110 uint64_t SegBase = 0;19111 uint64_t Mem;19112 ;19113         Index = xc-&gt;readIntRegOperand(this, 0);19114 Base = xc-&gt;readIntRegOperand(this, 1);19115 Data = xc-&gt;readIntRegOperand(this, 2);19116 SegBase = xc-&gt;readMiscRegOperand(this, 3);19117 ;19118         EA = SegBase + bits(scale * Index + Base + disp, addressSize * 8 - 1, 0);;19119         DPRINTF(X86, \"%s : %s: The address is %#x\\n\", instMnem, mnemonic, EA);1912019121         fault = readMemAtomic(xc, traceData, EA, Mem, dataSize, memFlags);1912219123         if (fault == NoFault) {19124             Data = merge(Data, Mem, dataSize);;19125         } else if (memFlags &amp; Request::PREFETCH) {19126             // For prefetches, ignore any faults/exceptions.19127             return NoFault;19128         }19129         if(fault == NoFault)19130         {191311913219133         {19134             uint64_t final_val = Data;19135             xc-&gt;setIntRegOperand(this, 0, final_val);1913619137             if (traceData) { traceData-&gt;setData(final_val); }19138         };19139         }1914019141         return fault;19142     }As shown in the Line 19133-19138, the CPP statements translated from Data keyword of the code-snippet are implemented as a result of replacing op_wb."
  },
  
  {
    "title": "Gem5 Macroop To Microop",
    "url": "/posts/gem5-macroop-to-microop/",
    "categories": "GEM5, Macroop, Microops, PLY",
    "tags": "",
    "date": "2020-06-01 00:00:00 -0400",
    





    
    "snippet": "Macroop and Microop in Computer Architecture  Macroop (Macro-operation):          A macroop is a high-level instruction or operation that is part of theinstruction set architecture (ISA) of a proce...",
    "content": "Macroop and Microop in Computer Architecture  Macroop (Macro-operation):          A macroop is a high-level instruction or operation that is part of theinstruction set architecture (ISA) of a processor. It represents a singleoperation that a program wants to perform, such as adding two numbers orloading a value from memory.      Macroops are what programmers typically interact with when writing code, as they correspond to the instructions in the assembly language or machinecode.        Microop (Micro-operation or μop):          A microop is a low-level operation that a processor’s control unitdecomposes a macroop into during the execution phase. It is a fundamental operation that the processor can execute directly.      Processors often use micro-operations internally to break down complexmacroops into simpler, more manageable tasks. These tasks are then executed in the processor’s pipeline.      In summary, the execution of a program involves the translation of macroops(high-level instructions in the ISA) into a sequence of microops (low-leveloperations) that the processor can execute efficiently. This translation allows for more parallel and optimized execution within the processor’s pipeline.Mov macroop implementation in GEM5As an illustration of defining each macroop based on microops, let’s see thehow Mov instruction in the X86 architecture can be implemented with microops.# gem5/src/arch/x86/isa/insts/general_purpose/data_transfer/move.pymicrocode = '''## Regular moves#def macroop MOV_R_MI {    limm t1, imm, dataSize=asz    ld reg, seg, [1, t0, t1]};def macroop MOV_MI_R {    limm t1, imm, dataSize=asz    st reg, seg, [1, t0, t1]};As depicted in the provided code, every macrooperation (macroop) is composed ofseveral microoperations (microops). Consequently, the processor transforms the macroop into a series of microops and proceeds to execute these microops ratherthan the original macroop. You may wonder about the process by which thesemacroop definitions are parsed and interpreted as the microoperations, allowing the processor to execute them within the pipeline. I will give you the details in this posting!GEM5 Domain Specific Language (DSL) and Python-Lex-Yacc (PLY)You may notice a resemblance between the code that defines the macroop semantics and Python syntax; nevertheless, it’s crucial to clarify that the code is not written in Python. GEM5 utilizes Domain-Specific Languages (DSL) to implement ISA including macroops and microops. As it is not a Python, it can be directlyinterpreted as it is, and PLY generates lexer and parser to interpret the ISAdefined with GEM5. Therefore,  it is highly recommended to read this tutorial about PLY before continuing reading this posting.Based on the predefined rules, GEM5 converts the macroops and microopsimplemented in the predefined DSL into corresponding python classes. As similarto how GEM5 generates CPP implementations for Params required for instantiatingthe hardware module in CPP, the goal of the parsing ISA is generating CPP implementations that can be utilized during simulation, for example to decode instruction and execute them in the processor pipeline.Parsing Instruction Set Architecture (ISA)Since GEM5 utilize SCons, it generates Actions and Builders to process input and generate output files as a result. Since the expected result of the parsingISA of the target architecture is CPP implementations of the target ISA, the SConscript initiates the parsing before compiling the CPP code base. Let’s seehow the SConscript initiates the parsing.#gem5/src/arch/SConscriptparser_py = File('isa_parser.py')micro_asm_py = File('micro_asm.py')import plydef run_parser(target, source, env):    # Add the current directory to the system path so we can import files.    sys.path[0:0] = [ parser_py.dir.abspath ]    import isa_parser    parser = isa_parser.ISAParser(target[0].dir.abspath)    parser.parse_isa_desc(source[0].abspath)desc_action = MakeAction(run_parser, Transform(\"ISA DESC\", 1))IsaDescBuilder = Builder(action=desc_action)......def ISADesc(desc, decoder_splits=1, exec_splits=1):    generated_dir = File(desc).dir.up().Dir('generated')    def gen_file(name):        return generated_dir.File(name)    gen = []    def add_gen(name):        gen.append(gen_file(name))    # Tell scons about the various files the ISA parser will generate.    add_gen('decoder-g.cc.inc')    add_gen('decoder-ns.cc.inc')    add_gen('decode-method.cc.inc')    add_gen('decoder.hh')    add_gen('decoder-g.hh.inc')    add_gen('decoder-ns.hh.inc')    add_gen('exec-g.cc.inc')    add_gen('exec-ns.cc.inc')    add_gen('max_inst_regs.hh')    # These generated files are also top level sources.    def source_gen(name):        add_gen(name)        Source(gen_file(name))    source_gen('decoder.cc')    if decoder_splits == 1:        source_gen('inst-constrs.cc')    else:        for i in range(1, decoder_splits + 1):            source_gen('inst-constrs-%d.cc' % i)    if exec_splits == 1:        source_gen('generic_cpu_exec.cc')    else:        for i in range(1, exec_splits + 1):            source_gen('generic_cpu_exec_%d.cc' % i)    # Actually create the builder.    sources = [desc, parser_py, micro_asm_py]    IsaDescBuilder(target=gen, source=sources, env=env)    return genrun_parser function is responsible for initiating the parsing for the target architecture. It is invoked when the IsaDescBuilder is called. The ISADescBuilder is created through the ISADesc function. In the context of a specific architecture,the developer’s responsibility is simply to call the ISADesc method in the local SConscript defined within the target architecture directory. The SConscript foreach architecture supplies details about the input ISA files that should be parsed. This information, in the form of a “desc” string, is then utilized by passing it to the ISADesc function.#gem5/src/arch/x86/SConscript    # Add in files generated by the ISA description.    isa_desc_files = ISADesc('isa/main.isa')    for f in isa_desc_files:        # Add in python file dependencies that won't be caught otherwise        for pyfile in python_files:            env.Depends(f, \"isa/insts/%s\" % pyfile)When you look at the SConscript of the X86 architecture, you will find that it passes isa/main.isa file to ISADesc function. The file ending with isa files are usually used to define ISA of the target architecture. In this example, the main.isa file is utilized to include all isa files that need a parsing.Additionally, observe that the isa_parser.py and micro_asm.py files are provided to the ISADescrBuilder. These files contain essential functions and classes for utilizing PLY and initializing the parsing process. As depicted in the run_parser function, it generates ISAParser python class object that actually handles ISA parsing utilizing the PLY. Note that this class is definedin the isa_parser.py file. Also parse_isa_desc is invoked through theISAParser to initiate the parsing.ISAParser (isa_parser.py)The ISAParser class is the most important python class that has two crucial rolesin parsing ISA files. It defines necessary rules for lexer and parser such as regular expression for tokenziation and context free grammars for parsing. SinceGEM5 utilizes PLY, the rules should be defined following the PLY’s syntax. For example, context free grammar required for parsing can be defined as a python method that starts with p_.Since the ISAParser and its method to parse the ISA files are utilized irrespective of the target architecture, it implies that all architectures utilize the same GEM5 DSL to define their respective ISAs. The GEM5 DSL is sufficiently adaptable to support the implementation of ISAs for diverse architectures, including x86, arm, power, mips, and sparc. As a result, there isgenerally no need to introduce additional rules for parsing most of the time.class ISAParser(Grammar):    # For Lexer    # Regular expressions for token matching    t_LPAREN           = r'\\('    t_RPAREN           = r'\\)'    t_LBRACKET         = r'\\['    t_RBRACKET         = r'\\]'    t_LBRACE           = r'\\{'    t_RBRACE           = r'\\}'    t_LESS             = r'\\&lt;'    t_GREATER          = r'\\&gt;'    t_EQUALS           = r'='    t_COMMA            = r','    t_SEMI             = r';'    t_DOT              = r'\\.'    t_COLON            = r':'    t_DBLCOLON         = r'::'    t_ASTERISK         = r'\\*'    ......    # For Parser    def p_specification(self, t):        'specification : opt_defs_and_outputs top_level_decode_block'        for f in self.splits.keys():            f.write('\\n#endif\\n')        for f in self.files.values(): # close ALL the files;            f.close() # not doing so can cause compilation to fail        self.write_top_level_files()        t[0] = True    ......Furthermore, the ISAParser class is utilized to instantiate the lexer and parser.However, you might wonder where is the parser and lexer instance generated fromPLY because there is no relevant attributes defined in the ISAParser class. Notethat it isinherited from Grammar class! Also, this class is closely related withthe parse_isa_desc function of the ISAParser.class Grammar(object):    def __getattr__(self, attr):        if attr == 'lexers':            self.lexers = []            return self.lexers                if attr == 'lex_kwargs':            self.setupLexerFactory()            return self.lex_kwargs                if attr == 'yacc_kwargs':            self.setupParserFactory()            return self.yacc_kwargs                if attr == 'lex':            self.lex = ply.lex.lex(module=self, **self.lex_kwargs)            return self.lex                if attr == 'yacc':            self.yacc = ply.yacc.yacc(module=self, **self.yacc_kwargs)            return self.yacc                if attr == 'current_lexer':            if not self.lexers:                return None            return self.lexers[-1][0]                if attr == 'current_source':            if not self.lexers:                return '&lt;none&gt;'            return self.lexers[-1][1]                if attr == 'current_line':            if not self.lexers:                return -1            return self.current_lexer.lineno    def parse_string(self, data, source='&lt;string&gt;', debug=None, tracking=0):        if not isinstance(data, string_types):            raise AttributeError(                \"argument must be a string, was '%s'\" % type(f))        lexer = self.lex.clone()        lexer.input(data)        self.lexers.append((lexer, source))        lrtab = ply.yacc.LRTable()        lrtab.lr_productions = self.yacc.productions        lrtab.lr_action = self.yacc.action        lrtab.lr_goto = self.yacc.goto        parser = ply.yacc.LRParser(lrtab, self.yacc.errorfunc)        result = parser.parse(lexer=lexer, debug=debug, tracking=tracking)        self.lexers.pop()        return resultActually, the Grammer class provides parse_string method that parser the passed ISA files utilizing PLY Lexer, Yacc, and Parser. As depicted in the code,the parse_string accesses lex and yacc attribute of the Grammar class. As theseattributes are not statically defined in the class, it is generated at runtimewhen they are accessed through the ‘getattr’ function. After generating the lexer and parser, parser function of the generated parser will parser the ISAfiles and return the result.    def _parse_isa_desc(self, isa_desc_file):        '''Read in and parse the ISA description.'''                    if isa_desc_file in ISAParser.AlreadyGenerated:            return                # grab the last three path components of isa_desc_file        self.filename = '/'.join(isa_desc_file.split('/')[-3:])        # Read file and (recursively) all included files into a string.        # PLY requires that the input be in a single string so we have to        # do this up front.        isa_desc = self.read_and_flatten(isa_desc_file)                # Initialize lineno tracker        self.lex.lineno = LineTracker(isa_desc_file)                # Parse.        self.parse_string(isa_desc)                ISAParser.AlreadyGenerated[isa_desc_file] = None            def parse_isa_desc(self, *args, **kwargs):        try:            self._parse_isa_desc(*args, **kwargs)        except ISAParserError as e:            print(backtrace(self.fileNameStack))            print(\"At %s:\" % e.lineno)            print(e)             sys.exit(1)Going back to run_parser function, it invokes the parse_isa_desc function of theParser python class. It further invokes the parser_string function from the Grammar class and starts parsing ISA files!MicroAssembler (micro_asm.py) Although the rules defined in the ISAParser class is generally sufficient to define ISA of one architecture, it might not be sufficient for specificarchitecture such as X86 that utilizes microops to define the macroop syntax. Since utilizing macroop and microop at the same time to define ISA is not common,for example only X86 architecture utilize both, in defining ISA, GEM5 provides another class called MicroAssembler and appropriate rules to parse the GEM5DSL used for defining macroops and microops.#gem5/src/arch/micro_asm.pyclass MicroAssembler(object):    def (self, macro_type, microops,        rom = None, rom_macroop_type = None):        self.lexer = lex.lex()        self.parser = yacc.yacc()        self.parser.macro_type = macro_type        self.parser.macroops = {}        self.parser.microops = microops        self.parser.rom = rom        self.parser.rom_macroop_type = rom_macroop_type        self.parser.symbols = {}        self.symbols = self.parser.symbolsSimilar to ISAParser class, it generates PLY lexer and parser  when the MicroAssembler object is instantiated. However, compared to ISAParser that onlyrequires the path for the output directory for the object instantiating, MicroAssembler class  requires additional information associated with the macroop and microops. I will explain the why they are necessary and how this information will be utilized in marcoop parsing soonCFG for ISA ParsingBefore explaining how macroop is parsed, I will go over few crucial rules that are necessary for parsing ISA files in most of the time. To understand how GEM5parses ISA files, the rules defined in the ISAParser python class is crucial. It also defines rules for tokenization fur lexer, but will not cover the details about lexer in this posting.#gem5/src/arch/isa_parser.py   def p_specification(self, t):       'specification : opt_defs_and_outputs top_level_decode_block'       for f in self.splits.iterkeys():           f.write('\\n#endif\\n')       for f in self.files.itervalues(): # close ALL the files;           f.close() # not doing so can cause compilation to fail       self.write_top_level_files()       t[0] = True   # 'opt_defs_and_outputs' is a possibly empty sequence of def and/or   # output statements. Its productions do the hard work of eventually   # instantiating a GenCode, which are generally emitted (written to disk)   # as soon as possible, except for the decode_block, which has to be   # accumulated into one large function of nested switch/case blocks.   def p_opt_defs_and_outputs_0(self, t):       'opt_defs_and_outputs : empty'   def p_opt_defs_and_outputs_1(self, t):       'opt_defs_and_outputs : defs_and_outputs'   def p_defs_and_outputs_0(self, t):       'defs_and_outputs : def_or_output'   def p_defs_and_outputs_1(self, t):       'defs_and_outputs : defs_and_outputs def_or_output'   # The list of possible definition/output statements.   # They are all processed as they are seen.   def p_def_or_output(self, t):       '''def_or_output : name_decl                        | def_format                        | def_bitfield                        | def_bitfield_struct                        | def_template                        | def_operand_types                        | def_operands                        | output                        | global_let                        | split'''The top rule is the specification. It means that content in the all isa filescan be interpreted as specification which can comprise of opt_defs_and_outputs and top_level_decode_block. We will mainly take a look at ‘opt_defs_and_outputs’because it can be further parsed down into macroop and microop blocks. As depictedin its grammar, it can be further parsed down to another defs_and_outputs and another def_or_output block. The def_or_output_block is the block that we willspecifically take a look at in detail in few sections.let {{ … }};The def_or_output_blocks can be further narrowed down into multiple different blocks, but the most important blocks related with defining macroops are starting with let.    def p_global_let(self, t):        'global_let : LET CODELIT SEMI'        self.updateExportContext()        self.exportContext[\"header_output\"] = ''        self.exportContext[\"decoder_output\"] = ''        self.exportContext[\"exec_output\"] = ''        self.exportContext[\"decode_block\"] = ''        self.exportContext[\"split\"] = self.make_split()        split_setup = '''def wrap(func):    def split(sec):        globals()[sec + '_output'] += func(sec)    return splitsplit = wrap(split)del wrap'''                              # This tricky setup (immediately above) allows us to just write        # (e.g.) \"split('exec')\" in the Python code and the split #ifdef's        # will automatically be added to the exec_output variable. The inner        # Python execution environment doesn't know about the split points,        # so we carefully inject and wrap a closure that can retrieve the        # next split's #define from the parser and add it to the current        # emission-in-progress.        try:            exec(split_setup+fixPythonIndentation(t[2]), self.exportContext)        except Exception as exc:            traceback.print_exc(file=sys.stdout)            if debug:                raise            error(t.lineno(1), 'In global let block: %s' % exc)        GenCode(self,                header_output=self.exportContext[\"header_output\"],                decoder_output=self.exportContext[\"decoder_output\"],                exec_output=self.exportContext[\"exec_output\"],                decode_block=self.exportContext[\"decode_block\"]).emit()When the parser comes across tokens composed of the “let “ block, it triggers the p_global_let function. The “let” block plays two vital roles in parsing ISA files. Initially, it executes Python code literal to declare the Python-based classes necessary for parsing. Subsequently, for generating CPP implementations of the ISA, it calls the GenCode function.GenCode: Generating CPP implementation for ISAclass GenCode(object):    # Constructor.    def __init__(self, parser,                 header_output = '', decoder_output = '', exec_output = '',                 decode_block = '', has_decode_default = False):        self.parser = parser        self.header_output = header_output        self.decoder_output = decoder_output        self.exec_output = exec_output        self.decode_block = decode_block        self.has_decode_default = has_decode_default    # Write these code chunks out to the filesystem.  They will be properly    # interwoven by the write_top_level_files().    def emit(self):        if self.header_output:            self.parser.get_file('header').write(self.header_output)        if self.decoder_output:            self.parser.get_file('decoder').write(self.decoder_output)        if self.exec_output:            self.parser.get_file('exec').write(self.exec_output)        if self.decode_block:            self.parser.get_file('decode_block').write(self.decode_block)During each iteration of the let block, header_output, decoder_output, exec_output, and decode_block are initially set to empty values before the let block is executed. The let block then fills these attributes as necessary. Additionally, the content stored in each attribute is generated through the execution of the let block, such as parsing the macroop. The GenCode class’s emit function write these automatically generated CPP implementations into files. We will see how decoder_output can be generated after parsing the macroop in the later section.Parsing X86 MacroopsIn this section, I will go over how GEM5 parses macroop definitions of X86 architecture. Regardless of the target architecture, the ISA parsing is initiatedby the same GEM5 provided function parse_isa_desc of the ISAParser class. Since most of the target architecture defines its ISA with multiple isa files, all list of the isa files that needs parsing will be usually specified in one isa file called main.isa.#gem5/src/arch/x86/isa/main.isa##include \"includes.isa\"namespace X86ISA;##include \"operands.isa\"##include \"bitfields.isa\"##include \"outputblock.isa\"##include \"formats/formats.isa\"##include \"microasm.isa\"......Upon opening the x86 architecture’s main.isa file, you’ll notice the inclusion of the microasm.isa file. This might cause confusion for readers anticipating isa files associated with macroops. Yet, considering that the X86 macroop is composed of one or more microops, assembling the microops to generate macroops becomes logical when you recall this relationship.#gem5/src/arch/x86/isa/microasm.isalet {    import sys    sys.path[0:0] = [\"src/arch/x86/isa/\"]    from insts import microcode    # print microcode    from micro_asm import MicroAssembler, Rom_Macroop    mainRom = X86MicrocodeRom('main ROM')    assembler = MicroAssembler(X86Macroop, microopClasses, mainRom, Rom_Macroop)    ......    assembler.symbols[\"fsw\"] = readFpReg(\"FSW\")    assembler.symbols[\"fcw\"] = readFpReg(\"FCW\")    assembler.symbols[\"ftw\"] = readFpReg(\"FTW\")        macroopDict = assembler.assemble(microcode)        decoder_output += mainRom.getDefinition()    header_output += mainRom.getDeclaration()As depicted in the let block, it first instantiates the MicroAssembler object.MicroAssembler class is responsible for parsing X86 macroops.Parsing a single macroop yields a Python class object that embodies the parsed macroop. Therefore, it requires the presence of Python class definitions capable of representing a single macroop, specifically, the X86Macroop Python class. Furthermore, given that a macroop comprises of multiple microops, details about the microops defined in the target architecture must be passed to the MicroAssembler, specifically, the microopClasses Python dictionary.X86Macroop The primary objective in creating an X86Macroop instance for a parsed macrooperation is to provide information about the parsed macroop. The macroop parsingplays a role in collecting all relevant information about the macroop, including its microops. It is noteworthy that the X86Macroop Python class provides a method named ‘add_microop.’ Also, the goal of the parsing the macroop is to generate a CPP implementation for that macroop, enabling execution by the processor pipeline. Therefore, the X86Macroop class provides several relevant functions generating CPP Implementations based on the collected information. I’ll soon elaborate on how this class proves beneficial in the automated generation of CPP implementations for macroops.#gem5/src/arch/x86/isa/macroop.isaclass X86Macroop(Combinational_Macroop):    def add_microop(self, mnemonic, microop):        microop.mnemonic = mnemonic        microop.micropc = len(self.microops)        self.microops.append(microop)    ......    def __init__(self, name):        super(X86Macroop, self).__init__(name)        self.directives = {            \"adjust_env\" : self.setAdjustEnv,            \"adjust_imm\" : self.adjustImm,            \"adjust_disp\" : self.adjustDisp,            \"serialize_before\" : self.serializeBefore,            \"serialize_after\" : self.serializeAfter,            \"function_call\" : self.function_call,            \"function_return\" : self.function_return        }        self.declared = False        self.adjust_env = \"\"        self.init_env = \"\"        self.adjust_imm = '''            uint64_t adjustedImm = IMMEDIATE;            //This is to pacify gcc in case the immediate isn't used.            adjustedImm = adjustedImm;        '''        self.adjust_disp = '''            uint64_t adjustedDisp = DISPLACEMENT;            //This is to pacify gcc in case the displacement isn't used.            adjustedDisp = adjustedDisp;        '''        self.serialize_before = False        self.serialize_after = False        self.function_call = False        self.function_return = False    def getAllocator(self, env):        return \"new X86Macroop::%s(machInst, %s)\" % \\                (self.name, env.getAllocator())    def getMnemonic(self):        mnemonic = self.name.lower()        mnemonic = re.match(r'[^_]*', mnemonic).group(0)        return mnemonic    def getDeclaration(self):        #FIXME This first parameter should be the mnemonic. I need to        #write some code which pulls that out        declareLabels = \"\"        for (label, microop) in self.labels.items():            declareLabels += \"const static uint64_t label_%s = %d;\\n\" \\                              % (label, microop.micropc)        iop = InstObjParams(self.getMnemonic(), self.name, \"Macroop\",                {\"code\" : \"\",                 \"declareLabels\" : declareLabels                })        return MacroDeclare.subst(iop);    def getDefinition(self, env):        #FIXME This first parameter should be the mnemonic. I need to        #write some code which pulls that out        numMicroops = len(self.microops)        allocMicroops = ''        micropc = 0        for op in self.microops:            flags = [\"IsMicroop\"]            if micropc == 0:                flags.append(\"IsFirstMicroop\")                if self.serialize_before:                    flags.append(\"IsSerializing\")                    flags.append(\"IsSerializeBefore\")            if micropc == numMicroops - 1:                flags.append(\"IsLastMicroop\")                if self.serialize_after:                    flags.append(\"IsSerializing\")                    flags.append(\"IsSerializeAfter\")                if self.function_call:                    flags.append(\"IsCall\")                    flags.append(\"IsUncondControl\")                if self.function_return:                    flags.append(\"IsReturn\")                    flags.append(\"IsUncondControl\")            else:                flags.append(\"IsDelayedCommit\")            allocMicroops += \\                \"microops[%d] = %s;\\n\" % \\                (micropc, op.getAllocator(flags))            micropc += 1        if env.useStackSize:            useStackSize = \"true\"        else:            useStackSize = \"false\"        if env.memoryInst:            memoryInst = \"true\"        else:            memoryInst = \"false\"        regSize = '''(%s || (env.base == INTREG_RSP &amp;&amp; %s) ?                     env.stackSize :                     env.dataSize)''' % (useStackSize, memoryInst)        iop = InstObjParams(self.getMnemonic(), self.name, \"Macroop\",                            {\"code\" : \"\", \"num_microops\" : numMicroops,                             \"alloc_microops\" : allocMicroops,                             \"adjust_env\" : self.adjust_env,                             \"adjust_imm\" : self.adjust_imm,                             \"adjust_disp\" : self.adjust_disp,                             \"disassembly\" : env.disassembly,                             \"regSize\" : regSize,                             \"init_env\" : self.initEnv})        return MacroConstructor.subst(iop) + \\               MacroDisassembly.subst(iop);microopClasses As one macroop can comprise multiple microops, the process of parsing a macroop is intricately connected to parsing the microops that make up the macroop. Consequently, the parser needs to be aware of the microops present in the targetarchitecture. This underscores the importance of passing the microopClasses Python dictionary to the MicroAssembler.#src/arch/x86/isa/microops/base.isalet {    # This will be populated with mappings between microop mnemonics and    # the classes that represent them.    microopClasses = {}};This Python dictionary containing pairs of microop mnemonic strings and theircorresponding class definitions. It’s crucial to highlight that each microop operation is expressed as a distinct Python class. The microopClasses dictionary, once provided, will be assigned to the ‘microops’ attribute of the MicroAssembler.Then how this dictionary is generated?#gem5/src/arch/x86/isa/microops/limop.isalet {    class LimmOp(X86Microop):    ......    microopClasses[\"limm\"] = LimmOp                                             As depicted in the code, every isa file defines the classes that can be employed to represent a microop. Additionally, it generates an entry to the microopClasses dictionary, linking the string identifier of the microop to the corresponding class that represents the microop. In the above example, generated dictionary in the microopClasses associates string “limm” to class “LimOp”.assemble function and microcodeParsing the actual macroop and generating X86Macroop objects for each macroop is accomplished by calling the assemble function of the MicroAssembler class.class MicroAssembler(object):    ......    def assemble(self, asm):        self.parser.parse(asm, lexer=self.lexer)        macroops = self.parser.macroops        self.parser.macroops = {}        return macroopsAs illustrated in the code, it parses the macroop definitions written in GEM5 DSLand retrieves the macroops. The macroops returned are Python objects, each being an instance of X86Macroop.#gem5/src/arch/x86/isa/microasm.isa                                                                                                                             let {                                                                               ......    macroopDict = assembler.assemble(microcode)                                 It’s important to highlight that the assemble function also receives microcode. Despite the potentially confusing name, it’s essential to clarify that ‘microcode’refers to an extensive concatenated Python string containing all macroopdefinitions in GEM5 DSL syntax. These microcode strings are aggregated from individual Python files found in the ‘isa/insts’ directory, where architecture-specific macroops are defined.# src/arch/x86/isa/inst/general_purpose/data_transfer/move.pymicrocode = '''## Regular moves#def macroop MOV_R_MI {    limm t1, imm, dataSize=asz    ld reg, seg, [1, t0, t1]};def macroop MOV_MI_R {    limm t1, imm, dataSize=asz    st reg, seg, [1, t0, t1]};As specified in the code, a microcode string is defined for each macroop mnemonic,encapsulating the macroop definitions implemented in GEM5 DSL. Given that these microcode strings are spread across individual ISA files for each macroop, it isnecessary to gather all the microcode strings from multiple ISA files.#gem5/src/arch/x86/isa/insts/general_purpose/data_transfer/__init__.pycategories = [\"conditional_move\",              \"move\",              \"stack_operations\",              \"xchg\"]microcode = \"\"for category in categories:    exec \"import %s as cat\" % category    microcode += cat.microcodeEvery ISA file defining the macrooperations for the x86 architecture is located under the ‘src/arch/x86/isa/insts/’ directory, categorized based on the instruction type. The ‘init.py’ file in each category directory collects the microcode string from Python files located in that directory. As depicted in the code, it specifies the Python files defining the macroop and imports the listedPython files to import microcode string. These strings are concatanated into the microcode string. Consequently, the microcode will have all macroop definitionsimplemented in GEM5 DSL.Context-free grammar for def macroop# gem5/src/arch/x86/isa/insts/general_purpose/data_transfer/move.py                                                                                             microcode = '''                                                                 def macroop MOV_R_MI {                                                              limm t1, imm, dataSize=asz                                                      ld reg, seg, [1, t0, t1]                                                    };    Upon opening any Python file that defines a macroop, you’ll readily observe that each macroop definition begins with def macroop. Like the previously encountered let block in ISA parsing, this structure is a component of the GEM5 DSL syntax associated with macroop definition. Therefore, corresponding CFG should be provided to parse the def macrop block.#gem5/src/arch/micro_asm.py# Defines a macroop that is combinationally generateddef p_macroop_def_1(t):    'macroop_def : DEF MACROOP ID block SEMI'    try:        curop = t.parser.macro_type(t[3])    except TypeError:        print_error(\"Error creating macroop object.\")        raise    for statement in t[4].statements:        handle_statement(t.parser, curop, statement)    t.parser.macroops[t[3]] = curopUpon encountering the def macroop block during microcode parsing, it invokes one of the p_macroop_def_X functions depending on the format of the macroop block, determined by the tokens within the def block. Given that the macroop block can assume various formats, the corresponding CFG precisely matching the macroop block is invoked. In the specific case we’re examining, which is the def macroop MOV_R_MI block, the p_macroop_def_1 function will be called.At line 7, it’s noticeable that t[3], representing the mnemonic of the currently parsed macroop, is passed to invoke macro_type. It’s important tonote that the macrop_type attribute of the MicroAssembler classhas been set as the X86Macroop Python class object. Consequently, t.parser.macro_type(t[3]) translates to X86Macroop(MOV_R_MI), leading tothe invocation of the constructor call for the X86Macroop object. This implies that whenever a def macroop block is encountered during parsing, it transforms the def block into an instance of the X86Macroop Python object.Block token: microops consisting of macroopYou may remember that a single X86Macroop object can encapsulate all the details about a specific macroop operation, including its constituent microops. However,immediately after populating the macroop instance, it lacks information about themicroops composing the current macroop class instance.class Block(object):    def __init__(self):        self.statements = []......# A block of statementsdef p_block(t):    'block : LBRACE statements RBRACE'    block = Block()    block.statements = t[2]    t[0] = blockAs outlined in the rule for block p_block, the parser proceeds to parse thesubsequent statements within the def macroop block and translates each microopoperations into the corresponding microop classes. Referring back to the earlierdefinition of a macroop, it becomes clear that the ‘block’ token represents a collection of multiple lines of microops, forming a single macroop. In detail,when the parser encounters the ‘block’ token, it instantiates the ‘block’ classobject and subsequently adds the parsed statements (microops in string) to thestatement field of the block instance.def p_statements_0(t):    'statements : statement'    if t[1]:        t[0] = [t[1]]    else:        t[0] = []def p_statements_1(t):    'statements : statements statement'    if t[2]:        t[1].append(t[2])    t[0] = t[1]def p_statement(t):    'statement : content_of_statement end_of_statement'    t[0] = t[1]# A statement can be a microop or an assembler directivedef p_content_of_statement_0(t):    '''content_of_statement : microop                            | directive'''    t[0] = t[1]# Ignore empty statementsdef p_content_of_statement_1(t):    'content_of_statement : '    pass# Statements are ended by newlines or a semi colondef p_end_of_statement(t):    '''end_of_statement : NEWLINE                        | SEMI'''    passThe statement is a string that comprises one or more microoperations. Therefore, each microop within the statement needs further parsing, necessitating anotherCFG for parsing rules. As depicted in the code, the statements can be furtheranalyzed as either microop or directives.class Statement(object):    def __init__(self):        self.is_microop = False        self.is_directive = False        self.params = \"\"class Microop(Statement):    def __init__(self):        super(Microop, self).__init__()        self.mnemonic = \"\"        self.labels = []        self.is_microop = True# Different flavors of microop to avoid shift/reduce errorsdef p_microop_0(t):    'microop : labels ID'    microop = Microop()    microop.labels = t[1]    microop.mnemonic = t[2]    t[0] = microopdef p_microop_3(t):    'microop : ID PARAMS'    microop = Microop()    microop.mnemonic = t[1]    microop.params = t[2]    t[0] = microopGEM5 establishes diverse grammar rules for parsing microops in different formats.For instance, the ‘limm t1, imm, dataSize=asz’ microop, which is part of the MOV_R_MI macroop, conforms to the ‘p_microop_3 grammar rule’. When the parser comes across a ‘microop’ statement containing a microop ID (mnemonic) and PARAMS (parameters of the microop), it generates a ‘Microop’ instance. Subsequently, the parser assigns the parsed mnemonic and parameters to their respective fields within the ‘Microop’ object.In the context of PLY, t[0] is used to assign the result of the parsing rule. In this case, t[0] = microop means that the parsed result of the ‘microop’ rule is set to the Microop instance created (microop). Therefore, the parsing result of the microop will be stored in the statements attribute of the block. Moreover, by tracing the call stack of the parsing rules, you can discover its utilization within the p_macroop_def_1 rule. Let’s take a closer look!handle_statements- handling microopsJust like the macroop is converted into a ‘X86Macroop’ Python object, its microop, which is stored in the statements attribute of the block, needs to undergo transformation into its respective Python classes.def p_macroop_def_1(t):                                                             'macroop_def : DEF MACROOP ID block SEMI'                                       .....    for statement in t[4].statements:                                                   handle_statement(t.parser, curop, statement)    Within the macroop parsing function, it iterates through each statement, where in this context, statements refer to the microops comprising the macroop. The ‘handle_statement’ function is then called for each statement. It’s important to highlight that this function requires both the ‘curop,’ which represents the currently parsed ‘X86Macroop’ object, and the ‘statement,’ which represents an individual microop.def handle_statement(parser, container, statement):    if statement.is_microop:        if statement.mnemonic not in parser.microops.keys():            raise Exception, \"Unrecognized mnemonic: %s\" % statement.mnemonic        parser.symbols[\"__microopClassFromInsideTheAssembler\"] = \\            parser.microops[statement.mnemonic]        try:            microop = eval('__microopClassFromInsideTheAssembler(%s)' %                    statement.params, {}, parser.symbols)        except:            print_error(\"Error creating microop object with mnemonic %s.\" % \\                    statement.mnemonic)            raise        try:            for label in statement.labels:                container.labels[label.text] = microop                if label.is_extern:                    container.externs[label.text] = microop            container.add_microop(statement.mnemonic, microop)        except:            print_error(\"Error adding microop.\")            raise    elif statement.is_directive:        if statement.name not in container.directives.keys():            raise Exception, \"Unrecognized directive: %s\" % statement.name        parser.symbols[\"__directiveFunctionFromInsideTheAssembler\"] = \\            container.directives[statement.name]        try:            eval('__directiveFunctionFromInsideTheAssembler(%s)' %                    statement.params, {}, parser.symbols)        except:            print_error(\"Error executing directive.\")            print(container.directives)            raise    else:        raise Exception, \"Didn't recognize the type of statement\", statementEvery statement can be categorized as either a ‘microop’ or a ‘directive,’ andthe processing of each statement is contingent upon its type. As illustrated inthe code, the microop dictionary is employed to look up the Pythonclass linked to the current microop mnemonic. X86 architecture defines microops using the GEM5 DSL, and it is parsed into python corredponding python classsesmatching with each microop. I will go over how these microops are implemented in GEM5 DSL in the next posting. Anyway, the class retrieved from this dictionary is then stored in the symbols attribute of the parser. It’s crucial to emphasize that the parsed microop’s mnemonic field (i.e., ‘statement.mnemonic’) is utilized to identify the corresponding reference to the microop class.The eval statement is employed to create an instance of the microop class. This involves utilizing the previously obtained microop class and passing the ‘statement.params’ as an argument to the ‘eval’ function. The argument encompassesall microop operands that follow a microop mnemonic. For instance, in the ‘MOV_R_MI’macroop definition, the ‘limm’ microop needs operands ‘t1, imm, dataSize=asz’.Before executing the ‘eval’ function, the microop’s operands are initially formatted as a string. However, since eval utilizes ‘parser.symbols’ as its local mapping dictionary, these string operands undergo transformation into code statements comprehensible by the microop classes. For example, the initialoperand ‘t1’ is translated into ‘InstRegIndex(NUM_INTREGS+1)’ as a result of the ‘eval.’ Subsequently, these translated operands are supplied to the constructor of the corresponding microop class based on the microop’s mnemonic. If you are curious about how this translation can happen please refer to appendixdef handle_statement(parser, container, statement):    if statement.is_microop:        ......        try:             for label in statement.labels:                container.labels[label.text] = microop                if label.is_extern:                    container.externs[label.text] = microop            container.add_microop(statement.mnemonic, microop)        except:            print_error(\"Error adding microop.\")            raiseThe handle_statement function creates Python class objects for parsed microops, comprising the macroop. These objects should be incorporated into the Python object of the corresponding macroop. In the provided code, the container refersto the Python object of the macroop, which is an instance of X86Macroop.After the successful parsing of a macroop, the corresponding macroop container is saved in the ‘parser.macroops’ dictionary. It will be returned to the assemble function and stored in the ‘macroopDict’ attribute.Translating parsed macroop into C++ class   The parsed macroops and its microops are instantiated as Python objects, not C++ class instances.The parsing outcome produces a Python dictionary containing all the macroop, each represented as X86Macroop objects. Additionally, the microops within each macroop are instances of Python classes stored within the respective macroop object. It is crucial to emphasize that GEM5 necessitates C++ classes to simulateboth macroops and microops, rather than Python objects. Therefore, GEM5 should generate CPP implmentations based on the parsed information! Let’s delve into how the automatic translation of Python objects into C++ classes can be achieved,using the ‘MOV_R_MI’ class as an illustrative example.#gem5/build/X86/arch/x86/generated/decoder-ns.cc.inc// Inst::MOV(['rAb', 'Ob'],{})        X86Macroop::MOV_R_MI::MOV_R_MI(                ExtMachInst machInst, EmulEnv _env)            : Macroop(\"mov\", machInst, 2, _env)        {            ;                uint64_t adjustedImm = IMMEDIATE;                //This is to pacify gcc in case the immediate isn't used.                adjustedImm = adjustedImm;            ;                uint64_t adjustedDisp = DISPLACEMENT;                //This is to pacify gcc in case the displacement isn't used.                adjustedDisp = adjustedDisp;            ;            env.setSeg(machInst);;        _numSrcRegs = 0;        _numDestRegs = 0;        _numFPDestRegs = 0;        _numVecDestRegs = 0;        _numVecElemDestRegs = 0;        _numVecPredDestRegs = 0;        _numIntDestRegs = 0;        _numCCDestRegs = 0;;            const char *macrocodeBlock = \"MOV_R_MI\";            //alloc_microops is the code that sets up the microops            //array in the parent class.            microops[0] =                (env.addressSize &gt;= 4) ?                    (StaticInstPtr)(new LimmBig(machInst,                        macrocodeBlock, (1ULL &lt;&lt; StaticInst::IsMicroop) | (1ULL &lt;&lt; StaticInst::IsFirstMicroop) | (1ULL &lt;&lt; StaticInst::IsDelayedCommit), InstRegIndex(NUM_INTREGS+1), adjustedImm,                        env.addressSize)) :                    (StaticInstPtr)(new Limm(machInst,                        macrocodeBlock, (1ULL &lt;&lt; StaticInst::IsMicroop) | (1ULL &lt;&lt; StaticInst::IsFirstMicroop) | (1ULL &lt;&lt; StaticInst::IsDelayedCommit), InstRegIndex(NUM_INTREGS+1), adjustedImm,                        env.addressSize))            ;microops[1] =                (env.dataSize &gt;= 4) ?                    (StaticInstPtr)(new LdBig(machInst,                        macrocodeBlock, (1ULL &lt;&lt; StaticInst::IsMicroop) | (1ULL &lt;&lt; StaticInst::IsLastMicroop), 1, InstRegIndex(NUM_INTREGS+0),                        InstRegIndex(NUM_INTREGS+1), 0, InstRegIndex(env.seg), InstRegIndex(env.reg),                        env.dataSize, env.addressSize, 0 | (machInst.legacy.addr ? (AddrSizeFlagBit &lt;&lt; FlagShift) : 0))) :                    (StaticInstPtr)(new Ld(machInst,                        macrocodeBlock, (1ULL &lt;&lt; StaticInst::IsMicroop) | (1ULL &lt;&lt; StaticInst::IsLastMicroop), 1, InstRegIndex(NUM_INTREGS+0),                        InstRegIndex(NUM_INTREGS+1), 0, InstRegIndex(env.seg), InstRegIndex(env.reg),                        env.dataSize, env.addressSize, 0 | (machInst.legacy.addr ? (AddrSizeFlagBit &lt;&lt; FlagShift) : 0)))            ;;        }The class above is generated automatically. Given that we already possess Python-based macroop containers defining the ‘MOV_R_MI’ macroop, it follows logically that the resulting C++ class is derived from the corresponding Pythonclass. Since all parsed macroop objects are accessible from the ‘macroopDict,’ it would be natural to investigate where the ‘macroopDict’ is employed to comprehend the translation process.#gem5/src/arch/x86/isa/macroop.isalet {    doModRMString = \"env.doModRM(machInst);\\n\"    noModRMString = \"env.setSeg(machInst);\\n\"    def genMacroop(Name, env):        blocks = OutputBlocks()        if not Name in macroopDict:            raise Exception, \"Unrecognized instruction: %s\" % Name        macroop = macroopDict[Name]        if not macroop.declared:            if env.doModRM:                macroop.initEnv = doModRMString            else:                macroop.initEnv = noModRMString            blocks.header_output = macroop.getDeclaration()            blocks.decoder_output = macroop.getDefinition(env)            macroop.declared = True        blocks.decode_block = \"return %s;\\n\" % macroop.getAllocator(env)        return blocks};As illustrated in the provided code, the ‘genMacroop’ function retrieves themacroop object associated with the name of a particular macroop. Subsequently, it invokes the ‘getDeclaration,’ ‘getDefinition,’ and ‘getAllocator’ functions of the obtained macroop object. It is important to note that the object obtained in this process is an instance of the ‘X86Macroop’ Python class, specifically linked to the ‘MOV_R_MI’ macroop.getDefinition: generating C++ class definition for macroop#gem5/src/arch/x86/isa/macroop.isa    class X86Macroop(Combinational_Macroop):        ......        def getDefinition(self, env):            numMicroops = len(self.microops)            allocMicroops = ''            micropc = 0            for op in self.microops:                flags = [\"IsMicroop\"]                if micropc == 0:                    flags.append(\"IsFirstMicroop\")                                if self.serialize_before:                        flags.append(\"IsSerializing\")                        flags.append(\"IsSerializeBefore\")                                    if micropc == numMicroops - 1:                    flags.append(\"IsLastMicroop\")                    if self.serialize_after:                        flags.append(\"IsSerializing\")                        flags.append(\"IsSerializeAfter\")                        if self.function_call:                        flags.append(\"IsCall\")                        flags.append(\"IsUncondControl\")                    if self.function_return:                        flags.append(\"IsReturn\")                        flags.append(\"IsUncondControl\")                    if self.control_direct:                        flags.append(\"IsDirectControl\")                    if self.control_indirect:                        flags.append(\"IsIndirectControl\")                else:                    flags.append(\"IsDelayedCommit\")                        allocMicroops += \\                    \"microops[%d] = %s;\\n\" % \\                    (micropc, op.getAllocator(flags))                micropc += 1            if env.useStackSize:                useStackSize = \"true\"            else:                useStackSize = \"false\"            if env.memoryInst:                memoryInst = \"true\"            else:                memoryInst = \"false\"            regSize = '''(%s || (env.base == INTREG_RSP &amp;&amp; %s) ?                         env.stackSize :                         env.dataSize)''' % (useStackSize, memoryInst)            iop = InstObjParams(self.getMnemonic(), self.name, \"Macroop\",                                {\"code\" : \"\", \"num_microops\" : numMicroops,                                 \"alloc_microops\" : allocMicroops,                                 \"adjust_env\" : self.adjust_env,                                 \"adjust_imm\" : self.adjust_imm,                                 \"adjust_disp\" : self.adjust_disp,                                 \"disassembly\" : env.disassembly,                                 \"regSize\" : regSize,                                 \"init_env\" : self.initEnv})            return MacroConstructor.subst(iop) + \\                   MacroDisassembly.subst(iop);The primary objective of the getDefinition function is to produce the CPP classimplementation for the macroop. In the context of X86, the semantics of a macroop are defined by the microops that compose it. Consequently, it is essential for the macroop’s constructor to properly initialize its microops consisting of the macroop.  The Python classes representing microops, which together form a macroop, arestored in the self.microops attribute of the X86Macroop objectConsidering that a macroop can comprise multiple microops, it is imperative to iterate through each Python microop class separately to generate the instantiation code for the corresponding C++ microop classes. It’s important to highlight thatin the provided code, the variable op represents each Python class object of a microop constituting the macroop.Just as each macroop can be translated into a CPP class, all microops are similarlytransformed into CPP. Consequently, to instantiate CPP class objects for the microops constituting the current macroop, it is necessary to determine whichCPP class of the microops should be instantiated. The code responsible for creatingan instance of each microop CPP class is retrieved from the getAllocator method of each Python microop class. The resulting code, which instantiates the microops,is stored in the allocMicroops variable.getAllocator: retrieve microop class instantiation codeThe getAllocator function, within the microop Python class, is responsible forgenerating C++ statements that create instances of C++ microop class objects. In our specific case, where our interest lies in the Limm microcode of the MOV_R_MI macroop, we will delve into how the getAllocator function of the LimmOp class is responsible for generating the corresponding C++ code.#gem5/src/arch/x86/isa/microops/limmop.isa106     class LimmOp(X86Microop):...116         def getAllocator(self, microFlags):117             allocString = '''118                 (%(dataSize)s &gt;= 4) ?119                     (StaticInstPtr)(new %(class_name)sBig(machInst,120                         macrocodeBlock, %(flags)s, %(dest)s, %(imm)s,121                         %(dataSize)s)) :122                     (StaticInstPtr)(new %(class_name)s(machInst,123                         macrocodeBlock, %(flags)s, %(dest)s, %(imm)s,124                         %(dataSize)s))125             '''126             allocator = allocString % {127                 \"class_name\" : self.className,128                 \"mnemonic\" : self.mnemonic,129                 \"flags\" : self.microFlagsText(microFlags),130                 \"dest\" : self.dest, \"imm\" : self.imm,131                 \"dataSize\" : self.dataSize}132             return allocatorThe getAllocator function within the LimmOp Python class is responsible forgenerating a Python docstring called allocString, which includes C++ formattedcode for instantiating microop objects. It’s important to note that the string contains format specifiers that need to be replaced with the appropriate values of the microop. The majority of the substituted content is derived from the attributes of the Python microop class.When an instance of the LimmOp Python class is created, microop-specific operandslike ‘dest’ and ‘imm’ are passed to the constructor of the LimmOp Python class. Recall how ‘eval’ translates the microop’s operands and creates the associated Python object. For instance, the className is set as ‘Limm’. It indicates that there should be a corresponding ‘Limm’ C++ class that can be instantiated. The‘Limm’ C++ class is also automatically generated by GEM5.//gem5/build/X86/arch/x86/generated/decoder-ns.hh.inc    class Limm : public X86ISA::X86MicroopBase    {      protected:        const RegIndex dest;        const uint64_t imm;        const uint8_t dataSize;        RegIndex foldOBit;        std::string generateDisassembly(Addr pc,            const SymbolTable *symtab) const;      public:        Limm(ExtMachInst _machInst,                const char * instMnem,                uint64_t setFlags, InstRegIndex _dest,                uint64_t _imm, uint8_t _dataSize);        Fault execute(ExecContext *, Trace::InstRecord *) const;    };For further details about microop, please wait for next posting!Finalize Macroop class definition with template substitution#gem5/src/arch/x86/isa/macroop.isa                                              def template MacroConstructor {        X86Macroop::%(class_name)s::%(class_name)s(                ExtMachInst machInst, EmulEnv _env)            : %(base_class)s(\"%(mnemonic)s\", machInst, %(num_microops)s, _env)        {            %(adjust_env)s;            %(adjust_imm)s;            %(adjust_disp)s;            %(init_env)s;            %(constructor)s;            const char *macrocodeBlock = \"%(class_name)s\";            //alloc_microops is the code that sets up the microops            //array in the parent class.            %(alloc_microops)s;        }       };         def template MacroDisassembly {    std::string    X86Macroop::%(class_name)s::generateDisassembly(            Addr pc, const Loader::SymbolTable *symtab) const    {        std::stringstream out;        out &lt;&lt; mnemonic &lt;&lt; \"\\t\";        int regSize = %(regSize)s;        %(disassembly)s        // Shut up gcc.        regSize = regSize;        return out.str();    }};    class X86Macroop(Combinational_Macroop):             iop = InstObjParams(self.getMnemonic(), self.name, \"Macroop\",                                       {\"code\" : \"\", \"num_microops\" : numMicroops,                                      \"alloc_microops\" : allocMicroops,                                               \"adjust_env\" : self.adjust_env,                                                 \"adjust_imm\" : self.adjust_imm,                                                 \"adjust_disp\" : self.adjust_disp,                                               \"disassembly\" : env.disassembly,                                                \"regSize\" : regSize,                                                            \"init_env\" : self.initEnv})                                return MacroConstructor.subst(iop) + \\                                                 MacroDisassembly.subst(iop);         I haven’t covered the def template block yet, but when you look at the automatically generated macroop class definitions in CPP, it will have similar code skeleton presented in MacroConstructor. It is used for generating CPP class for macroop! Based on the passed parameter InstObjParams, it substitutesthe template and generate the CPP code! It’s worth noting that the placeholder %(alloc_microops)s gets substituted with the constructor code for the microop classes that were generated by the previous getAllocator method. The MacroDisassembly template is used to automatically introduce member functiongenerateDisassembly to automatically generated macroop class. If you are interested in def template block, please bear with me I will explain the details of the def template in the next post.AppendixSymbols of parserWhen discussing the translation of microop’s operands, I didn’t delve into the details of how this translation takes place. As mentioned earlier, ‘parser.symbols’ serves as a dictionary that associates particular strings withcorresponding code statements. The string argument of the microop comprises operands like ‘rax,’ ‘rbx,’ ‘t1,’ ‘t2,’ and so on. However, these operands mustbe converted into the appropriate code statements for register references, suchas ‘InstRegIndex(NUM_INTREGS+1).’ The mapping between one register to code referencing it is defined within the ‘parser.symbols’ dictionary.#gem5/src/arch/x86/isa/microasm.isa 61     def regIdx(idx): 62         return \"InstRegIndex(%s)\" % idx 63 64     assembler.symbols[\"regIdx\"] = regIdx 65 66     # Add in symbols for the microcode registers 67     for num in range(16): 68         assembler.symbols[\"t%d\" % num] = regIdx(\"NUM_INTREGS+%d\" % num) 69     for num in range(8): 70         assembler.symbols[\"ufp%d\" % num] = \\ 71             regIdx(\"FLOATREG_MICROFP(%d)\" % num) 72     # Add in symbols for the segment descriptor registers 73     for letter in (\"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"S\"): 74         assembler.symbols[\"%ss\" % letter.lower()] = \\ 75             regIdx(\"SEGMENT_REG_%sS\" % letter) 76 77     # Add in symbols for the various checks of segment selectors. 78     for check in (\"NoCheck\", \"CSCheck\", \"CallGateCheck\", \"IntGateCheck\", 79                   \"SoftIntGateCheck\", \"SSCheck\", \"IretCheck\", \"IntCSCheck\", 80                   \"TRCheck\", \"TSSCheck\", \"InGDTCheck\", \"LDTCheck\"): 81         assembler.symbols[check] = \"Seg%s\" % check 82 83     for reg in (\"TR\", \"IDTR\"): 84         assembler.symbols[reg.lower()] = regIdx(\"SYS_SEGMENT_REG_%s\" % reg) 85 86     for reg in (\"TSL\", \"TSG\"): 87         assembler.symbols[reg.lower()] = regIdx(\"SEGMENT_REG_%s\" % reg) 88 89     # Miscellaneous symbols 90     symbols = { 91         \"reg\" : regIdx(\"env.reg\"), 92         \"xmml\" : regIdx(\"FLOATREG_XMM_LOW(env.reg)\"), 93         \"xmmh\" : regIdx(\"FLOATREG_XMM_HIGH(env.reg)\"), 94         \"regm\" : regIdx(\"env.regm\"), 95         \"xmmlm\" : regIdx(\"FLOATREG_XMM_LOW(env.regm)\"), 96         \"xmmhm\" : regIdx(\"FLOATREG_XMM_HIGH(env.regm)\"), 97         \"mmx\" : regIdx(\"FLOATREG_MMX(env.reg)\"), 98         \"mmxm\" : regIdx(\"FLOATREG_MMX(env.regm)\"), 99         \"imm\" : \"adjustedImm\",100         \"disp\" : \"adjustedDisp\",101         \"seg\" : regIdx(\"env.seg\"),102         \"scale\" : \"env.scale\",103         \"index\" : regIdx(\"env.index\"),104         \"base\" : regIdx(\"env.base\"),105         \"dsz\" : \"env.dataSize\",106         \"asz\" : \"env.addressSize\",107         \"ssz\" : \"env.stackSize\"108     }109     assembler.symbols.update(symbols)As we examine the provided code snippet, which is a component of the symbol update process, we can observe that different register is mapped to different code that can reference that specific register. These symbols play a crucial role in converting strings into actual reference code.#gem5/src/arch/x86/isa/microops/limop.isa105 let {106     class LimmOp(X86Microop):107         def __init__(self, dest, imm, dataSize=\"env.dataSize\"):108             self.className = \"Limm\"109             self.mnemonic = \"limm\"110             self.dest = dest111             if isinstance(imm, (int, long)):112                 imm = \"ULL(%d)\" % imm113             self.imm = imm114             self.dataSize = dataSize115116         def getAllocator(self, microFlags):117             allocString = '''118                 (%(dataSize)s &gt;= 4) ?119                     (StaticInstPtr)(new %(class_name)sBig(machInst,120                         macrocodeBlock, %(flags)s, %(dest)s, %(imm)s,121                         %(dataSize)s)) :122                     (StaticInstPtr)(new %(class_name)s(machInst,123                         macrocodeBlock, %(flags)s, %(dest)s, %(imm)s,124                         %(dataSize)s))125             '''126             allocator = allocString % {127                 \"class_name\" : self.className,128                 \"mnemonic\" : self.mnemonic,129                 \"flags\" : self.microFlagsText(microFlags),130                 \"dest\" : self.dest, \"imm\" : self.imm,131                 \"dataSize\" : self.dataSize}132             return allocator133134     microopClasses[\"limm\"] = LimmOpIt’s worth noting that the ‘LimmOp’ Python class requires three operands, andthe actual microcode of ‘MOV_R_MI’ provides these three operands when utilizingthe ‘limm’ microop. As a result of the ‘eval’ function, the operands of the ‘limm’ microop are translated into a format that GEM5 can comprehend and are then passed to the ‘init’ definition of the ‘LimmOp’ class.The ‘LimmOp’ class object, which corresponds to the ‘limm’ microop operation used to implement the ‘MOV_R_MI’ macroop, is instantiated. This ‘LimmOp’ class object is subsequently stored within the ‘X86Macroop’ object of the ‘MOV_R_MI’ instruction, accomplished through the ‘add_microop’ definition of ‘X86Macroop.’What class object is used for microop class generation?It might be anticipated that the parsed microop instructions would be instantiated from a class with the same name as the microop mnemonic (microop opcode), like ‘Ld.’ However, upon closer examination, it becomes evident that a more general class is associated with the group of microops, and it doesn’t precisely match the microop mnemonic. For example, in the case of the ‘ld’ microop, the associated template class used is ‘LoadOp’ instead of ‘Ld’.Similarly, for the ‘limm’ microop instruction, an ‘LimmOp’ class will representthis microop. The explanation for this behavior can be found in the ‘let’ block.#gem5/src/arch/x86/isa/microops/limop.isalet {    class LimmOp(X86Microop):        def __init__(self, dest, imm, dataSize=\"env.dataSize\"):            self.className = \"Limm\"            self.mnemonic = \"limm\"            self.dest = dest            if isinstance(imm, (int, long)):                imm = \"ULL(%d)\" % imm            self.imm = imm            self.dataSize = dataSize        def getAllocator(self, microFlags):            allocString = '''                (%(dataSize)s &gt;= 4) ?                    (StaticInstPtr)(new %(class_name)sBig(machInst,                        macrocodeBlock, %(flags)s, %(dest)s, %(imm)s,                        %(dataSize)s)) :                    (StaticInstPtr)(new %(class_name)s(machInst,                        macrocodeBlock, %(flags)s, %(dest)s, %(imm)s,                        %(dataSize)s))            '''            allocator = allocString % {                \"class_name\" : self.className,                \"mnemonic\" : self.mnemonic,                \"flags\" : self.microFlagsText(microFlags),                \"dest\" : self.dest, \"imm\" : self.imm,                \"dataSize\" : self.dataSize}            return allocator    microopClasses[\"limm\"] = LimmOp    ......A key takeaway from the ‘let’ block above is that it associates the class‘LimmOp’ with its corresponding mnemonic (‘limm’) within the Python dictionary called ‘microopClasses’. Consequently, when the dictionary is queried using amicroop’s mnemonic, such as ‘limm,’ it will return the related Python class, ‘LimmOp.’"
  },
  
  {
    "title": "Gem5 Event Loop",
    "url": "/posts/gem5-event-loop/",
    "categories": "GEM5",
    "tags": "",
    "date": "2020-05-20 00:00:00 -0400",
    





    
    "snippet": "I covered how GEM5 configures the hardware parameters through the python script.Also, in this posting I explained details how python can instantiate the CPP classes that actually simulate hardware ...",
    "content": "I covered how GEM5 configures the hardware parameters through the python script.Also, in this posting I explained details how python can instantiate the CPP classes that actually simulate hardware components. Similar to this, actualsimulation loop is implemented in CPP, but the simulation is started from the python script.m5.instantiate()exit_event = m5.simulate()After instantiating the platform, it invokes ‘simulate’ function from m5 moduleto initiate simulation. Since actual simulation will be handled by the CPP, Let’s see how this python code will transfer execution control to the CPP implementation.Overview of simulation loopdef simulate(*args, **kwargs):    global need_startup            if need_startup:        root = objects.Root.getInstance()        for obj in root.descendants(): obj.startup()        need_startup = False                # Python exit handlers happen in reverse order.        # We want to dump stats last.        atexit.register(stats.dump)            # register our C++ exit callback function with Python        atexit.register(_m5.core.doExitCleanup)            # Reset to put the stats in a consistent state.        stats.reset()            if _drain_manager.isDrained():        _drain_manager.resume()                # We flush stdout and stderr before and after the simulation to ensure the    # output arrive in order.    sys.stdout.flush()    sys.stderr.flush()    sim_out = _m5.event.simulate(*args, **kwargs)    sys.stdout.flush()    sys.stderr.flush()The simulate function invokes startup function from all SimObjects instantiatedby the python script, if it needs startup. After some initialization, it invokesCPP simulate function through ‘_m5.event’ module. Remind that some CPP functionsrelated with simulation are exported to python through pybind in the very firstpart of GEM5 execution. This is time to say good bye to python! From now on, 99%of simulation code will be CPP! Be prepared to jumping into real hardware simulation logic!//gem5/src/sim/simulate.ccGlobalSimLoopExitEvent *simulate(Tick num_cycles){    // The first time simulate() is called from the Python code, we need to    // create a thread for each of event queues referenced by the    // instantiated sim objects.    static bool threads_initialized = false;    static std::vector&lt;std::thread *&gt; threads;    if (!threads_initialized) {        threadBarrier = new Barrier(numMainEventQueues);        // the main thread (the one we're currently running on)        // handles queue 0, so we only need to allocate new threads        // for queues 1..N-1.  We'll call these the \"subordinate\" threads.        for (uint32_t i = 1; i &lt; numMainEventQueues; i++) {            threads.push_back(new std::thread(thread_loop, mainEventQueue[i]));        }        threads_initialized = true;        simulate_limit_event =            new GlobalSimLoopExitEvent(mainEventQueue[0]-&gt;getCurTick(),                                       \"simulate() limit reached\", 0);    }    inform(\"Entering event queue @ %d.  Starting simulation...\\n\", curTick());    if (num_cycles &lt; MaxTick - curTick())        num_cycles = curTick() + num_cycles;    else // counter would roll over or be set to MaxTick anyhow        num_cycles = MaxTick;    simulate_limit_event-&gt;reschedule(num_cycles);    GlobalSyncEvent *quantum_event = NULL;    if (numMainEventQueues &gt; 1) {        if (simQuantum == 0) {            fatal(\"Quantum for multi-eventq simulation not specified\");        }        quantum_event = new GlobalSyncEvent(curTick() + simQuantum, simQuantum,                            EventBase::Progress_Event_Pri, 0);        inParallelMode = true;    }    // all subordinate (created) threads should be waiting on the    // barrier; the arrival of the main thread here will satisfy the    // barrier, and all threads will enter doSimLoop in parallel    threadBarrier-&gt;wait();    Event *local_event = doSimLoop(mainEventQueue[0]);    assert(local_event != NULL);    inParallelMode = false;    // locate the global exit event and return it to Python    BaseGlobalEvent *global_event = local_event-&gt;globalEvent();    assert(global_event != NULL);    GlobalSimLoopExitEvent *global_exit_event =        dynamic_cast&lt;GlobalSimLoopExitEvent *&gt;(global_event);    assert(global_exit_event != NULL);    //! Delete the simulation quantum event.    if (quantum_event != NULL) {        quantum_event-&gt;deschedule();        delete quantum_event;    }    return global_exit_event;}After going through some initialization related with threads, it invokes the main simulation loop ‘doSimLoop’.//gem5/src/sim/simulate.ccEvent *doSimLoop(EventQueue *eventq){    // set the per thread current eventq pointer    curEventQueue(eventq);    eventq-&gt;handleAsyncInsertions();    while (1) {        // there should always be at least one event (the SimLoopExitEvent        // we just scheduled) in the queue        assert(!eventq-&gt;empty());        assert(curTick() &lt;= eventq-&gt;nextTick() &amp;&amp;               \"event scheduled in the past\");        if (async_event &amp;&amp; testAndClearAsyncEvent()) {            // Take the event queue lock in case any of the service            // routines want to schedule new events.            std::lock_guard&lt;EventQueue&gt; lock(*eventq);            if (async_statdump || async_statreset) {                Stats::schedStatEvent(async_statdump, async_statreset);                async_statdump = false;                async_statreset = false;            }            if (async_io) {                async_io = false;                pollQueue.service();            }            if (async_exit) {                async_exit = false;                exitSimLoop(\"user interrupt received\");            }            if (async_exception) {                async_exception = false;                return NULL;            }        }        Event *exit_event = eventq-&gt;serviceOne();        if (exit_event != NULL) {            return exit_event;        }    }    // not reached... only exit is return on SimLoopExitEvent}The central simulation sequence in GEM5 is the “doSimLoop.” This loop persistsuntil it encounters an “exit_event” that signals the end of the simulation. In the event of program termination due to an unhandled fault, GEM5 schedules the “exit_event,” prompting the “doSimLoop” to conclude. Most of the cases, it willnot face the exit_event and process events through the serviceOne function of the EventQueue.EventQueue: managing all EventsEventQueue manages several functions to manage generated Events, such as inserting and deleting the Event object from the queue. The main simulation looputilize this Queue to simulate hardware events.  EventQueue class is defined as friend class of Event class so it can access private and protected members of the Event objects managed by the queue.serviceOne: handle scheduled eventBefore deviling into the details of event, to grab the idea about how GEM5 utilize this event for simulation, it would be helpful to go over below function.203 Event *204 EventQueue::serviceOne()205 {206     std::lock_guard&lt;EventQueue&gt; lock(*this);207     Event *event = head;208     Event *next = head-&gt;nextInBin;209     event-&gt;flags.clear(Event::Scheduled);210 211     if (next) {212         // update the next bin pointer since it could be stale213         next-&gt;nextBin = head-&gt;nextBin;214 215         // pop the stack216         head = next;217     } else {218         // this was the only element on the 'in bin' list, so get rid of219         // the 'in bin' list and point to the next bin list220         head = head-&gt;nextBin;221     }222 223     // handle action224     if (!event-&gt;squashed()) {225         // forward current cycle to the time when this event occurs.226         setCurTick(event-&gt;when());227 228         event-&gt;process();229         if (event-&gt;isExitEvent()) {230             assert(!event-&gt;flags.isSet(Event::Managed) ||231                    !event-&gt;flags.isSet(Event::IsMainQueue)); // would be silly232             return event;233         }234     } else {235         event-&gt;flags.clear(Event::Squashed);236     }237 238     event-&gt;release();239 240     return NULL;241 }The “serviceOne” function is responsible for processing events scheduled by simulated hardware components. Unless an event is marked as squashed, it proceedsto execute the task associated with that event by invoking the event’s process function. The process function describe the hardware logic that needsto be simulated.Other operations of EventQueueThe most important method of the EventQueue is serviceOne function. Becauseit actually executes the hardware simulation logic. 41 inline void 42 EventQueue::schedule(Event *event, Tick when, bool global) 43 { 44     assert(when &gt;= getCurTick()); 45     assert(!event-&gt;scheduled()); 46     assert(event-&gt;initialized()); 47  48     event-&gt;setWhen(when, this); 49  50     // The check below is to make sure of two things 51     // a. a thread schedules local events on other queues through the asyncq 52     // b. a thread schedules global events on the asyncq, whether or not 53     //    this event belongs to this eventq. This is required to maintain 54     //    a total order amongst the global events. See global_event.{cc,hh} 55     //    for more explanation. 56     if (inParallelMode &amp;&amp; (this != curEventQueue() || global)) { 57         asyncInsert(event); 58     } else { 59         insert(event); 60     } 61     event-&gt;flags.set(Event::Scheduled); 62     event-&gt;acquire(); 63  64     if (DTRACE(Event)) 65         event-&gt;trace(\"scheduled\"); 66 }To add a new event to the queue, rather than using the queue’s insert function directly, it is required to use the ‘schedule’ function. The ‘schedule’ functionis responsible for setting critical fields like ‘_when’ and the event’s flags (e.g., Event::Scheduled) during the insertion process. Additionally, it triggersthe ‘insert’ function to effectively place the new item into the queue.117 void118 EventQueue::insert(Event *event)119 {120     // Deal with the head case121     if (!head || *event &lt;= *head) {122         head = Event::insertBefore(event, head);123         return;124     }125 126     // Figure out either which 'in bin' list we are on, or where a new list127     // needs to be inserted128     Event *prev = head;129     Event *curr = head-&gt;nextBin;130     while (curr &amp;&amp; *curr &lt; *event) {131         prev = curr;132         curr = curr-&gt;nextBin;133     }134 135     // Note: this operation may render all nextBin pointers on the136     // prev 'in bin' list stale (except for the top one)137     prev-&gt;nextBin = Event::insertBefore(event, curr);138 }An interesting aspect to observe in the ‘insert’ function is how it arranges theinsertion of a new item in a specific order. Given that the EventQueue managesvarious events with distinct priorities scheduled at varying times, the sequencein which Events are organized within the queue plays a vital role in emulating events in cycle-accurate manner. To define the order, it needs a metric to compare two Event objects.415 inline bool416 operator&lt;(const Event &amp;l, const Event &amp;r)417 {418     return l.when() &lt; r.when() ||419         (l.when() == r.when() &amp;&amp; l.priority() &lt; r.priority());420 }421 422 inline bool423 operator&gt;(const Event &amp;l, const Event &amp;r)424 {425     return l.when() &gt; r.when() ||426         (l.when() == r.when() &amp;&amp; l.priority() &gt; r.priority());427 }428 429 inline bool430 operator&lt;=(const Event &amp;l, const Event &amp;r)431 {432     return l.when() &lt; r.when() ||433         (l.when() == r.when() &amp;&amp; l.priority() &lt;= r.priority());434 }435 inline bool436 operator&gt;=(const Event &amp;l, const Event &amp;r)437 {438     return l.when() &gt; r.when() ||439         (l.when() == r.when() &amp;&amp; l.priority() &gt;= r.priority());440 }441 442 inline bool443 operator==(const Event &amp;l, const Event &amp;r)444 {445     return l.when() == r.when() &amp;&amp; l.priority() == r.priority();446 }447 448 inline bool449 operator!=(const Event &amp;l, const Event &amp;r)450 {451     return l.when() != r.when() || l.priority() != r.priority();452 }As depicted in the code above, operator overloading offers a mechanism for comparing Event objects. This comparison involves assessing the timing of two distinct events, indicating when these events are scheduled to occur. Additionally,it evaluates the priority of events in cases where two events are scheduled to be executed during the same cycle.Event: the basic unit of execution on GEM5 simulationAs a cycle-level simulator, GEM5 simulates hardware logic for each cycle. Toenable the execution of specific logic at precise points in the cycle, it shouldbe able to know which hardware component needs to be simulated at which cycle. To this end, GEM5 asks each hardware component to generate event that describeswhich event should be simulated at which specific cycle. The generated events are managed by the EventQueue where the main simulation loop fetches the event from and execute simulation logic.Event classThe Event class defines fundamental operations necessary for the execution of GEM5 events, crucial for simulating architecture. Each hardware component can communicate with simulation loop through the Event.class Event : public EventBase, public Serializable{    friend class EventQueue;  private:    // The event queue is now a linked list of linked lists.  The    // 'nextBin' pointer is to find the bin, where a bin is defined as    // when+priority.  All events in the same bin will be stored in a    // second linked list (a stack) maintained by the 'nextInBin'    // pointer.  The list will be accessed in LIFO order.  The end    // result is that the insert/removal in 'nextBin' is    // linear/constant, and the lookup/removal in 'nextInBin' is    // constant/constant.  Hopefully this is a significant improvement    // over the current fully linear insertion.    Event *nextBin;    Event *nextInBin;    static Event *insertBefore(Event *event, Event *curr);    static Event *removeItem(Event *event, Event *last);    Tick _when;         //!&lt; timestamp when event should be processed    Priority _priority; //!&lt; event priority    Flags flags;    .....  public:    /*     * Event constructor     * @param queue that the event gets scheduled on     */    Event(Priority p = Default_Pri, Flags f = 0)        : nextBin(nullptr), nextInBin(nullptr), _when(0), _priority(p),          flags(Initialized | f)    {        assert(f.noneSet(~PublicWrite));#ifndef NDEBUG        instance = ++instanceCounter;        queue = NULL;#endif#ifdef EVENTQ_DEBUG        whenCreated = curTick();        whenScheduled = 0;#endif    }    virtual ~Event();    /// describing the event class.    virtual const char *description() const;    /// Dump the current event data    void dump() const;  public:    virtual void process() = 0;    /// Determine if the current event is scheduled    bool scheduled() const { return flags.isSet(Scheduled); }    /// Squash the current event    void squash() { flags.set(Squashed); }    /// Check whether the event is squashed    bool squashed() const { return flags.isSet(Squashed); }    /// See if this is a SimExitEvent (without resorting to RTTI)    bool isExitEvent() const { return flags.isSet(IsExitEvent); }    /// Check whether this event will auto-delete    bool isManaged() const { return flags.isSet(Managed); }    bool isAutoDelete() const { return isManaged(); }    /// Get the time that the event is scheduled    Tick when() const { return _when; }    /// Get the event priority    Priority priority() const { return _priority; }    //! If this is part of a GlobalEvent, return the pointer to the    //! Global Event.  By default, there is no GlobalEvent, so return    //! NULL.  (Overridden in GlobalEvent::BarrierEvent.)    virtual BaseGlobalEvent *globalEvent() { return NULL; }    void serialize(CheckpointOut &amp;cp) const override;    void unserialize(CheckpointIn &amp;cp) override;};When an event object is chosen from the queue, the main execution loop calls theprocess member function of that class. It’s important to highlight that the process function is declared as virtual, allowing child classes inheriting from the Event class to supply the necessary operations to simulate specific events. Additionally, the Event class has a member field named _when, which specifies the precise clock cycle at which the event should be simulated. 93 class EventBase 94 { 95   protected: 96     typedef unsigned short FlagsType; 97     typedef ::Flags&lt;FlagsType&gt; Flags; 98  99     static const FlagsType PublicRead    = 0x003f; // public readable flags100     static const FlagsType PublicWrite   = 0x001d; // public writable flags101     static const FlagsType Squashed      = 0x0001; // has been squashed102     static const FlagsType Scheduled     = 0x0002; // has been scheduled103     static const FlagsType Managed       = 0x0004; // Use life cycle manager104     static const FlagsType AutoDelete    = Managed; // delete after dispatch105     /**106      * This used to be AutoSerialize. This value can't be reused107      * without changing the checkpoint version since the flag field108      * gets serialized.109      */110     static const FlagsType Reserved0     = 0x0008;111     static const FlagsType IsExitEvent   = 0x0010; // special exit event112     static const FlagsType IsMainQueue   = 0x0020; // on main event queue113     static const FlagsType Initialized   = 0x7a40; // somewhat random bits114     static const FlagsType InitMask      = 0xffc0; // mask for init bits115 116   public:117     typedef int8_t Priority;118 119     /// Event priorities, to provide tie-breakers for events scheduled120     /// at the same cycle.  Most events are scheduled at the default121     /// priority; these values are used to control events that need to122     /// be ordered within a cycle.123 124     /// Minimum priority125     static const Priority Minimum_Pri =          SCHAR_MIN;126 127     /// If we enable tracing on a particular cycle, do that as the128     /// very first thing so we don't miss any of the events on129     /// that cycle (even if we enter the debugger).130     static const Priority Debug_Enable_Pri =          -101;131 132     /// Breakpoints should happen before anything else (except133     /// enabling trace output), so we don't miss any action when134     /// debugging.135     static const Priority Debug_Break_Pri =           -100;137     /// CPU switches schedule the new CPU's tick event for the138     /// same cycle (after unscheduling the old CPU's tick event).139     /// The switch needs to come before any tick events to make140     /// sure we don't tick both CPUs in the same cycle.141     static const Priority CPU_Switch_Pri =             -31;142 143     /// For some reason \"delayed\" inter-cluster writebacks are144     /// scheduled before regular writebacks (which have default145     /// priority).  Steve?146     static const Priority Delayed_Writeback_Pri =       -1;147 148     /// Default is zero for historical reasons.149     static const Priority Default_Pri =                  0;150 151     /// DVFS update event leads to stats dump therefore given a lower priority152     /// to ensure all relevant states have been updated153     static const Priority DVFS_Update_Pri =             31;154 155     /// Serailization needs to occur before tick events also, so156     /// that a serialize/unserialize is identical to an on-line157     /// CPU switch.158     static const Priority Serialize_Pri =               32;159 160     /// CPU ticks must come after other associated CPU events161     /// (such as writebacks).162     static const Priority CPU_Tick_Pri =                50;163 164     /// If we want to exit a thread in a CPU, it comes after CPU_Tick_Pri165     static const Priority CPU_Exit_Pri =                64;166 167     /// Statistics events (dump, reset, etc.) come after168     /// everything else, but before exit.169     static const Priority Stat_Event_Pri =              90;170 171     /// Progress events come at the end.172     static const Priority Progress_Event_Pri =          95;173 174     /// If we want to exit on this cycle, it's the very last thing175     /// we do.176     static const Priority Sim_Exit_Pri =               100;177 178     /// Maximum priority179     static const Priority Maximum_Pri =          SCHAR_MAX;180 };The EventBase class can be utilized for setting event priorities in GEM5. As GEM5 comprehensively simulates each system tick, multiple events can occur simultaneously in the same cycle. In such cases, the order of event processing depends on the event type and is influencedby the priority assigned to each event.How hardware component generates event?I explained that through the event each hardware component can communicate withmain simulation loop, especially providing the logic and time (clock cycle) specifying when the simulation should be processed. Then how each hardware component generates the event? Primarily, the Event class can be employed in two distinct manners. The first approach is creating a new class that inherits from the Event class and implementing the process method. This approach is particularly valuable when theEvent function needs additional arguments to run the process. However, for simpler functions that don’t mandate the creation of an additional class, GEM5provides the pre-defined class, EventFunctionWrapper.819 class EventFunctionWrapper : public Event820 {821   private:822       std::function&lt;void(void)&gt; callback;823       std::string _name;824 825   public:826     EventFunctionWrapper(const std::function&lt;void(void)&gt; &amp;callback,827                          const std::string &amp;name,828                          bool del = false,829                          Priority p = Default_Pri)830         : Event(p), callback(callback), _name(name)831     {832         if (del)833             setFlags(AutoDelete);834     }835 836     void process() { callback(); }837 838     const std::string839     name() const840     {841         return _name + \".wrapped_function_event\";842     }843 844     const char *description() const { return \"EventFunctionWrapped\"; }845 };As depicted above, when a callback function is passed to the constructor of theEventFunctionWrapper, it will be executed when the event is chosen by the emulation loop. It’s essential to note that the process function in this class simply invokes the provided callback.How to schedule generated event?We’ve seen that the generated event can be inserted to the queue through the schedule function of the queue. Then it would be reasonable to think that eachCPP classes simulating hardware component should have the access to the queue to schedule the event. However, you won’t locate a mention of the queue in the class; instead, you’ll discover that it simply calls the schedule() function.Embarrassingly, there is no definition for schedule function in the class! Yes it is inherited from another class!Similar to that all python classes representing hardware components are inherited from SimObject python class, all hardware component classes in CPP should be a child of SimObjectclass SimObject : public EventManager, public Serializable, public Drainable,                  public Stats::GroupHowever, still you won’t find the schedule member function in the SimObject class. Based on its declaration, we can understand that it inherits several other classses. Because schedule function register the Event to the EventQueue,it should be Event related with class that manages Event, EventManager.class EventManager{   protected:    /** A pointer to this object's event queue */    EventQueue *eventq;    public:    EventManager(EventManager &amp;em) : eventq(em.eventq) {}    EventManager(EventManager *em) : eventq(em-&gt;eventq) {}    EventManager(EventQueue *eq) : eventq(eq) {}        EventQueue *     eventQueue() const    {           return eventq;    }        void    schedule(Event &amp;event, Tick when)    {           eventq-&gt;schedule(&amp;event, when);    }        void    deschedule(Event &amp;event)    {           eventq-&gt;deschedule(&amp;event);    }        void    reschedule(Event &amp;event, Tick when, bool always = false)    {           eventq-&gt;reschedule(&amp;event, when, always);    }    ......    void setCurTick(Tick newVal) { eventq-&gt;setCurTick(newVal); }};Yes! EventManager defines the schedule function, and it schedules an Event objectto the EventQueue managed by the EventManager. Wait! Because it is a wrapper class of EventQueue to help any classes inheriting SimObject utilize the queue(e.g., scheduling event) it should have access to the EventQueue. When you lookat the constructor of the EventManager, it needs a pointer to EventQueue! Whenthe pointer is passed to the constructor, it will be set as member field eventq,and member functions of EventManager will utilize this EventQueue.SimObject::SimObject(const Params *p)    : EventManager(getEventQueue(p-&gt;eventq_index)),      Stats::Group(nullptr),      _params(p){#ifdef DEBUG    doDebugBreak = false;#endif    simObjectList.push_back(this);    probeManager = new ProbeManager(this);}As shown in constructor of SimObject, it initializes the EventManager with return value from getEventQueue function, which is the EventQueue pointer.Generating EventQueueAs GEM5 can have multiple mainEventQueue, EventQueue objects should be generated at runtime as much as it needs. The new EventQueue generation and its retrieval can be handled both by the ‘getEventQueue’ functionEventQueue *getEventQueue(uint32_t index){    while (numMainEventQueues &lt;= index) {        numMainEventQueues++;        mainEventQueue.push_back(            new EventQueue(csprintf(\"MainEventQueue-%d\", index)));    }    return mainEventQueue[index];}If existing number of mainEventQueue is smaller than the index, it generates new EventQueue. If not it will just return the indexed EventQueue. Okay everythinglooks good except one thing. Who set the eventq_index value? You might already noticed that it is from Param which is used to reference automatically generatedCPP struct from the python class. Based on this, you can know that this parameteris set by python beforehand.class SimObject(object):    # Specify metaclass.  Any class inheriting from SimObject will    # get this metaclass.    type = 'SimObject'    abstract = True    cxx_header = \"sim/sim_object.hh\"    cxx_extra_bases = [ \"Drainable\", \"Serializable\", \"Stats::Group\" ]    eventq_index = Param.UInt32(Parent.eventq_index, \"Event Queue Index\")class Root(SimObject):\t.....    # By default, root sim object and hence all other sim objects schedule    # event on the eventq with index 0.    eventq_index = 0By default, it is set as zero and make all SimObjects utilize the first eventqunless it is specified.    Event *local_event = doSimLoop(mainEventQueue[0]);Also, when you look at the invocation of doSimLoop, mainEventQueue[0] is passed as its paramter, which makes the simulation loop and all SimObjects presentinghardware components can communicate through the mainEventQueue[0].Event scheduling in actionNow all CPP classes inheriting SimObject can utilize schedule function to schedule any events to the queue so that GEM5 simulation loop (doSimLoop) can fetches the event and simulate hardware logic at designated clock cycle. Let’s take a look at the FullO3CPU class simulating O3 CPU pipeline as an example !template &lt;class Impl&gt;class FullO3CPU : public BaseO3CPU{     ......    EventFunctionWrapper tickEvent;    ......}template &lt;class Impl&gt;FullO3CPU&lt;Impl&gt;::FullO3CPU(DerivO3CPUParams *params)    : BaseO3CPU(params),      itb(params-&gt;itb),      dtb(params-&gt;dtb),      tickEvent([this]{ tick(); }, \"FullO3CPU tick\",      ......The constructor of the FullO3CPU class creates an instance of the tickEvent, which is an EventFunctionWrapper with lambda function calling tick function. This implies that when the tickEvent is scheduled and retrieved from the EventQueue, it will execute the tick() function.Tick! Tick! Tick!template &lt;class Impl&gt;voidFullO3CPU&lt;Impl&gt;::tick(){       DPRINTF(O3CPU, \"\\n\\nFullO3CPU: Ticking main, FullO3CPU.\\n\");    assert(!switchedOut());    assert(drainState() != DrainState::Drained);        ++numCycles;    updateCycleCounters(BaseCPU::CPU_STATE_ON);        //Tick each of the stages    fetch.tick();        decode.tick();        rename.tick();        iew.tick();        commit.tick();        // Now advance the time buffers    timeBuffer.advance();        fetchQueue.advance();    decodeQueue.advance();    renameQueue.advance();    iewQueue.advance();        activityRec.advance();        if (removeInstsThisCycle) {        cleanUpRemovedInsts();    }    if (!tickEvent.scheduled()) {        if (_status == SwitchedOut) {            DPRINTF(O3CPU, \"Switched out!\\n\");            // increment stat            lastRunningCycle = curCycle();        } else if (!activityRec.active() || _status == Idle) {            DPRINTF(O3CPU, \"Idle!\\n\");            lastRunningCycle = curCycle();            timesIdled++;        } else {            schedule(tickEvent, clockEdge(Cycles(1)));            DPRINTF(O3CPU, \"Scheduling next tick!\\n\");        }    }    if (!FullSystem)        updateThreadPriority();    tryDrain();}I will not cover the details of the tick function of O3 in this posting, but it simulate pipeline stage of O3 processor such as fetch, decode, rename, iew, and commit in the tick function. When the Event is fetched from the EventQueue in the serviceOne function, the Scheduled flag of the Event will be unset.Since the tick function should be invoked at every clock cycle (to push the pipe line), another event should be rescheduled to be occurred at next clock cycle. Therefore, the tick function simulating the O3CPU will be invoked at every clock cycle and simulate the entire processor pipeline!Initial activationTo start the CPU, initial tick event should be scheduled. I will not cover the details here, but if you are interested in it pleas take carefully look at the below functions !template &lt;class Impl&gt;voidO3ThreadContext&lt;Impl&gt;::activate(){    DPRINTF(O3CPU, \"Calling activate on Thread Context %d\\n\",            threadId());    if (thread-&gt;status() == ThreadContext::Active)        return;    thread-&gt;lastActivate = curTick();    thread-&gt;setStatus(ThreadContext::Active);    // status() == Suspended    cpu-&gt;activateContext(thread-&gt;threadId());}template &lt;class Impl&gt;voidFullO3CPU&lt;Impl&gt;::activateContext(ThreadID tid) {    assert(!switchedOut());    // Needs to set each stage to running as well.    activateThread(tid);    // We don't want to wake the CPU if it is drained. In that case,    // we just want to flag the thread as active and schedule the tick    // event from drainResume() instead.    if (drainState() == DrainState::Drained)        return;    // If we are time 0 or if the last activation time is in the past,    // schedule the next tick and wake up the fetch unit    if (lastActivatedCycle == 0 || lastActivatedCycle &lt; curTick()) {        scheduleTickEvent(Cycles(0));    ......template &lt;class Impl&gt;class FullO3CPU : public BaseO3CPU{     ......    void scheduleTickEvent(Cycles delay)    {        if (tickEvent.squashed())            reschedule(tickEvent, clockEdge(delay));        else if (!tickEvent.scheduled())            schedule(tickEvent, clockEdge(delay));    }"
  },
  
  {
    "title": "GEM5, from entry point to simulation loop",
    "url": "/posts/gem5-simobject/",
    "categories": "Gem5, SimObject, pybind11, metaclass",
    "tags": "",
    "date": "2020-05-19 00:00:00 -0400",
    





    
    "snippet": "Complexity of Gem5: mix of CPP and PythonWhen you first take a look at the GEM5 source code, it could be confusing because it has Python, CPP, and isa files which you might haven’t seen before. In ...",
    "content": "Complexity of Gem5: mix of CPP and PythonWhen you first take a look at the GEM5 source code, it could be confusing because it has Python, CPP, and isa files which you might haven’t seen before. In GEM5, Most of the simulation logic is implemented as a CPP, but it utilizes Python heavily for platform configuration and automatic generation ofCPP class from templates. For example, ISA files define hardware logic with Domain Specific Language (DSL) and will be translated into CPP classes with the help of Python. Forget about isa file in this posting! I will go over it in another blog posting.Since the GEM5 execution is a mixture of CPP functions and Python scripts, and there are duplicated names for classes and functions in both files, it would be tricky to understand how GEM5 works. This confusion even start from the program execution. Let’s see the gem5 execution command, particularly for simulating X86architecture. If this is first time of using GEM5 please read some articles fromGEM5 tutorialbefore reading this post.  build/X86/gem5.opt &lt;Python script for configuration&gt;The gem5.opt is a elf binary compiled from GEM5 CPP code base. However, to run simulation, GEM5 requires additional configuration script written in Python. This script specifies configuration of the platform you want to simulate with GEM5, which includes configuration of CPU, Memory, caches and buses. Wait, I told you most of the simulation logic is implemented as CPP, then why suddenlyPython script is necessary for configuring simulated platform?I couldn’t find any documents detailing the integration of Python and CPP for GEM5 implementation. In my opinion, the idea is to decouple hardware configuration from the main code base to avoid re-compilation cost. If a userdoesn’t introduce new component nor want to simulate another architecture, adjusting the configuration of the simulated platform would not need another compilation.Irrespective of the rationale, since GEM5 extensively utilizes both Python and C++, the execution of code involves frequent transitions between the two languages. Moreover, given that the class names and attributes are either identical or highly similar in both Python and C++, navigating the codebase can be quite confusing.Motivating Example# create the system we are going to simulate                                    system = System()                                                                                                                                               # Create a simple CPU                                                           system.cpu = TimingSimpleCPU()                                                  When examining the Python script passed to gem5.opt, you’ll notice that it callsvarious functions associated with hardware components. For instance, it invokes the TimingSimpleCPU function.class TimingSimpleCPU(BaseSimpleCPU):    type = 'TimingSimpleCPU'    cxx_header = \"cpu/simple/timing.hh\"    @classmethod    def memory_mode(cls):        return 'timing'    @classmethod    def support_take_over(cls):        return TrueGiven that it is Python code, it’s evident that this function is defined in Python. Indeed, as evident in the provided code, it is a Python function instantiating a TimingSimpleCPU class object. However, somewhat confusingly, you can also simultaneously find the CPP implementation for TimingSimpleCPU.class TimingSimpleCPU : public BaseSimpleCPU{  public:    TimingSimpleCPU(TimingSimpleCPUParams * params);    virtual ~TimingSimpleCPU();    void init() override;As mentioned earlier, the CPP implementation handles the actual hardware simulation, while the configuration is accomplished through the Python script. Consequently, it is necessary to translate Python class objects into CPP class objects to simulate the architecture. In this posting, I will explain how this transformation is accomplished in GEM5.Allowing Python to access CPP definitionsSince Python require access to classes and attributes implemented in different languages, CPP, a wrapper or helper is essential. GEM5 extensively employs pybind11 to facilitate Python scripts in accessing CPP-defined classes and structs. Detailed information about pybind11 is not covered in this post, so it is recommended to read pybind11 documentation, before proceeding with further reading._m5 module exporting CPP to Python  Please be aware that our current operations involve the execution of CPP main functions, not Python. The gem5.opt is an ELF binary compiled from CPP.GEM5 exports required CPP implementation as the _m5 Python module through pybind11. Additionally, the sub-modules are organized based on the categories of the C++ implementation.//src/sim/main.ccintmain(int argc, char **argv){    int ret;    // Initialize m5 special signal handling.    initSignals();#if PY_MAJOR_VERSION &gt;= 3    std::unique_ptr&lt;wchar_t[], decltype(&amp;PyMem_RawFree)&gt; program(        Py_DecodeLocale(argv[0], NULL),        &amp;PyMem_RawFree);    Py_SetProgramName(program.get());#else    Py_SetProgramName(argv[0]);#endif    // Register native modules with Python's init system before    // initializing the interpreter.    registerNativeModules();    // initialize embedded Python interpreter    Py_Initialize();    // Initialize the embedded m5 Python library    ret = EmbeddedPython::initAll();    if (ret == 0) {        // start m5        ret = m5Main(argc, argv);    }    // clean up Python intepreter.    Py_Finalize();    return ret;    }GEM5 defines EmbeddedPython class in CPP to provide all functions and attributes for developer to export through the pybind11. The EmbeddedPython::initAll function exposes the required C++ definitions for running the simulation. Upon export, it then executes the m5Main function, transferring control from the CPP to Python.// src/sim/init.ccEmbeddedPyBind::initAll(){    std::list&lt;EmbeddedPyBind *&gt; pending;    py::module m_m5 = py::module(\"_m5\");    m_m5.attr(\"__package__\") = py::cast(\"_m5\");    pybind_init_core(m_m5);    pybind_init_debug(m_m5);    pybind_init_event(m_m5);    pybind_init_stats(m_m5);    for (auto &amp;kv : getMap()) {        auto &amp;obj = kv.second;        if (obj-&gt;base.empty()) {            obj-&gt;init(m_m5);        } else {            pending.push_back(obj);        }    }    while (!pending.empty()) {        for (auto it = pending.begin(); it != pending.end(); ) {            EmbeddedPyBind &amp;obj = **it;            if (obj.depsReady()) {                obj.init(m_m5);                it = pending.erase(it);            } else {                ++it;            }        }    }#if PY_MAJOR_VERSION &gt;= 3    return m_m5.ptr();#endif}The Python-exported CPP implementations from the initAll module can be categorized into two groups. The initial category comprises CPP functions associated with general operations essential for simulation, including debugging, simulation loops,and statistical operations. The second category involves exporting a parameter struct designed for instantiating hardware components responsible for simulating the architecture.Pybind initialization for simulationFunctions named  “pybind_init_XXX” exports CPP implementations required for thesimulation. It exports CPP classes and functions relevant to specific categories as sub modules such as core, debug, event, and stats.pybind_init_event(py::module &amp;m_native){    py::module m = m_native.def_submodule(\"event\");    m.def(\"simulate\", &amp;simulate,          py::arg(\"ticks\") = MaxTick);    m.def(\"exitSimLoop\", &amp;exitSimLoop);    m.def(\"getEventQueue\", []() { return curEventQueue(); },          py::return_value_policy::reference);    m.def(\"setEventQueue\", [](EventQueue *q) { return curEventQueue(q); });    m.def(\"getEventQueue\", &amp;getEventQueue,          py::return_value_policy::reference);    py::class_&lt;EventQueue&gt;(m, \"EventQueue\")        .def(\"name\",  [](EventQueue *eq) { return eq-&gt;name(); })        .def(\"dump\", &amp;EventQueue::dump)        .def(\"schedule\", [](EventQueue *eq, PyEvent *e, Tick t) {                eq-&gt;schedule(e, t);            }, py::arg(\"event\"), py::arg(\"when\"))        .def(\"deschedule\", &amp;EventQueue::deschedule,             py::arg(\"event\"))        .def(\"reschedule\", &amp;EventQueue::reschedule,             py::arg(\"event\"), py::arg(\"tick\"), py::arg(\"always\") = false)        ;For example, Python function should be able to invoke CPP functions associated with hardware simulation because actual simulation is done by CPP not python. As depicted in the example, it exports simulation-related functions under sub-modules ‘_m5.event’. Later, the exported simulate function will be invokedfrom python to start hardware simulation.Pybind Initialization for HW componentsIn contrast to functions in the first category, which are already implemented inthe CPP code base of GEM5, certain CPP implementations are automatically generatedduring compile time. Consequently, exporting them is not feasible in the same manner as the first category, as the module name is unknown prior to generation. Additionally, if users incorporate extra hardware components, they must be added to the CPP class for proper exportation through Pybind. I will explain details about what CPP implementations will be automatically generated soon, so please bear with me.std::map&lt;std::string, EmbeddedPyBind *&gt; &amp;EmbeddedPyBind::getMap(){       static std::map&lt;std::string, EmbeddedPyBind *&gt; objs;    return objs;}voidEmbeddedPyBind::init(py::module &amp;m){    if (!registered) {        initFunc(m);        registered = true;    } else {        cprintf(\"Warning: %s already registered.\\n\", name);    }}EmbeddedPyBind class defines map ‘objs’ to manage all EmbeddedPyBind objects andreturn this map when the getMap function is invoked.  The initAll functioniterates this objects returned from getMap function and invokes ‘init’ function of the EmbeddedPyBind object. It further invokes initFunc which is a private function pointer member field of EmbeddedPyBind class.EmbeddedPyBind::EmbeddedPyBind(const char *_name,                               void (*init_func)(py::module &amp;),                               const char *_base)    : initFunc(init_func), registered(false), name(_name), base(_base){    getMap()[_name] = this;}EmbeddedPyBind::EmbeddedPyBind(const char *_name,                               void (*init_func)(py::module &amp;))    : initFunc(init_func), registered(false), name(_name), base(\"\"){    getMap()[_name] = this;}This function pointer is initialized by constructor of the EmbeddedPyBind class. However, you will not be able to find any relevant code instantiating EmbeddedPyBind for system component class. The reason is GEM5 automatically generate CPP code snippet, and EmbeddedPyBind class will be instantiated by that code. I will cover the details soon! Let’s assume that all required CPP implementations were exported to Python through pybind11.Transferring execution control to PythonAfter exporting CPP implementation, now it can finally jumps to GEM5 Python code base. Note that it will not execute the script initially passed to the gem5.opt executable.//src/sim/init.ccconst char * __attribute__((weak)) m5MainCommands[] = {    \"import m5\",    \"m5.main()\",    0 // sentinel is required};int m5Main(int argc, char **_argv){#if HAVE_PROTOBUF    // Verify that the version of the protobuf library that we linked    // against is compatible with the version of the headers we    // compiled against.    GOOGLE_PROTOBUF_VERIFY_VERSION;#endif            #if PY_MAJOR_VERSION &gt;= 3    typedef std::unique_ptr&lt;wchar_t[], decltype(&amp;PyMem_RawFree)&gt; WArgUPtr;    std::vector&lt;WArgUPtr&gt; v_argv;    std::vector&lt;wchar_t *&gt; vp_argv;    v_argv.reserve(argc);    vp_argv.reserve(argc);    for (int i = 0; i &lt; argc; i++) {        v_argv.emplace_back(Py_DecodeLocale(_argv[i], NULL), &amp;PyMem_RawFree);        vp_argv.emplace_back(v_argv.back().get());    }        wchar_t **argv = vp_argv.data();#else    char **argv = _argv;#endif    PySys_SetArgv(argc, argv);            // We have to set things up in the special __main__ module    PyObject *module = PyImport_AddModule(PyCC(\"__main__\"));    if (module == NULL)        panic(\"Could not import __main__\");    PyObject *dict = PyModule_GetDict(module);    // import the main m5 module    PyObject *result;    const char **command = m5MainCommands;    // evaluate each command in the m5MainCommands array (basically a    // bunch of Python statements.    while (*command) {        result = PyRun_String(*command, Py_file_input, dict, dict);        if (!result) {            PyErr_Print();            return 1;        }        Py_DECREF(result);        command++;    }#if HAVE_PROTOBUF    google::protobuf::ShutdownProtobufLibrary();#endif    return 0;}To transfer execution control to Python code, it invokes PyRun_String.PyRun_String is a function in the Python C API that allows you to execute a Python code snippet from a C program. It takes a string containing the Python code as one of its arguments and executes it within the Python interpreter.As depicted in the above CPP string, m5MainCommands, it will invoks m5.main through PyRun_String.GEM5 m5 main Python code  From this part, the execution is transferred to Python.//src/Python/m5/main.py def main(*args):    import m5        from . import core    from . import debug    from . import defines    from . import event    from . import info    from . import stats    from . import trace        from .util import inform, fatal, panic, isInteractive    from m5.util.terminal_formatter import TerminalFormatter        if len(args) == 0:        options, arguments = parse_options()    elif len(args) == 2:        options, arguments = args    else:        raise TypeError(\"main() takes 0 or 2 arguments (%d given)\" % len(args))        m5.options = options        # Set the main event queue for the main thread.    event.mainq = event.getEventQueue(0)    event.setEventQueue(event.mainq)    ......    sys.argv = arguments    sys.path = [ os.path.dirname(sys.argv[0]) ] + sys.path    filename = sys.argv[0]    filedata = open(filename, 'r').read()    filecode = compile(filedata, filename, 'exec')    scope = { '__file__' : filename,              '__name__' : '__m5_main__' }    if options.pdb:        import pdb        import traceback        pdb = pdb.Pdb()        try:            pdb.run(filecode, scope)        except SystemExit:            print(\"The program exited via sys.exit(). Exit status: \", end=' ')            print(sys.exc_info()[1])        except:            traceback.print_exc()            print(\"Uncaught exception. Entering post mortem debugging\")            t = sys.exc_info()[2]            while t.tb_next is not None:                t = t.tb_next                pdb.interaction(t.tb_frame,t)    else:        exec(filecode, scope)There are two important initialization code in the above Python code: initializing main event queue and execute Python snippet originally provided to gem5.opt. The event queue will be covered in another blog posting.You might remember that we have passed config script defining configuration of one platform we want to simulate. That Python script is passed to above Python code snippet through ‘sys.argv[0]’, and compiled and exec. Therefore, it will not return to CPP, but the execution control is transferredto configuration Python script!Python configuration to CPP implementation!To understand how Python configuration script interacts with CPP implementation in simulating one architecture, I will pick very simple configuration script provided by GEM5.# create the system we are going to simulate system = System() # Create a simple CPUsystem.cpu = TimingSimpleCPU()# Create a memory bus, a system crossbar, in this casesystem.membus = SystemXBar()# Hook the CPU ports up to the membussystem.cpu.icache_port = system.membus.slavesystem.cpu.dcache_port = system.membus.slave# Create a DDR3 memory controller and connect it to the membussystem.mem_ctrl = MemCtrl()system.mem_ctrl.dram = DDR3_1600_8x8()system.mem_ctrl.dram.range = system.mem_ranges[0]system.mem_ctrl.port = system.membus.masterIn the Python configuration script above, it creates instances of the CPU, crossbar, and DRAM. Additionally, it establishes the connection between the CPU and memory via the crossbar. As previously explained, GEM5 represents each hardware component as a CPP class for simulation. Consequently, the configurations defined in Python for the simulated platform need to be translatedinto CPP implementation to effectively simulate hardware logic. This involves transforming the hardware components and the interconnections between them.Start from relevance between python and CPP classCurrently, the instantiation process of CPP class objects by a Python class remains unclear. However, by examining the constructor function of the CPP class and the attributes of the Python class, we can make an informed assumption that the Python class is likely associated with the parameters needed for instantiating the CPP class object. Let’s explore this further!  CPP implementation of System classclass System(SimObject):    type = 'System'    cxx_header = \"sim/system.hh\"    system_port = RequestPort(\"System port\")    cxx_exports = [        PyBindMethod(\"getMemoryMode\"),        PyBindMethod(\"setMemoryMode\"),    ]    memories = VectorParam.AbstractMemory(Self.all,                                          \"All memories in the system\")    mem_mode = Param.MemoryMode('atomic', \"The mode the memory system is in\")    thermal_model = Param.ThermalModel(NULL, \"Thermal model\")    thermal_components = VectorParam.SimObject([],            \"A collection of all thermal components in the system.\")    # When reserving memory on the host, we have the option of    # reserving swap space or not (by passing MAP_NORESERVE to    # mmap). By enabling this flag, we accommodate cases where a large    # (but sparse) memory is simulated.    mmap_using_noreserve = Param.Bool(False, \"mmap the backing store \" \\                                          \"without reserving swap\")    # The memory ranges are to be populated when creating the system    # such that these can be passed from the I/O subsystem through an    # I/O bridge or cache    mem_ranges = VectorParam.AddrRange([], \"Ranges that constitute main memory\")    shared_backstore = Param.String(\"\", \"backstore's shmem segment filename, \"        \"use to directly address the backstore from another host-OS process. \"        \"Leave this empty to unset the MAP_SHARED flag.\")    cache_line_size = Param.Unsigned(64, \"Cache line size in bytes\")    ......  Python implementation of System class  System::System(Params *p)    : SimObject(p), _systemPort(\"system_port\", this),      multiThread(p-&gt;multi_thread),      pagePtr(0),      init_param(p-&gt;init_param),      physProxy(_systemPort, p-&gt;cache_line_size),      workload(p-&gt;workload),#if USE_KVM      kvmVM(p-&gt;kvm_vm),#else      kvmVM(nullptr),#endif      physmem(name() + \".physmem\", p-&gt;memories, p-&gt;mmap_using_noreserve,              p-&gt;shared_backstore),      memoryMode(p-&gt;mem_mode),      _cacheLineSize(p-&gt;cache_line_size),      workItemsBegin(0),      workItemsEnd(0),      numWorkIds(p-&gt;num_work_ids),      thermalModel(p-&gt;thermal_model),      _params(p),      _m5opRange(p-&gt;m5ops_base ?                 RangeSize(p-&gt;m5ops_base, 0x10000) :                 AddrRange(1, 0)), // Create an empty range if disabled      totalNumInsts(0),      redirectPaths(p-&gt;redirect_paths){  As an illustration, consider the attribute “cache_line_size” within the Python class. This attribute not only exists within the Python class but is also employedto initialize a _cacheLineSize member field of the CPP System class. Note that this attribute is accessed through the Param struct, which encapsulates all the configuration details of the hardware module necessary for instantiating it as a CPP class object.Automatic Param generationFrom the example, you might grab the idea of python script. It provides hardware configurations to instantiate hardware component for simulation! As each hardwarecomponent may need distinctive configurations, like cache line size or the number of buffer entries, Python classes gather all pertinent hardware configuration details and transmit them to the CPP implementation through the Param struct.Nevertheless, the CPP Param struct passed to the System constructor is not explicitly present in the codebase. This absence is attributed to the automatic generation of the Param struct for instantiating the System class object, transitioning seamlessly from Python class to CPP struct. Let’s examine the the automatically generated struct to gain a comprehensive understanding.struct SystemParams    : public SimObjectParams{    System * create();    ByteOrder byte_order;    unsigned cache_line_size;    bool exit_on_work_items;    uint64_t init_param;    KvmVM * kvm_vm;    Addr m5ops_base;    Enums::MemoryMode mem_mode;    std::vector&lt; AddrRange &gt; mem_ranges;    std::vector&lt; AbstractMemory * &gt; memories;    bool mmap_using_noreserve;    bool multi_thread;    int num_work_ids;    std::string readfile;    std::vector&lt; RedirectPath * &gt; redirect_paths;    std::string shared_backstore;    std::string symbolfile;    std::vector&lt; SimObject * &gt; thermal_components;    ThermalModel * thermal_model;    Counter work_begin_ckpt_count;    int work_begin_cpu_id_exit;    Counter work_begin_exit_count;    Counter work_cpus_ckpt_count;    Counter work_end_ckpt_count;    Counter work_end_exit_count;    int work_item_id;    Workload * workload;    unsigned int port_system_port_connection_count;};The SystemParams struct is automatically created and supplied to the constructorof the System class, facilitating the initialization of hardware parameters for the System component. Furthermore, the generated struct contains member fields that correspond to certain attributes of the System class in Python. I will explain which attributes of the python classes can be translated into CPP counterparts soon. Please bear with me!Automatic pybind generationAs the generated struct is implemented in CPP, it needs to be exported to Pythonso that it can configure the parameters based on the information provided by thePython configuration script.static voidmodule_init(py::module &amp;m_internal){    py::module m = m_internal.def_submodule(\"param_System\");    py::class_&lt;SystemParams, SimObjectParams, std::unique_ptr&lt;SystemParams, py::nodelete&gt;&gt;(m, \"SystemParams\")        .def(py::init&lt;&gt;())        .def(\"create\", &amp;SystemParams::create)        .def_readwrite(\"byte_order\", &amp;SystemParams::byte_order)        .def_readwrite(\"cache_line_size\", &amp;SystemParams::cache_line_size)        .def_readwrite(\"exit_on_work_items\", &amp;SystemParams::exit_on_work_items)        .def_readwrite(\"init_param\", &amp;SystemParams::init_param)        .def_readwrite(\"kvm_vm\", &amp;SystemParams::kvm_vm)        .def_readwrite(\"m5ops_base\", &amp;SystemParams::m5ops_base)        .def_readwrite(\"mem_mode\", &amp;SystemParams::mem_mode)        .def_readwrite(\"mem_ranges\", &amp;SystemParams::mem_ranges)        .def_readwrite(\"memories\", &amp;SystemParams::memories)        .def_readwrite(\"mmap_using_noreserve\", &amp;SystemParams::mmap_using_noreserve)        .def_readwrite(\"multi_thread\", &amp;SystemParams::multi_thread)        .def_readwrite(\"num_work_ids\", &amp;SystemParams::num_work_ids)        .def_readwrite(\"readfile\", &amp;SystemParams::readfile)        .def_readwrite(\"redirect_paths\", &amp;SystemParams::redirect_paths)        .def_readwrite(\"shared_backstore\", &amp;SystemParams::shared_backstore)        .def_readwrite(\"symbolfile\", &amp;SystemParams::symbolfile)        .def_readwrite(\"thermal_components\", &amp;SystemParams::thermal_components)        .def_readwrite(\"thermal_model\", &amp;SystemParams::thermal_model)        .def_readwrite(\"work_begin_ckpt_count\", &amp;SystemParams::work_begin_ckpt_count)        .def_readwrite(\"work_begin_cpu_id_exit\", &amp;SystemParams::work_begin_cpu_id_exit)        .def_readwrite(\"work_begin_exit_count\", &amp;SystemParams::work_begin_exit_count)        .def_readwrite(\"work_cpus_ckpt_count\", &amp;SystemParams::work_cpus_ckpt_count)        .def_readwrite(\"work_end_ckpt_count\", &amp;SystemParams::work_end_ckpt_count)        .def_readwrite(\"work_end_exit_count\", &amp;SystemParams::work_end_exit_count)        .def_readwrite(\"work_item_id\", &amp;SystemParams::work_item_id)        .def_readwrite(\"workload\", &amp;SystemParams::workload)        .def_readwrite(\"port_system_port_connection_count\", &amp;SystemParams::port_system_port_connection_count)        ;    py::class_&lt;System, SimObject, std::unique_ptr&lt;System, py::nodelete&gt;&gt;(m, \"System\")        .def(\"getMemoryMode\", &amp;System::getMemoryMode)        .def(\"setMemoryMode\", &amp;System::setMemoryMode)        ;}static EmbeddedPyBind embed_obj(\"System\", module_init, \"SimObject\");As illustrated in the provided code, GEM5 automatically generates “module_init” function that utilizes the pybind library to export the automatically generated Param struct and its member fields to Python. Additionally, it creates an EmbeddedPyBind object to register the automatically generated module_init function to the object. The module_init function is responsible for exporting the CPP implementation toPython, falling into the second category outlined in Pybind Initialization for HW components.Now you can understand why GEM5 main function exports the CPP implementation in two different ways.SimObject Python classTo understand how GEM5 automatically generate CPP Param struct and its pybind from Python classes, we should understand SimObject class and MetaSimObject metaclass. In GEM5, a SimObject represents a simulation entity and forms the basis for modeling components within the system being simulated. In other words, all system component related classes instantiated in the Python configuration script should be inherited from the SimObject class.# gem5/src/Python/m5/SimbObject.py@add_metaclass(MetaSimObject)class SimObject(object):    # Specify metaclass.  Any class inheriting from SimObject will    # get this metaclass.    type = 'SimObject'    abstract = TrueAlthough SimObject is the fundamental building block of GEM5 simulation in defining hardware components, its metaclass, MetaSibObject is the most importantin terms of bridging Python to CPP.MetaSimObject: metaclass of SimObjectWhen one Python class has metaclass, functions defined in metaclass can pre-process newly defined class. For example, attributes of Python class having metaclass can be audited by metaclasss functions such as ‘init, new or, call’. Since metaclass can access class and its attributes (dictionary), it can modify or introduce particular attribute and logic during the class instantiation. For further details of metaclass, please refer to Python-metaclass.#src/Python/m5/SimObject.pyclass MetaSimObject(type):    # Attributes that can be set only at initialization time    init_keywords = {        'abstract' : bool,        'cxx_class' : str,        'cxx_type' : str,        'cxx_header' : str,        'type' : str,        'cxx_base' : (str, type(None)),        'cxx_extra_bases' : list,        'cxx_exports' : list,        'cxx_param_exports' : list,        'cxx_template_params' : list,    }    # Attributes that can be set any time    keywords = { 'check' : FunctionType }Generate CPP Param struct and its pybindExamining the Python class for System reveals the presence of certain Python attributes unrelated to the CPP System class. Our task is to selectively filter out only those Python class attributes essential for instantiating their CPP class counterparts. If the notion of Python metaclass comes to mind, there’s noneed for an in-depth exploration. As mentioned earlier, Python metaclass providesaccess to all attributes of a class that has it as a metaclass, making it an ideal location to efficiently filter and extract only the necessary attributes.Before delving into how the metaclass aids in filtering out only the relevant attributes from the Python class, let’s first examine the Python code responsible for automatically generating the Param struct. This will provide insight into which attributes of the Python classes need to be filtered out to generate CPP code. Given that many Python-based automatic code generation processes involve string manipulation and substitution, locating the function can be done by searching for relevant logic within the generated CPP code.    def cxx_param_decl(cls, code):        params = list(map(lambda k_v: k_v[1], sorted(cls._params.local.items())))        ports = cls._ports.local        ......        if cls == SimObject:            code('''#include &lt;string&gt;''')                cxx_class = CxxClass(cls._value_dict['cxx_class'],                             cls._value_dict['cxx_template_params'])                # A forward class declaration is sufficient since we are just        # declaring a pointer.        cxx_class.declare(code)                for param in params:            param.cxx_predecls(code)        for port in ports.values():            port.cxx_predecls(code)        code()                if cls._base:            code('#include \"params/$.hh\"')            code()                for ptype in ptypes:            if issubclass(ptype, Enum):                code('#include \"enums/$.hh\"')                code()                # now generate the actual param struct        code(\"struct ${cls}Params\")        if cls._base:            code(\"    : public $Params\")        code(\"{\")        if not hasattr(cls, 'abstract') or not cls.abstract:            if 'type' in cls.__dict__:                code(\"    $ create();\")                code.indent()        if cls == SimObject:            code('''    SimObjectParams() {}    virtual ~SimObjectParams() {}    std::string name;            ''')                for param in params:            param.cxx_decl(code)        for port in ports.values():            port.cxx_decl(code)                code.dedent()        code('};')By comparing the code above with the automatically generated struct, it will become apparent which sections of the Python code correspond to the which partsof the CPP implementation. Presented below is the Python code responsible for generating the CPP function (i.e., module_init) that exports the automatically generated struct to Python through pybind.    def pybind_predecls(cls, code):        code('#include \"$\"')        def pybind_decl(cls, code):        py_class_name = cls.pybind_class                # The 'local' attribute restricts us to the params declared in        # the object itself, not including inherited params (which        # will also be inherited from the base class's param struct        # here). Sort the params based on their key        params = list(map(lambda k_v: k_v[1], sorted(cls._params.local.items())))        ports = cls._ports.local                code('''#include \"pybind11/pybind11.h\"#include \"pybind11/stl.h\"#include \"params/$cls.hh\"#include \"Python/pybind11/core.hh\"#include \"sim/init.hh\"#include \"sim/sim_object.hh\"#include \"$\"''')                    for param in params:            param.pybind_predecls(code)                code('''namespace py = pybind11;static voidmodule_init(py::module &amp;m_internal){    py::module m = m_internal.def_submodule(\"param_${cls}\");''')            code.indent()        if cls._base:            code('py::class_&lt;${cls}Params, $Params, ' \\                 'std::unique_ptr&lt;$Params, py::nodelete&gt;&gt;(' \\                 'm, \"${cls}Params\")')        else:            code('py::class_&lt;${cls}Params, ' \\                 'std::unique_ptr&lt;${cls}Params, py::nodelete&gt;&gt;(' \\                 'm, \"${cls}Params\")')                code.indent()        if not hasattr(cls, 'abstract') or not cls.abstract:            code('.def(py::init&lt;&gt;())')            code('.def(\"create\", &amp;${cls}Params::create)')                param_exports = cls.cxx_param_exports + [            PyBindProperty(k)            for k, v in sorted(cls._params.local.items())        ] + [            PyBindProperty(\"port_%s_connection_count\" % port.name)            for port in ports.values()        ]        for exp in param_exports:            exp.export(code, \"%sParams\" % cls)class PyBindProperty(PyBindExport):    def __init__(self, name, cxx_name=None, writable=True):        self.name = name        self.cxx_name = cxx_name if cxx_name else name        self.writable = writable            def export(self, code, cname):        export = \"def_readwrite\" if self.writable else \"def_readonly\"        code('.${export}(\"$\", &amp;${cname}::$)')Upon a thorough comparison of the automatically generated code and the corresponding Python code, it becomes evident that attributes instantiated as instances of Port and Param play a crucial role in both generating the CPP implementation of the Param struct and facilitating its pybind integration.init of MetaSimObjectFrom the Python code template, it becomes clear that attributes related to Param and Port are crucial for automatically generating the Param struct and its pybind integration. The next step is to determine how to selectively filter out theattributes relevant to Param and Port from the classes associated with each hardware component. This is where SimObject becomes useful. We will explore how a metaclass aids in filtering out attributes related to Param and Port from Python classes that inherit from SimObject.    def __init__(cls, name, bases, dict):                                               super(MetaSimObject, cls).__init__(name, bases, dict)                                                                                                           # initialize required attributes                                                                                                                                # class-only attributes                                                         cls._params = multidict() # param descriptions                                  cls._ports = multidict()  # port descriptions                                                                                                                   cls._deprecated_params = multidict()                                                                                                                            # class or instance attributes                                                  cls._values = multidict()   # param values                                      cls._hr_values = multidict() # human readable param values                      cls._children = multidict() # SimObject children                                cls._port_refs = multidict() # port ref objects                                 cls._instantiated = False # really instantiated, cloned, or subclassed                                                                                          ......        for key,val in cls._value_dict.items():                                             # param descriptions                                                            if isinstance(val, ParamDesc):                                                      cls._new_param(key, val)                                                                                                                                    # port objects                                                                  elif isinstance(val, Port):                                                         cls._new_port(key, val)                                                                                                                                     # Deprecated variable names                                                     elif isinstance(val, DeprecatedParam):                                              new_name, new_val = cls._get_param_by_value(val.newParam)                       # Note: We don't know the (string) name of this variable until                  # here, so now we can finish setting up the dep_param.                          val.oldName = key                                                               val.newName = new_name                                                          cls._deprecated_params[key] = val                                                                                                                           # init-time-only keywords                                                       elif key in cls.init_keywords:                                                      cls._set_keyword(key, val, cls.init_keywords[key])                                                                                                          # default: use normal path (ends up in __setattr__)                             else:                                                                               setattr(cls, key, val)     The MetaSimObject metaclass’s above init function is triggered for any Python classes inheriting SimObject, thanks to SimObject setting MetaSimObject as its metaclass. This function iterates through all attributes in the class and checksif it is an instance of either ParamDesc or Port. Depending on the instance type, it invokes ‘_new_param’ or ‘new_port,’ placing the attribute in the ‘_params’ or ‘_ports’ dictionary of the class. In essence, this process effectively filters out and organizes these two types of attributes in their respective dictionaries.    def _new_param(cls, name, pdesc):        # each param desc should be uniquely assigned to one variable        assert(not hasattr(pdesc, 'name'))        pdesc.name = name        cls._params[name] = pdesc        if hasattr(pdesc, 'default'):            cls._set_param(name, pdesc.default, pdesc)    def _new_port(cls, name, port):        # each port should be uniquely assigned to one variable        assert(not hasattr(port, 'name'))        port.name = name        cls._ports[name] = portUpon revisiting the code, you candiscern the utilization of the newly introduced dictionaries as keys inautomatically generating the CPP implementation for the Param struct and its corresponding pybind code.Type sensitive Python ParamsPython Param class is a placeholder for any parameter that should be translated into CPP from Python attribute. Compared with Python which doesn’t need a stricttype for attribute, CPP need clear and distinct type for all variables. Therefore,to translate Python attribute to CPP variable, GEM5 should manage value and typealtogether, which is achieved by ParamDesc class.# Python/m5/params.pyclass ParamDesc(object):    def cxx_decl(self, code):        code('$ $;')The ‘cxx_decl’ method is called during GEM5’s process of automatically converting Python attributes (instances of ParamDesc) into CPP variables in the cxx_param_declfunction. As depicted in the code, it retrieves the CPP type from the‘self.ptype.cxx_type’ and its name from ‘self.name’. We will take a look at how Param related python classes manages those two attributes. You might wonder whyit is ParamDesc not Param previously used to define attributes in Python class.Param = ParamFactory(ParamDesc)The trick is assigning another class ParamFactory to Param so that developer can easily instantiate ParamDesc object per attribute, required for generating CPP class parameter.class ParamFactory(object):    def __init__(self, param_desc_class, ptype_str = None):        self.param_desc_class = param_desc_class        self.ptype_str = ptype_str        def __getattr__(self, attr):        if self.ptype_str:            attr = self.ptype_str + '.' + attr        return ParamFactory(self.param_desc_class, attr)        def __call__(self, *args, **kwargs):        ptype = None        try:            ptype = allParams[self.ptype_str]        except KeyError:            # if name isn't defined yet, assume it's a SimObject, and            # try to resolve it later            pass        return self.param_desc_class(self.ptype_str, ptype, *args, **kwargs)As Python lacks a type that can directly correspond to C++ types on a one-to-one basis, it employs various Python classes to represent C++ types that the Param should be translated into. Let’s explore how the ParamFactory and ParamDesc canbe used to generate a Python class object representing specific C++ types.    cache_line_size = Param.Unsigned(64, \"Cache line size in bytes\")The right-hand side of the assignment appears simple at first glance, but it involves multiple function invocations in detail. Initially, the interpretation of Param is as ParamFactory(ParamDesc), resulting in a ParamFactory object with ‘param_desc_class’ set to ‘ParamDesc’. This returned object is then utilized toaccess its ‘Unsigned’ attribute. As it defines the ‘getattr’ function, this function is invoked instead of directly accessing the ‘Unsigned’ attribute. Consequently, it generates another ParamFactory object with ‘param_desc_class’set to ‘ParamDesc’ and ‘ptype_str’ set to ‘Unsigned’. The parentheses following the ParamFactory object are interpreted as a function call, leading to the invocation of the ‘call’ method. While allParams is not yet known, it returnsthe class that matches ptype_str (‘Unsigned’). Subsequently, it returns a ParamDesc object initialized with “Unsigned” and the class matching with the pytype_str.Then how GEM5 generates dictionary mapping ptype_str to class objectassociated with the string? To manage allParams dictionary, GEM5 utilize another Python metaclass, MetaParamValue.class MetaParamValue(type):    def __new__(mcls, name, bases, dct):        cls = super(MetaParamValue, mcls).__new__(mcls, name, bases, dct)        if name in allParams:            warn(\"%s already exists in allParams. This may be caused by the \" \\                 \"Python 2.7 compatibility layer.\" % (name, ))        allParams[name] = cls        return cls As shown in the ‘new’ function of the metaclass,it produces an ‘allParams’ dictionary that can be accessed using its class name and returns the corresponding class object. Consequently, Python classes intended for translating the ‘Param’attribute to the appropriate CPP type implementation should designate ‘MetaParamValue’ as their metaclass.class Unsigned(CheckedInt): cxx_type = 'unsigned'; size = 32; unsigned = Trueclass CheckedIntType(MetaParamValue):    def __init__(cls, name, bases, dict):        super(CheckedIntType, cls).__init__(name, bases, dict)            # CheckedInt is an abstract base class, so we actually don't        # want to do any processing on it... the rest of this code is        # just for classes that derive from CheckedInt.        if name == 'CheckedInt':            return        if not (hasattr(cls, 'min') and hasattr(cls, 'max')):            if not (hasattr(cls, 'size') and hasattr(cls, 'unsigned')):                panic(\"CheckedInt subclass %s must define either\\n\" \\                      \"    'min' and 'max' or 'size' and 'unsigned'\\n\",                      name);            if cls.unsigned:                cls.min = 0                cls.max = 2 ** cls.size - 1            else:                cls.min = -(2 ** (cls.size - 1))                cls.max = (2 ** (cls.size - 1)) - 1As indicated in the class definition, the ‘Unsigned’ class inherits from ‘CheckedIntType,’ which designates ‘MetaParamValue’ as its metaclass. Consequently, during the initialization of the ‘Unsigned’ class, it will callthe ‘new’ function of the ‘MetaParamValue’ metaclass, creating a mapping fromthe string ‘Unsigned’ to the class object ‘Unsigned’ in the ‘allParams’.Therefore, in the preceding code, the ‘ptype’ returned from ‘allParams’ shouldbe an object of the ‘Unsigned’ class. In summary, the RHS of the assignment will be  ParamDesc(“Unsigned”, Unsigned class object, *args, **kwargs)Therefore the cache_line_size will have the ParamDesc class object instantiatedby the code block.class ParamDesc(object):    def __init__(self, ptype_str, ptype, *args, **kwargs):        self.ptype_str = ptype_str        # remember ptype only if it is provided        if ptype != None:            self.ptype = ptype        if args:            if len(args) == 1:                self.desc = args[0]            elif len(args) == 2:                self.default = args[0]                self.desc = args[1]            else:                raise TypeError('too many arguments')        ......The passed string and class object are stored in the ParamDesc attributes and will be used to generate the CPP type!Python Port describes connectivityWhile going through Params and its pybind, you may have noticed that MetaSimObject filter out Port attributes from Python classes separately as wellas the ParamDesc instance. Given that GEM5 serves as a comprehensive system simulator, it necessitates not only diverse hardware elements like CPU and memory controllers that make up the platform but also the interconnecting wiresfacilitating communication between these hardware components. Given that the communication medium functions as a hardware component, GEM5 simulates it just like any other hardware components in the system.Moreover, as well as the Python classes are utilized to provide parameters of hardware components and instantiate the simulation for each hardware component implemented in CPP, the connectivity presented in Python code should be translated into CPP and generate connection between CPP class objects. Therefore, we need to understand how this transformation happens.How Python script establish the connection?Let’s see how Python script generates connection between two different hardwarecomponents through the port.# create the system we are going to simulatesystem = System()# Create a memory bus, a system crossbar, in this casesystem.membus = SystemXBar()# Connect the system up to the membussystem.system_port = system.membus.slaveclass System(SimObject):                                                            type = 'System'                                                                 cxx_header = \"sim/system.hh\"                                                    system_port = RequestPort(\"System port\")   class Port(object):    ......    def __init__(self, role, desc, is_source=False):        self.desc = desc        self.role = role        self.is_source = is_source    ......class RequestPort(Port):    # RequestPort(\"description\")    def __init__(self, desc):        super(RequestPort, self).__init__(                'GEM5 REQUESTOR', desc, is_source=True)class ResponsePort(Port):    # ResponsePort(\"description\")    def __init__(self, desc):        super(ResponsePort, self).__init__('GEM5 RESPONDER', desc)As illustrated in the Python script, it sets up the configuration for the Systemand SystemXBar, creating a connection between them by linking the Response port (SystemXBar.slave) to the Request port (System.system_port). While this may appear as a straightforward assignment, there are intricate details underlying the support for Port assignment. To grasp this, it’s essential to recall thatall hardware component classes inherit from SimObject. The SimObject class defines the getattr and setattr functions to control attribute accessand assignment. Let’s delve into each of these details in turn.    def __setattr__(self, attr, value):        # normal processing for private attributes        if attr.startswith('_'):            object.__setattr__(self, attr, value)            return                if attr in self._deprecated_params:            dep_param = self._deprecated_params[attr]            dep_param.printWarning(self._name, self.__class__.__name__)             return setattr(self, self._deprecated_params[attr].newName, value)                if attr in self._ports:            # set up port connection            self._get_port_ref(attr).connect(value)            return                param = self._params.get(attr)        if param:            try:                hr_value = value                value = param.convert(value)            except Exception as e:                msg = \"%s\\nError setting param %s.%s to %s\\n\" % \\                      (e, self.__class__.__name__, attr, value)                e.args = (msg, )                raise            self._values[attr] = value            # implicitly parent unparented objects assigned as params            if isSimObjectOrVector(value) and not value.has_parent():                self.add_child(attr, value)            # set the human-readable value dict if this is a param            # with a literal value and is not being set as an object            # or proxy.            if not (isSimObjectOrVector(value) or\\                    isinstance(value, m5.proxy.BaseProxy)):                self._hr_values[attr] = hr_value                        return                # if RHS is a SimObject, it's an implicit child assignment        if isSimObjectOrSequence(value):            self.add_child(attr, value)            return                # no valid assignment... raise exception        raise AttributeError(\"Class %s has no parameter %s\" \\              % (self.__class__.__name__, attr))To understand the reference system.membus.slave, it’s crucial to grasp how membus becomes an attribute of the System. Given that there is no statically predefined attribute named membus in the System class, it is dynamically added to the Systemclass object during runtime. The setattr function in the SimObject class comes into play when a new attribute is introduced to the object. Since the added value is another SimObject class object obtained from SystemXBar(), it is treatedas a child of the System.    # Add a new child to this object.    def add_child(self, name, child):        child = coerceSimObjectOrVector(child)        if child.has_parent():            warn(\"add_child('%s'): child '%s' already has parent\", name,                child.get_name())        if name in self._children:            # This code path had an undiscovered bug that would make it fail            # at runtime. It had been here for a long time and was only            # exposed by a buggy script. Changes here will probably not be            # exercised without specialized testing.            self.clear_child(name)        child.set_parent(self, name)        if not isNullPointer(child):            self._children[name] = childRecall my earlier mention that SimObject can be structured hierarchically. Considering that the System class represents the entire simulated system, it follows logically that the crossbar connecting hardware components in the systemshould be a child of the System. Now, let’s explore the outcome when attempting to access system.membus.slave!    def __getattr__(self, attr):        if attr in self._deprecated_params:            dep_param = self._deprecated_params[attr]            dep_param.printWarning(self._name, self.__class__.__name__)            return getattr(self, self._deprecated_params[attr].newName)                if attr in self._ports:            return self._get_port_ref(attr)                if attr in self._values:            return self._values[attr]                if attr in self._children:            return self._children[attr]                # If the attribute exists on the C++ object, transparently        # forward the reference there.  This is typically used for        # methods exported to Python (e.g., init(), and startup())        if self._ccObject and hasattr(self._ccObject, attr):            return getattr(self._ccObject, attr)                err_string = \"object '%s' has no attribute '%s'\" \\              % (self.__class__.__name__, attr)                if not self._ccObject:            err_string += \"\\n  (C++ object is not yet constructed,\" \\                          \" so wrapped C++ methods are unavailable.)\"                raise AttributeError(err_string)The access is accomplished through two calls to the __getattr__ method. It canbe conceptualized as ‘(system.membus).slave’. Since System is a SimObject, andthe SimObject class defines the __getattr__ function, this function isautomatically invoked to access the membus attribute. As membus has beenregistered as a child of the system, the SystemXBar class object is retrievedfirst. Additionally, since it is a SimObject, when the slave attribute isaccessed, it triggers another __getattr__ function. As the slave attribute isdeclared as a Port in the BaseXBar class, which is the base Python class ofSystemXBar, it should have been filtered out as ‘_ports’ when the SystemXBar isdefined, thanks to the metaclass. Therefore, when an attribute related to Portis accessed, it invokes ‘_get_port_ref’ to return a reference to that port.PortRef, connecting two end ports    def _get_port_ref(self, attr):        # Return reference that can be assigned to another port        # via __setattr__.  There is only ever one reference        # object per port, but we create them lazily here.        ref = self._port_refs.get(attr)        if ref == None:            ref = self._ports[attr].makeRef(self)            self._port_refs[attr] = ref        return ref SimObject has cache for reference of Port, self._port_refs. If it is the firsttime to access this attribute, then the cache should be empty and will invokemakeRef function of the system_port object.class Port(object):                                                                 ......    # Port(\"role\", \"description\")     # Generate a PortRef for this port on the given SimObject with the              # given name                                                                    def makeRef(self, simobj):                                                          return PortRef(simobj, self.name, self.role, self.is_source)  makeRef creates an instance of PortRef, where Port serves as a wrapper class that conveys information about the port, and the actual reference to the Port isdefined by the PortRef class. The retrieved PortRef instance is then stored in the ‘_ports’ attribute of the SimObject. This storage will later be employed to transfer the Python-presented connectivity between the hardware components toC++. Regardless, the right-hand side of the assignment is transformed into an object of PortRef. Assigning it to ‘system.system_port’ on the left-hand sidetriggers another ‘setattr’ within the SimObject!        if attr in self._ports:                                                             # set up port connection                                                        self._get_port_ref(attr).connect(value)                                         return                     At this point, since ‘system_port’ is an instance of a Port class, this attributemust exist in the ‘_ports’ dictionary. When considering the assignment in terms of ports logically, it should establish a connection between two hardwarecomponents. To accomplish this, it effectively invokes the ‘connect’ function!Given that ‘_get_port_ref’ returns another PortRef for the ‘system_port’, itproceeds to connect the PortRef of ‘system_port’ and ‘slave’.class PortRef(object):     def connect(self, other):        if isinstance(other, VectorPortRef):            # reference to plain VectorPort is implicit append            other = other._get_next()        if self.peer and not proxy.isproxy(self.peer):            fatal(\"Port %s is already connected to %s, cannot connect %s\\n\",                  self, self.peer, other);        self.peer = other        if proxy.isproxy(other):            other.set_param_desc(PortParamDesc())            return        elif not isinstance(other, PortRef):            raise TypeError(\"assigning non-port reference '%s' to port '%s'\" \\                  % (other, self))        if not Port.is_compat(self, other):            fatal(\"Ports %s and %s with roles '%s' and '%s' \"                    \"are not compatible\", self, other, self.role, other.role)        if other.peer is not self:            other.connect(self)SConscript: generating files for CPP implementationNow we can understand where the Python code is located and how the data requiredfor generating CPP implementation can be gathered from each Python class. Then,some program should invoke the Python method to generate actual CPP and headerfile containing automatically generated CPP implementation. That is done at the compile time by scone. SCons is a build automation tool that uses Python scriptsfor configuration and build control.# gem5/src/SConscript# Generate all of the SimObject param C++ struct header filesparams_hh_files = []for name,simobj in sorted(sim_objects.items()):    # If this simobject's source changes, we need to regenerate the header.    py_source = PySource.modules[simobj.__module__]    extra_deps = [ py_source.tnode ]        # Get the params for just this SimObject, excluding base classes.    params = simobj._params.local.values()    # Extract the parameters' c++ types.    types = sorted(map(lambda p: p.ptype.cxx_type, params))    # If any of these types have changed, we need to regenerate the header.    extra_deps.append(Value(types))        hh_file = File('params/%s.hh' % name)    params_hh_files.append(hh_file)    env.Command(hh_file, Value(name),                MakeAction(createSimObjectParamStruct, Transform(\"SO PARAM\")))    env.Depends(hh_file, depends + extra_deps)def createSimObjectParamStruct(target, source, env):    assert len(target) == 1 and len(source) == 1    name = source[0].get_text_contents()    obj = sim_objects[name]    code = code_formatter()    obj.cxx_param_decl(code)    code.write(target[0].abspath)# Generate SimObject Python bindings wrapper filesif env['USE_PYTHON']:    for name,simobj in sorted(sim_objects.iteritems()):        py_source = PySource.modules[simobj.__module__]        extra_deps = [ py_source.tnode ]        cc_file = File('Python/_m5/param_%s.cc' % name)        env.Command(cc_file, Value(name),                    MakeAction(createSimObjectPyBindWrapper,                               Transform(\"SO PyBind\")))        env.Depends(cc_file, depends + extra_deps)        Source(cc_file)def createSimObjectPyBindWrapper(target, source, env):    name = source[0].get_text_contents()    obj = sim_objects[name]    code = code_formatter()    obj.pybind_decl(code)    code.write(target[0].abspath)When you take a look at the main SConScript located in the root src directory,there are two locations invoking Python methods that you must be familiar with(obj.pybind_decl and obj.cxx_param_decl). One thing not clear in the SConscript is sim_objects.# gem5/src/SConscript  sim_objects = m5.SimObject.allClassesIn the SConscript, it is defined as attribute from m5.SibObject Python module. Then what is allClasses? The answer is in the MetaSimObject!# Python/m5/SimObject.py# list of all SimObject classesallClasses = {}class MetaSimObject(type):    def __new__(mcls, name, bases, dict):        assert name not in allClasses, \"SimObject %s already present\" % name                # Copy \"private\" attributes, functions, and classes to the        # official dict.  Everything else goes in _init_dict to be        # filtered in __init__.        cls_dict = {}        value_dict = {}        cxx_exports = []        ......        cls = super(MetaSimObject, mcls).__new__(mcls, name, bases, cls_dict)        if 'type' in value_dict:            allClasses[name] = cls        return clsAs depicted in the above code, the new method generate dictionary consistingof its name and actual class object and assign it to allClasses attribute in them5.SimObject. However, to make MetaSimObject to invoke new method for allPython classes inheriting from SimObject and fill out allClasses dictionary,all Python classes should be imported first.class SimObject(PySource):    '''Add a SimObject Python file as a Python source object and add    it to a list of sim object modules'''    fixed = False    modnames = []    def __init__(self, source, tags=None, add_tags=None):        '''Specify the source file and any tags (automatically in        the m5.objects package)'''        super(SimObject, self).__init__('m5.objects', source, tags, add_tags)        if self.fixed:            raise AttributeError(\"Too late to call SimObject now.\")        bisect.insort_right(SimObject.modnames, self.modname)for modname in SimObject.modnames:    exec('from m5.objects import %s' % modname)To import relevant Python classes, root SConscript defines SimObject class and import the classes to the Python environment where SConscript is being executed.  Note that SimObject class is not identical to SimObject class havingMetaSimObject as its metaclass used for defining simulated components.Then who instantiates above SimObjects? SConscript can exist in subdirectories to handle the build details. Let’s take a look at the sub-directory containing Python code defining Python classes inheriting SimObject.# /gem5/src/sim/SConscriptImport('*')SimObject('ClockedObject.py')SimObject('TickedObject.py')SimObject('Workload.py')SimObject('Root.py')SimObject('ClockDomain.py')SimObject('VoltageDomain.py')SimObject('System.py')SimObject('DVFSHandler.py')SimObject('SubSystem.py')SimObject('RedirectPath.py')SimObject('PowerState.py')SimObject('PowerDomain.py')As shown in the above script, it instantiates SimObject to import Python classesdefining hardware simulation building block. You might wonder how this exported CPP implementation can be utilized by Python classes, but please bear with me!I will give you details after covering Port!Python to CPP transformationWhoa! It was quite intense because GEM5 heavily relies on Python classes torepresent and organize hardware components. However, the most crucial questionremains unanswered! As Python serves merely as a wrapper for CPP classes usedin the actual simulation, the configuration specified in Python must betranslated into CPP implementation, encompassing the instantiation of CPPclasses and the establishment of connections between them.# set up the root SimObject and start the simulationroot = Root(full_system = False, system = system)# instantiate all of the objects we've created abovem5.instantiate()When you finish configuration, you should instantiate Root class and invoke ‘instantiate’ function to transform your Python configurations into CPP implementation.class Root(SimObject):        _the_instance = None    def __new__(cls, **kwargs):        if Root._the_instance:            fatal(\"Attempt to allocate multiple instances of Root.\")            return None        Root._the_instance = SimObject.__new__(cls)        return Root._the_instance    @classmethod    def getInstance(cls):        return Root._the_instance        type = 'Root'    cxx_header = \"sim/root.hh\"                          # By default, root sim object and hence all other sim objects schedule    # event on the eventq with index 0.    eventq_index = 0    # Simulation Quantum for multiple main event queue simulation.    # Needs to be set explicitly for a multi-eventq simulation.    sim_quantum = Param.Tick(0, \"simulation quantum\")    full_system = Param.Bool(\"if this is a full system simulation\")    # Time syncing prevents the simulation from running faster than real time.    time_sync_enable = Param.Bool(False, \"whether time syncing is enabled\")    time_sync_period = Param.Clock(\"100ms\", \"how often to sync with real time\")    time_sync_spin_threshold = \\            Param.Clock(\"100us\", \"when less than this much time is left, spin\")The Root Python class utilize singleton design pattern that restricts the instantiation of a class to only one instance and provides a global point of access to that instance. The getInstance returns singleton object.Instantiate CPP implementationdef instantiate(ckpt_dir=None):    from m5 import options        root = objects.Root.getInstance()            if not root:        fatal(\"Need to instantiate Root() before calling instantiate()\")        # we need to fix the global frequency    ticks.fixGlobalFrequency()    # Make sure SimObject-valued params are in the configuration    # hierarchy so we catch them with future descendants() walks    for obj in root.descendants(): obj.adoptOrphanParams()        # Unproxy in sorted order for determinism    for obj in root.descendants(): obj.unproxyParams()        ......        # Create the C++ sim objects and connect ports    for obj in root.descendants(): obj.createCCObject()    for obj in root.descendants(): obj.connectPorts()    # Do a second pass to finish initializing the sim objects    for obj in root.descendants(): obj.init()        # Do a third pass to initialize statistics    stats._bindStatHierarchy(root)    root.regStats()    ......The main role of the instantiate function is iterating all SimObjects in the hierarchies and initialize CPP classes and ports. As we already set-up all required information to instantiate CPP classes (e.g., Params) and connectivitybetween the hardware components (e.g., Ports), its role is invoking proper CPP functions to finalize set-up!    def descendants(self):        yield self        # The order of the dict is implementation dependent, so sort        # it based on the key (name) to ensure the order is the same        # on all hosts        for (name, child) in sorted(self._children.items()):            for obj in child.descendants():                yield objOne thing to note is because Root Python class is also SimObject, but the rootnode in hierarchy, it can access other system components through ‘descendants’ method provided by the SimObject. It just iterates all SimObject!Create CPP objects!    # Call C++ to create C++ object corresponding to this object    def createCCObject(self):        self.getCCParams()        self.getCCObject() # force creationOur goal is instantiating CPP class object representing hardware component. Sincethe information required for initializing this class is conveyed to the class constructor, so first of all it needs the Param struct. And then, we need to instantiate the CPP class representing the hardware component.    def getCCParams(self):        if self._ccParams:            return self._ccParams                cc_params_struct = getattr(m5.internal.params, '%sParams' % self.type)        cc_params = cc_params_struct()        cc_params.name = str(self)                param_names = list(self._params.keys())        param_names.sort()        for param in param_names:            value = self._values.get(param)            if value is None:                fatal(\"%s.%s without default or user set value\",                      self.path(), param)                        value = value.getValue()            if isinstance(self._params[param], VectorParamDesc):                assert isinstance(value, list)                vec = getattr(cc_params, param)                assert not len(vec)                # Some types are exposed as opaque types. They support                # the append operation unlike the automatically                # wrapped types.                if isinstance(vec, list):                    setattr(cc_params, param, list(value))                else:                    for v in value:                        getattr(cc_params, param).append(v)            else:                setattr(cc_params, param, value)                port_names = list(self._ports.keys())        port_names.sort()        for port_name in port_names:            port = self._port_refs.get(port_name, None)            if port != None:                 port_count = len(port)            else:                port_count = 0             setattr(cc_params, 'port_' + port_name + '_connection_count',                    port_count)        self._ccParams = cc_params        return self._ccParamsAs we have all required information to generate and initialize Param struct, we should utilize it! First of all, we need to instantiate the Param struct associatedwith the class that we want to instantiate to simulate one hardware component.We’ve seen that the automatically generated pybind code exports the Param structto Python.# gem5/src/Python/m5/internal/params.pyfor name, module in inspect.getmembers(_m5):    if name.startswith('param_') or name.startswith('enum_'):        exec(\"from _m5.%s import *\" % name)As all CPP implementations are exported to Python as a module of ‘_m5’ the Struct mapped for Param that we need to instantiate the CPP class should be accessible from importing ‘_m5’ module.static voidmodule_init(py::module &amp;m_internal){    py::module m = m_internal.def_submodule(\"param_BaseTLB\");    py::class_&lt;BaseTLBParams, SimObjectParams, std::unique_ptr&lt;BaseTLBParams, py::nodelete&gt;&gt;(m, \"BaseTLBParams\")        .def_readwrite(\"port_cpu_side_ports_connection_count\", &amp;BaseTLBParams::port_cpu_side_ports_connection_count)        .def_readwrite(\"port_mem_side_port_connection_count\", &amp;BaseTLBParams::port_mem_side_port_connection_count)        ;    py::class_&lt;BaseTLB, SimObject, std::unique_ptr&lt;BaseTLB, py::nodelete&gt;&gt;(m, \"BaseTLB\")        ;}For example, BaseTLBParams CPP struct will be exported as ‘_m5.param_BaseTLB’.Therefore all pybind exported CPP implementation will be imported to m5.internal.params module and ‘cc_params_struct’ will get the Python Classobject of the Param struct. By instantiating it, cc_params will have the instanceof the class, and will be initialized based on the values previously collected from the Python class BaseTLB. After initializing all fields of the struct, it assign the object to the ‘_ccParams’.    def getCCObject(self):        if not self._ccObject:            # Make sure this object is in the configuration hierarchy            if not self._parent and not isRoot(self):                raise RuntimeError(\"Attempt to instantiate orphan node\")            # Cycles in the configuration hierarchy are not supported. This            # will catch the resulting recursion and stop.            self._ccObject = -1            if not self.abstract:                params = self.getCCParams()                self._ccObject = params.create()        elif self._ccObject == -1:            raise RuntimeError(\"%s: Cycle found in configuration hierarchy.\" \\                  % self.path())        return self._ccObjectNow as we have the Param struct object, we need to instantiate a CPP class object that actually simulate the hardware logic. ‘getCCParams’ returns the Param struct object previously assigned to ‘_ccParams’. Afterwards, it invokescreate() method defined in the Param struct. That would be weird to you becauseyou cannot find the create method in the automatically generated Param struct. However, when you look at the auto-generated pybind code, you can definitely find it exports the create function defined for Param struct. That’s because the create method should be implemented manually by the developer beforehand incpp code.// gem5/src/sim/system.cc System *SystemParams::create(){    return new System(this);}Each create method for hardware component should instantiate the CPP class associated with the component. Also note that it passes this to the System constructor which is the SystemParams passing all information required to initialize the class! Finally you can retrieve CPP class object instantiated with Param configured based on Python script.Connect Ports in CPPAs we’ve seen in previous Python code, we established all connectivity betweenhardware components in Python, not in actual CPP simulation. It is time to transfer this connection implemented in Python to CPP implementations to establish actual connection between simulated hardware components.class SimObject(object):      def connectPorts(self):        # Sort the ports based on their attribute name to ensure the        # order is the same on all hosts        for (attr, portRef) in sorted(self._port_refs.items()):            portRef.ccConnect()class PortRef(object):    def ccConnect(self):        if self.ccConnected: # already done this            return                peer = self.peer        if not self.peer: # nothing to connect to            return            port = self.simobj.getPort(self.name, self.index)        peer_port = peer.simobj.getPort(peer.name, peer.index)        port.bind(peer_port)            self.ccConnected = True Since PortRef Python class have all information required to establish connectionbetween two components (two end ports, self and peer), the remaining task is invoking proper CPP functions to establish the connection as described by the Python PortRef. First of all, it needs information about two end-ports that should be connected to each other in CPP implementation.class SimObject(object):     @cxxMethod(return_value_policy=\"reference\")    def getPort(self, if_name, idx):        passdef cxxMethod(*args, **kwargs):    \"\"\"Decorator to export C++ functions to Python\"\"\"    def decorate(func):        name = func.__name__        override = kwargs.get(\"override\", False)        cxx_name = kwargs.get(\"cxx_name\", name)        return_value_policy = kwargs.get(\"return_value_policy\", None)        static = kwargs.get(\"static\", False)                args, varargs, keywords, defaults = inspect.getargspec(func)        if varargs or keywords:            raise ValueError(\"Wrapped methods must not contain variable \" \\                             \"arguments\")            # Create tuples of (argument, default)        if defaults:            args = args[:-len(defaults)] + \\                   list(zip(args[-len(defaults):], defaults))        # Don't include self in the argument list to PyBind        args = args[1:]            @wraps(func)        def cxx_call(self, *args, **kwargs):            ccobj = self.getCCClass() if static else self.getCCObject()            return getattr(ccobj, name)(*args, **kwargs)            @wraps(func)        def py_call(self, *args, **kwargs):            return func(self, *args, **kwargs)        f = py_call if override else cxx_call        f.__pybind = PyBindMethod(name, cxx_name=cxx_name, args=args,                                  return_value_policy=return_value_policy,                                  static=static)        return f    if len(args) == 0:        return decorate    elif len(args) == 1 and len(kwargs) == 0:        return decorate(*args)    else:        raise TypeError(\"One argument and no kwargs, or only kwargs expected\")GEM5 defines decorator function cxxMethod which bridge the Python function callto corresponding CPP function call. I will not cover details of decorator in Python. Please refer to this blog posting for further details.The important thing is when you invoke getPort, it will invoke getPort member function defined in the CPP class representing hardware component having the port. Let’s see the example.Port &amp;BaseXBar::getPort(const std::string &amp;if_name, PortID idx){    if (if_name == \"mem_side_ports\" &amp;&amp; idx &lt; memSidePorts.size()) {        // the memory-side ports index translates directly to the vector        // position        return *memSidePorts[idx];    } else  if (if_name == \"default\") {        return *memSidePorts[defaultPortID];    } else if (if_name == \"cpu_side_ports\" &amp;&amp; idx &lt; cpuSidePorts.size()) {        // the CPU-side ports index translates directly to the vector position        return *cpuSidePorts[idx];    } else {        return ClockedObject::getPort(if_name, idx);    }}BaseXBar has three member fields describing hardware ports. Therefore, when the getPort is invoked from the Python class, for example SystemXBar inheriting the BaseXBar, it will invoke the getPort of the BaseXBar. Also, based on the passedstring, and index (for vectored ports), it will return CPP port member field of the class to Python! Since the returned object is CPP object not Python, the bind function will be invoked from CPP.voidRequestPort::bind(Port &amp;peer){    auto *response_port = dynamic_cast&lt;ResponsePort *&gt;(&amp;peer);    fatal_if(!response_port, \"Can't bind port %s to non-response port %s.\",             name(), peer.name());    // request port keeps track of the response port    _responsePort = response_port;    Port::bind(peer);    // response port also keeps track of request port    _responsePort-&gt;responderBind(*this);}The actual binding is very simple operation because it just tracks the informationabout the port and its connection (peer port). I will cover how the ports are utilized in simulation to transfer data between the components connected through the ports. In this posting, understanding how to establish the connection betweentwo different components having port would be sufficient.class Port{    /** Attach to a peer port. */    virtual void    bind(Port &amp;peer)    {        _peer = &amp;peer;        _connected = true;    }voidResponsePort::responderBind(RequestPort&amp; request_port){    _requestPort = &amp;request_port;    Port::bind(request_port);}Binding should be done in bi-directional, so the above function establish the connection in two different ports (request and response).Final remarksAt the first outlook, it must be very confusing to understand why GEM5 has Python script and CPP defining similar classes, which makes you misunderstand what is the role of each classes defined in different languages. However, the the points is the actual simulation is mostly achieved by CPP implementation, and the Python is a wrapper for those CPP implementation. In detail, the Pythonis utilized to configure hardware parameters such as cache size, data size, andwhatnot. I covered several Python specific concepts such as metaclass and pybind to allowGEM5 automatically generate several CPP implementations and its binding to Pythonso that Python script can instantiate the CPP class object for actual simulation.Hope that this posting can help you understand mysterious configuration of GEM5."
  }
  
]

